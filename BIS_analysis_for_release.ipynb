{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "BIS_analysis_for_release.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "gHVBpmntzDEw",
        "Wbg5E0VTzMBx",
        "LN8NY3I9zOid",
        "aIe-QK-u5XQK",
        "iMYR2CJv9Naj",
        "M5Lss9JKlYKs",
        "Wb9Q9_pQuIom",
        "RQD1dcYhuLHh",
        "WAe1UnF4refB",
        "AOghhwq9XT90"
      ],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/paulsedille/NeurIPS-Broader-Impact-Statements/blob/main/BIS_analysis_for_release.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7mFuBu_CBLJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "570e122b-60a1-40d4-a094-8606e73a7865"
      },
      "source": [
        "import pandas as pd\n",
        "from os import sep\n",
        "import numpy as np\n",
        "import re\n",
        "from google.colab import drive\n",
        "from PIL import Image\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import auth\n",
        "import gspread\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import shutil\n",
        "import glob\n",
        "from collections import Counter\n",
        "import string\n",
        "import nltk\n",
        "from nltk.sentiment.util import mark_negation\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
            "  warnings.warn(\"The twython library has not been installed. \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7oPLf9kCprT"
      },
      "source": [
        "auth.authenticate_user()\n",
        "\n",
        "gc = gspread.authorize(GoogleCredentials.get_application_default())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8o6QI-sNFYG3",
        "outputId": "e3cfa93a-8e8f-4f37-e6a2-81b038c709ae"
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2V0hmidqJhSP"
      },
      "source": [
        "# It should be authenticated and able to get the NeurIPS 2020 BIS spreadsheet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJo-qe7KsNb6"
      },
      "source": [
        "### Run these next three cells"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vv0NjaHiFX5N"
      },
      "source": [
        "# Open sheet and make dataframe\n",
        "sheet = gc.open('NeurIPS 2020 BIS').sheet1\n",
        "rows = sheet.get_all_values()\n",
        "megaDf = pd.DataFrame.from_records(rows)\n",
        "megaDf.columns = megaDf.iloc[0]\n",
        "megaDf = megaDf.drop(megaDf.index[0])\n",
        "#megaDf = megaDf.drop('Random Check', axis=1)\n",
        "\n",
        "# Convert strings to numeric\n",
        "megaDf['academic'] = pd.to_numeric(megaDf['academic'])\n",
        "megaDf['industry'] = pd.to_numeric(megaDf['industry'])\n",
        "megaDf['mixed'] = pd.to_numeric(megaDf['mixed'])\n",
        "megaDf['word count'] = pd.to_numeric(megaDf['word count'])\n",
        "megaDf['sentence count'] = pd.to_numeric(megaDf['sentence count'])\n",
        "megaDf['citation count'] = pd.to_numeric(megaDf['citation count'])\n",
        "\n",
        "# define valid split groupings\n",
        "splits = ['all','aff','cluster','loc','us-ch','bigTech'] # no primary grouping but can add\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sz3uqnztoBrA"
      },
      "source": [
        "# Generate statements with negated terms\n",
        "megaDf['bis_with_neg'] = megaDf.apply(lambda x: x['impact statement'].translate(str.maketrans({'.':' . ','!':' ! ','?':' ? ',',':' , ',':':' : ',';':' ; '})),axis=1)\n",
        "# apply negation\n",
        "useDoubleNeg = True\n",
        "megaDf['bis_with_neg'] = megaDf.apply(lambda x: mark_negation(x['bis_with_neg'].split(),double_neg_flip=useDoubleNeg), axis=1)\n",
        "\n",
        "megaDf['bis_with_neg'].apply(lambda x: ' '.join(x)).to_csv('negated statements.csv')\n",
        "#' '.join(x for x in megaDf['bis_with_neg'])\n",
        "#.to_csv('negated statements.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvAFWVr3cfzX"
      },
      "source": [
        "#### Helper functions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggMcNc54cTNu"
      },
      "source": [
        "# Helper functions to group by continent\n",
        "def makeDf(relList):\n",
        "  countryRegex = r\"'(\\w*\\s*\\w*)'\"\n",
        "  rowList = []\n",
        "\n",
        "  for index, row in megaDf.iterrows():\n",
        "    countries = re.findall(countryRegex,row['country'])\n",
        "    for country in countries:\n",
        "      if country in relList:\n",
        "        rowList.append(row)\n",
        "        break\n",
        "  return pd.DataFrame(rowList)\n",
        "\n",
        "\n",
        "# Helper function to generate dataframes for each continent\n",
        "def generateContinentDfs():\n",
        "  ccSheet = gc.open('labelled_countries').sheet1\n",
        "  ccRows = ccSheet.get_all_values()\n",
        "  upContDf = pd.DataFrame.from_records(ccRows)\n",
        "  upContDf.columns = upContDf.iloc[0]\n",
        "  upContDf = upContDf.drop(upContDf.index[0])\n",
        "  upContDf = upContDf.drop(upContDf.columns[0],axis=1)\n",
        "  \n",
        "  asia = upContDf[upContDf['continent'] == 'Asia']\n",
        "  asiaList = list(asia['country'].values)\n",
        "  africa = upContDf[upContDf['continent'] == 'Africa']\n",
        "  africaList = list(africa['country'].values)\n",
        "  northAm = upContDf[upContDf['continent'] == 'North America']\n",
        "  northAmList = list(northAm['country'].values)\n",
        "  southAm = upContDf[upContDf['continent'] == 'South America']\n",
        "  southAmList = list(southAm['country'].values)\n",
        "  europe = upContDf[upContDf['continent'] == 'Europe']\n",
        "  europeList = list(europe['country'].values)\n",
        "  oceania = upContDf[upContDf['continent'] == 'Oceania']\n",
        "  oceaniaList = list(oceania['country'].values)\n",
        "\n",
        "  asiaDf = makeDf(asiaList)\n",
        "  africaDf = makeDf(africaList)\n",
        "  northAmDf = makeDf(northAmList)\n",
        "  southAmDf = makeDf(southAmList)\n",
        "  europeDf = makeDf(europeList)\n",
        "  oceaniaDf = makeDf(oceaniaList)\n",
        "\n",
        "  return([asiaDf,africaDf,northAmDf,southAmDf,europeDf,oceaniaDf],\n",
        "         ['Asia','Africa','North America','South America','Europe','Oceania'])\n",
        "\n",
        "\n",
        "# Helper function to generate dataframes grouped by a column\n",
        "# @col: the column name\n",
        "# @top10: True to limit to 10 largest groupings\n",
        "def generateSbjDfs(col,top10=False):\n",
        "  dfs = []\n",
        "\n",
        "  sbjDf = megaDf.groupby(by=col)['title'].count()\n",
        "  sbjDf = sbjDf.sort_values(ascending=False)\n",
        "\n",
        "  if top10:\n",
        "    sbjDf = sbjDf[:10]\n",
        "\n",
        "  for sbj in sbjDf.keys():\n",
        "    dfs.append(megaDf[megaDf[col] == str(sbj)])\n",
        "  return (dfs,sbjDf.keys().tolist())\n",
        "\n",
        "\n",
        "# Generates a subset of the megaDf\n",
        "# @sub: \n",
        "# 'all' = all papers\n",
        "# 'aff' = split by affiliation (academic, industry, mixed)\n",
        "# 'primary' = primary subject area (top 10)\n",
        "# 'cluster' = clustering subject area (top 10)\n",
        "# 'loc' = continent\n",
        "# 'us-ch' = US and China affiliations\n",
        "# 'bigTech' = Apple, Amazon, Microsoft, Google, Facebook, Huawei\n",
        "def generateSubset(sub):\n",
        "  # subset data\n",
        "  if sub == 'all':\n",
        "    dfList = [megaDf]\n",
        "    names = ['all']\n",
        "  elif sub == 'aff':\n",
        "    acaDf = megaDf.loc[(megaDf['academic'] == 1) & (megaDf['mixed'] == 0)]\n",
        "    indDf = megaDf.loc[(megaDf['industry'] == 1) & (megaDf['mixed'] == 0)]\n",
        "    mixedDf = megaDf[megaDf['mixed'] == 1]\n",
        "    dfList = [acaDf,indDf,mixedDf]\n",
        "    names = ['academic','industry','mixed-affiliation']\n",
        "  elif sub == 'primary':\n",
        "    (dfList,names) = generateSbjDfs('primary subject area')\n",
        "  elif sub == 'cluster':\n",
        "    (dfList,names) = generateSbjDfs('clustering subject preference')\n",
        "  elif sub == 'loc':\n",
        "    (dfList,names) = generateContinentDfs()\n",
        "  elif sub == 'us-ch':\n",
        "    usOnlyDf = megaDf[megaDf['country'] == \"{'USA'}\"]\n",
        "    chOnlyDf = megaDf[megaDf['country'] == \"{'China'}\"]\n",
        "    dfList = [usOnlyDf,chOnlyDf]\n",
        "    names = ['United States','China']\n",
        "  elif sub == 'bigTech':\n",
        "    appleDf = megaDf[megaDf['affiliations'].str.contains('Apple')]#megaDf['apple' in str(megaDf['affiliations']).lower()]\n",
        "    amazonDf = megaDf[megaDf['affiliations'].str.contains('Amazon')]\n",
        "    microsoftDf = megaDf[megaDf['affiliations'].str.contains('Microsoft')]\n",
        "    googleDf = megaDf[megaDf['affiliations'].str.contains('Google')]\n",
        "    facebookDf = megaDf[megaDf['affiliations'].str.contains('Facebook')]\n",
        "    huaweiDf = megaDf[megaDf['affiliations'].str.contains('Huawei')]\n",
        "    dfList = [appleDf, amazonDf, microsoftDf, googleDf, facebookDf, huaweiDf]\n",
        "    names = ['Apple', 'Amazon', 'Microsoft', 'Google', 'Facebook', 'Huawei']\n",
        "  else:\n",
        "      print(\"Subset string not recognized.\")\n",
        "      return\n",
        "\n",
        "  return [dfList, names]\n",
        "\n",
        "\n",
        "# Helper function to make a list of words from a string separated by /\n",
        "# @words: the word string\n",
        "def makeWordList(words):\n",
        "  wL = words.split('/')\n",
        "  return [x.lower().strip() for x in wL]\n",
        "\n",
        "\n",
        "# Make frequency DataFrames with a wordlist\n",
        "# @df: the DataFrame to calculate frequencies for\n",
        "# @wordlist: the wordlist containing the words to calculate frequencies for\n",
        "# @useNeg: whether or not to also use the negated versions of the wordlist \n",
        "def makeFreqDfsWithWordlist(df,wordlist,useNeg=False):\n",
        "  wordFreqDict = {}\n",
        "  stmtFreqDict = {}\n",
        "  \n",
        "  any = '<any>' # any words count here\n",
        "  anyNoNeg = '<any_no_neg>' # only words that aren't negated count here\n",
        "\n",
        "  wordFreqDict[any] = 0\n",
        "  wordFreqDict[anyNoNeg] = 0\n",
        "\n",
        "  stmtFreqDict[any] = 0\n",
        "  stmtFreqDict[anyNoNeg] = 0\n",
        "\n",
        "  countForAnyProps = 0\n",
        "\n",
        "  stopwords = set(STOPWORDS) # using wordcloud stopwords\n",
        "  stopwords.update([' ','','\\n'])\n",
        "  totalLen = 0\n",
        "\n",
        "  wordlist = [x.strip().lower() for x in wordlist] # assumes no punctuation in wordlist\n",
        "  wordlist = [' ' + x + ' ' for x in wordlist] # add padding spaces \n",
        "\n",
        "  # get version with negation tags if necessary\n",
        "  if useNeg:\n",
        "    statements = df['bis_with_neg'].values\n",
        "  else:\n",
        "    statements = df['impact statement'].values\n",
        "\n",
        "  for statement in statements:\n",
        "    componentWords = set()\n",
        "\n",
        "    if useNeg: \n",
        "      words = statement # presplit if using negated\n",
        "    else: \n",
        "      words = statement.split(' ')\n",
        "    totalLen += len(words)\n",
        "\n",
        "    # remove punctuation and go lowercase\n",
        "    # using spaces so can count with regex\n",
        "    words = [x.strip().lower() for x in words]\n",
        "    words = [re.sub(\"[^\\w\\s]\", \" \", x) for x in words]\n",
        "\n",
        "    joinedStmt = ' ' + ' '.join(words) + ' '\n",
        "    \n",
        "    for target in wordlist:\n",
        "      #numOccs = len(re.findall(target,joinedStmt))\n",
        "\n",
        "      strippedTar = target.strip()\n",
        "      numOccs = len(re.findall(f'{strippedTar} ',joinedStmt))\n",
        "\n",
        "      if numOccs > 0:\n",
        "        componentWords.add(strippedTar)\n",
        "        wordFreqDict[any] += numOccs # update <any> row\n",
        "\n",
        "        if '_neg' not in strippedTar: # update <any_no_neg> row if word not negated\n",
        "          wordFreqDict[anyNoNeg] += numOccs\n",
        "\n",
        "        if strippedTar in wordFreqDict:\n",
        "          wordFreqDict[strippedTar] += numOccs \n",
        "        else: \n",
        "          wordFreqDict[strippedTar] = numOccs \n",
        "\n",
        "\n",
        "    # add to number of statements words appear in\n",
        "    if len(componentWords) > 0: # if any of the words have occurred, updated <any> row\n",
        "      stmtFreqDict[any] += 1\n",
        "\n",
        "      compsNoNeg = [x for x in componentWords if '_neg' not in x] # if at least one non-negated word, update <any_no_neg> row\n",
        "      if len(compsNoNeg) > 0:\n",
        "        stmtFreqDict[anyNoNeg] += 1\n",
        "\n",
        "    for word in componentWords: # update statement count\n",
        "      if word in stmtFreqDict:\n",
        "        stmtFreqDict[word] += 1\n",
        "      else: stmtFreqDict[word] = 1\n",
        "\n",
        "  # make dataframes\n",
        "  wordFreqs = pd.DataFrame.from_dict(wordFreqDict,orient='index',columns=['word_freq'])\n",
        "  wordFreqs['word_prop'] = wordFreqs['word_freq']/totalLen\n",
        "  wordFreqs = wordFreqs.sort_values(by='word_prop',ascending=False)\n",
        "\n",
        "  stmtFreqs = pd.DataFrame.from_dict(stmtFreqDict,orient='index',columns=['stmt_freq'])\n",
        "  stmtFreqs['stmt_prop'] = stmtFreqs['stmt_freq']/len(statements)\n",
        "  stmtFreqs = stmtFreqs.sort_values(by='stmt_prop',ascending=False)\n",
        "\n",
        "  # calculate average number of occurrences\n",
        "  avgs = {}\n",
        "  for word in wordFreqDict.keys():\n",
        "    avgs[word] = wordFreqDict[word]/len(df)\n",
        "  avgOccs = pd.DataFrame.from_dict(avgs,orient='index',columns=['avg_occs'])\n",
        "  avgOccs = avgOccs.sort_values(by='avg_occs',ascending=False)\n",
        "\n",
        "  return (wordFreqs, stmtFreqs, avgOccs)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOfW2Ybzfr4r"
      },
      "source": [
        "#### The important functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_t7bG_GffdDx"
      },
      "source": [
        "# Function to print proportions of records containing words and save to csv.\n",
        "# @sub:\n",
        "# all = all papers\n",
        "# aff = split by affiliation\n",
        "# primary = primary subject area (top 10)\n",
        "# cluster = clustering subject area (top 10)\n",
        "# loc = continent\n",
        "# (potentially to come: countries of interest)\n",
        "# @wordlist: list of words to consider\n",
        "# @filename: '<path/to/file.csv>'\n",
        "def getProportions(sub, wordlist, fileName):\n",
        "  dfList = []\n",
        "  names = []\n",
        "\n",
        "  # subset data\n",
        "  if sub == 'all':\n",
        "    dfList = [megaDf]\n",
        "    names = ['all']\n",
        "  elif sub == 'aff':\n",
        "    acaDf = megaDf.loc[(megaDf['academic'] == 1) & (megaDf['mixed'] == 0)]\n",
        "    indDf = megaDf.loc[(megaDf['industry'] == 1) & (megaDf['mixed'] == 0)]\n",
        "    mixedDf = megaDf[megaDf['mixed'] == 1]\n",
        "    dfList = [acaDf,indDf,mixedDf]\n",
        "    names = ['academic','industry','mixed-affiliation']\n",
        "  elif sub == 'primary':\n",
        "    (dfList,names) = generateSbjDfs('primary subject area')\n",
        "  elif sub == 'cluster':\n",
        "    (dfList,names) = generateSbjDfs('clustering subject preference')\n",
        "  elif sub == 'loc':\n",
        "    (dfList,names) = generateContinentDfs()\n",
        "  elif sub =='us-ch':\n",
        "    usOnlyDf = megaDf[megaDf['country'] == \"{'USA'}\"]\n",
        "    chOnlyDf = megaDf[megaDf['country'] == \"{'China'}\"]\n",
        "    dfList = [usOnlyDf,chOnlyDf]\n",
        "    names = ['United States','China']\n",
        "  elif sub == 'bigTech':\n",
        "    appleDf = megaDf[megaDf['affiliations'].str.contains('Apple')]#megaDf['apple' in str(megaDf['affiliations']).lower()]\n",
        "    amazonDf = megaDf[megaDf['affiliations'].str.contains('Amazon')]\n",
        "    microsoftDf = megaDf[megaDf['affiliations'].str.contains('Microsoft')]\n",
        "    googleDf = megaDf[megaDf['affiliations'].str.contains('Google')]\n",
        "    facebookDf = megaDf[megaDf['affiliations'].str.contains('Facebook')]\n",
        "    huaweiDf = megaDf[megaDf['affiliations'].str.contains('Huawei')]\n",
        "    dfList = [appleDf, amazonDf, microsoftDf, googleDf, facebookDf, huaweiDf]\n",
        "    names = ['Apple', 'Amazon', 'Microsoft', 'Google', 'Facebook', 'Huawei']\n",
        "  else:\n",
        "    print(\"Subset string not recognized.\")\n",
        "    return\n",
        "\n",
        "  # calculate proportions and write to csv\n",
        "  bigListForDf = []\n",
        "  for word in wordlist:\n",
        "    word = re.sub(\"[^\\w\\s]\", \"\", word.lower().strip()) \n",
        "    listForDf = []\n",
        "    for df, name in zip(dfList,names):\n",
        "      containsDf = df[df['impact statement'].str.contains(word)]\n",
        "      prop = len(containsDf)/len(df)\n",
        "      listForDf.append([word, name, prop])\n",
        "    bigListForDf = bigListForDf + listForDf\n",
        "  allWordsDf = pd.DataFrame(bigListForDf,columns=['word','category','proportion'])#allWordsDf.append(pd.Series(listForDf,index=allWordsDf.columns),ignore_index=True)\n",
        "  allWordsDf.to_csv(fileName)\n",
        "\n",
        "\n",
        "# Describe citations for a subset of data\n",
        "# @sub: \n",
        "# 'all' = all papers\n",
        "# 'aff' = split by affiliation\n",
        "# 'primary' = primary subject area (top 10)\n",
        "# 'cluster' = clustering subject area (top 10)\n",
        "# 'loc' = continent\n",
        "# 'us-ch' = US and China affiliations\n",
        "# 'bigTech' = Apple, Amazon, Microsoft, Google, Facebook, Huawei\n",
        "def describeCitations(sub):\n",
        "  # subset data\n",
        "  if sub == 'all':\n",
        "    dfList = [megaDf]\n",
        "    names = ['all']\n",
        "  elif sub == 'aff':\n",
        "    acaDf = megaDf.loc[(megaDf['academic'] == 1) & (megaDf['mixed'] == 0)]\n",
        "    indDf = megaDf.loc[(megaDf['industry'] == 1) & (megaDf['mixed'] == 0)]\n",
        "    mixedDf = megaDf[megaDf['mixed'] == 1]\n",
        "    dfList = [acaDf,indDf,mixedDf]\n",
        "    names = ['academic','industry','mixed-affiliation']\n",
        "  elif sub == 'primary':\n",
        "    (dfList,names) = generateSbjDfs('primary subject area')\n",
        "  elif sub == 'cluster':\n",
        "    (dfList,names) = generateSbjDfs('clustering subject preference')\n",
        "  elif sub == 'loc':\n",
        "    (dfList,names) = generateContinentDfs()\n",
        "  elif sub =='us-ch':\n",
        "    usOnlyDf = megaDf[megaDf['country'] == \"{'USA'}\"]\n",
        "    chOnlyDf = megaDf[megaDf['country'] == \"{'China'}\"]\n",
        "    dfList = [usOnlyDf,chOnlyDf]\n",
        "    names = ['United States','China']\n",
        "  elif sub == 'bigTech':\n",
        "    appleDf = megaDf[megaDf['affiliations'].str.contains('Apple')]#megaDf['apple' in str(megaDf['affiliations']).lower()]\n",
        "    amazonDf = megaDf[megaDf['affiliations'].str.contains('Amazon')]\n",
        "    microsoftDf = megaDf[megaDf['affiliations'].str.contains('Microsoft')]\n",
        "    googleDf = megaDf[megaDf['affiliations'].str.contains('Google')]\n",
        "    facebookDf = megaDf[megaDf['affiliations'].str.contains('Facebook')]\n",
        "    huaweiDf = megaDf[megaDf['affiliations'].str.contains('Huawei')]\n",
        "    dfList = [appleDf, amazonDf, microsoftDf, googleDf, facebookDf, huaweiDf]\n",
        "    names = ['Apple', 'Amazon', 'Microsoft', 'Google', 'Facebook', 'Huawei']\n",
        "  else:\n",
        "    print(\"Subset string not recognized.\")\n",
        "    return\n",
        "\n",
        "  for df,name in zip(dfList,names):\n",
        "    print(f\"{name} citations:\\n {df['citation count'].describe()}\")\n",
        "\n",
        "\n",
        "# Calculate the word frequencies for a pregenerated subset and outputs to CSV.\n",
        "# @dfList: a list of dataframes\n",
        "# @names: a list of names for the dataframes in the dfList\n",
        "# @fileName: '<path/to/file.csv>'\n",
        "# @wordList: the list of qords to get frequencies for\n",
        "# @numWords: the max number of word frequencies to output. Defaults to all words.\n",
        "# @useNeg: whether or not to use negated words in the list. Defaults to no. \n",
        "def getFreqsForGeneratedSubset(dfList,names,fileName,wordList,numWords=None,useNeg=False):\n",
        "  wfDfList = []\n",
        "  stDfList = []\n",
        "  avgDfList = []\n",
        "\n",
        "  for df, name in zip(dfList, names):\n",
        "    (wDf,sDf,avgDf) = makeFreqDfsWithWordlist(df,wordList,useNeg)\n",
        "    # All frequencies are subset frequencies--\n",
        "    # so the frequency of occurrence in academic, or South American,\n",
        "    # or NLP, etc statements\n",
        "\n",
        "    wCols = ['category','word','word_freq','word_prop'] \n",
        "    wDf['category'] = name\n",
        "    wDf['word'] = wDf.index\n",
        "    wDf = wDf.reset_index(drop=True)\n",
        "    wDf = wDf[wCols]\n",
        "    \n",
        "    sCols = ['category','word','stmt_freq','stmt_prop'] \n",
        "    sDf['category'] = name\n",
        "    sDf['word'] = sDf.index\n",
        "    sDf = sDf.reset_index(drop=True)\n",
        "    sDf = sDf[sCols]\n",
        "    \n",
        "    avgCols = ['category','word','avg_occs']\n",
        "    avgDf['category'] = name\n",
        "    avgDf['word'] = avgDf.index\n",
        "    avgDf = avgDf.reset_index(drop=True)\n",
        "    avgDf = avgDf[avgCols]\n",
        "\n",
        "    wfDfList.append(wDf[:numWords])\n",
        "    stDfList.append(sDf[:numWords])\n",
        "    avgDfList.append(avgDf[:numWords])\n",
        "\n",
        "  mlWfDf = pd.concat(wfDfList).sort_values(['word','category']).set_index(['word','category'])#.to_csv(totalFreqFile)\n",
        "  mlStDf = pd.concat(stDfList).sort_values(['word','category']).set_index(['word','category'])\n",
        "  mlAvgDf = pd.concat(avgDfList).sort_values(['word','category']).set_index(['word','category'])#.to_csv('avg.csv')\n",
        "\n",
        "  megaMerge = mlWfDf.merge(mlStDf,how='outer',on=['word','category'])\n",
        "  megaMerge = megaMerge.merge(mlAvgDf,how='outer',on=['word','category'])\n",
        "  megaMerge.columns = ['total word frequency (count)','total word frequency (proportion)',\n",
        "                       'statement frequency (count)', 'statement frequency (proportion)',\n",
        "                       'average number of occurrences']\n",
        "  megaMerge.to_csv(fileName)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttwQjMzujfe3"
      },
      "source": [
        "### Generating the big database of frequencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249
        },
        "id": "5Mf7U4J4ZV1M",
        "outputId": "d722f4f4-a197-480c-cf61-0eb423de05b7"
      },
      "source": [
        "# Generate big database for standard wordlists with negations\n",
        "\n",
        "fileBase = 'BFD_norm_2'\n",
        "with open('/content/drive/My Drive/NeurIPS_BIS_Analysis/BIS_keywords.txt','r') as listFile:\n",
        "  wordLists = listFile.read().split('\\n')\n",
        "\n",
        "wordLists = [x for x in wordLists if x != '']\n",
        "wordLists = [makeWordList(x) for x in wordLists]\n",
        "for l in wordLists:\n",
        "  l_with_neg = [x + '_neg' for x in l]\n",
        "  l.extend(l_with_neg)\n",
        "\n",
        "idx = 1\n",
        "\n",
        "for wl in wordLists:\n",
        "  \n",
        "  for ind, zipped in enumerate(splitList):\n",
        "    if idx%20 == 0: print(f'working on {idx}')\n",
        "    fileName = f'./{fileBase}/{idx + 1000}_{str(wl[:3])}_{ind}.csv'\n",
        "    \n",
        "    df = zipped[0]\n",
        "    names = zipped[1]\n",
        "\n",
        "    getFreqsForGeneratedSubset(df,names,fileName,wl,useNeg=True)\n",
        "    idx += 1\n",
        "print('Completed.')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-dffa0ddc4031>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#fileBase = 'BFD_norm_2'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/NeurIPS_BIS_Analysis/BIS_keywords.txt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mlistFile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0mwordLists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlistFile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/NeurIPS_BIS_Analysis/BIS_keywords.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "prYYiCHlzKx9",
        "outputId": "46ce5278-d75e-4dab-a01d-4a948356384e"
      },
      "source": [
        "# Zip it up, then turn into one excel file in separate file\n",
        "shutil.make_archive(fileBase, 'zip', fileBase)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/BFD_norm_2.zip'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ye2B09n8digA",
        "outputId": "65e85d9e-3937-473b-e84b-9c66bbf10519"
      },
      "source": [
        "# Generate big database for synonym wordlists\n",
        "# For each list of words, generate for each cluster:\n",
        "# Total frequency\n",
        "# Statement frequency\n",
        "# Average number of occurrences \n",
        "fileRoot = 'BFD_exp_5-8'\n",
        "with open('/content/drive/My Drive/NeurIPS_BIS_Analysis/synonym_list.txt','r') as listFile:\n",
        "  wordLists = listFile.read().split('\\n')\n",
        "\n",
        "wordLists = [x for x in wordLists if x != '']\n",
        "wordLists = [makeWordList(x) for x in wordLists]\n",
        "for l in wordLists:\n",
        "  l_with_neg = [x + '_neg' for x in l]\n",
        "  l.extend(l_with_neg)\n",
        "\n",
        "idx = 1\n",
        "\n",
        "for wl in wordLists:\n",
        "  for ind, zipped in enumerate(splitList):\n",
        "    if idx%20 == 0: print(f'working on {idx}')\n",
        "    fileName = f'./{fileRoot}/{idx + 1000}_{str(wl[:3])}_{ind}.csv'\n",
        "\n",
        "    df = zipped[0]\n",
        "    names = zipped[1]\n",
        "\n",
        "    getFreqsForGeneratedSubset(df,names,fileName,wl,useNeg=True)\n",
        "    idx += 1\n",
        "print('Completed.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "working on 20\n",
            "Completed.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "6EHYD_XdzGzK",
        "outputId": "98acd14d-942f-4077-c496-f6be473af538"
      },
      "source": [
        "# zip it all up\n",
        "# Conversion to excel took place in separate file\n",
        "shutil.make_archive(fileRoot, 'zip', fileRoot)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/BFD_exp_5-8.zip'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zywj8nKxE6xI"
      },
      "source": [
        "### Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYZEF2ywtQct"
      },
      "source": [
        "# Most common affiliations\n",
        "affs = megaDf['affiliations']\n",
        "regex = r\"'\\w*\\s*\\w*'\"\n",
        "affDict = {}\n",
        "\n",
        "for row in affs:\n",
        "  matches = re.findall(regex,row)\n",
        "  for match in matches:\n",
        "    match = match.lower()[1:-1]\n",
        "    if match in affDict:\n",
        "      affDict[match] += 1\n",
        "    else:\n",
        "      affDict[match] = 1\n",
        "\n",
        "toPop = []\n",
        "for k,v in affDict.items():\n",
        "  if 'university' in k and k != 'shanghaitech university': # find and combine universities\n",
        "    for k1,v1 in affDict.items(): # find prefix\n",
        "      if k1 in k and k != k1:\n",
        "        affDict[k] += v1\n",
        "        toPop.append(k1)\n",
        "        break\n",
        "\n",
        "# manual cleaning\n",
        "affDict['deepmind'] += affDict['google deepmind']\n",
        "toPop.append('google deepmind')\n",
        "affDict['element ai'] += affDict['elementai']\n",
        "toPop.append('elementai')\n",
        "affDict['eth zürich'] += affDict['eth zurich']\n",
        "toPop.append('eth zurich')\n",
        "affDict['facebook'] += affDict['facebook inc']\n",
        "toPop.append('facebook inc')\n",
        "affDict['facebook'] += affDict['facebook research']\n",
        "toPop.append('facebook research')\n",
        "affDict['facebook'] += affDict['facebook ai']\n",
        "toPop.append('facebook ai')\n",
        "affDict['facebook'] += affDict['fair']\n",
        "toPop.append('fair')\n",
        "affDict['google'] += affDict['google inc']\n",
        "toPop.append('google inc')\n",
        "affDict['google'] += affDict['google llc']\n",
        "toPop.append('google llc')\n",
        "affDict['google'] += affDict['google research']\n",
        "toPop.append('google research')\n",
        "affDict['google'] += affDict['google brain']\n",
        "toPop.append('google brain')\n",
        "affDict['google'] += affDict['google ai']\n",
        "toPop.append('google ai')\n",
        "affDict['google'] += affDict['google health']\n",
        "toPop.append('google health')\n",
        "affDict['amazon'] += affDict['amazon aws']\n",
        "toPop.append('amazon aws')\n",
        "affDict['huawei'] += affDict['huawei technologies']\n",
        "toPop.append('huawei technologies')\n",
        "affDict['huawei'] += affDict['huawei noah']\n",
        "toPop.append('huawei noah')\n",
        "affDict['ibm'] += affDict['ibm corp']\n",
        "toPop.append('ibm corp')\n",
        "affDict['ibm'] += affDict['ibm research']\n",
        "toPop.append('ibm research')\n",
        "affDict['salesforce'] += affDict['salesforce research']\n",
        "toPop.append('salesforce research')\n",
        "affDict['intel'] += affDict['intel corporation']\n",
        "toPop.append('intel corporation')\n",
        "affDict['jp morgan'] += affDict['jpmorgan']\n",
        "toPop.append('jpmorgan')\n",
        "affDict['linkedin'] += affDict['linkedin corporation']\n",
        "toPop.append('linkedin corporation')\n",
        "affDict['megvii'] += affDict['megvii technology']\n",
        "toPop.append('megvii technology')\n",
        "affDict['microsoft'] += affDict['microsoft corporation']\n",
        "toPop.append('microsoft corporation')\n",
        "affDict['microsoft'] += affDict['microsoft research']\n",
        "toPop.append('microsoft research')\n",
        "affDict['baidu'] += affDict['baidu research']\n",
        "toPop.append('baidu research')\n",
        "affDict['adobe'] += affDict['adobe research']\n",
        "toPop.append('adobe research')\n",
        "affDict['amazon'] += affDict['amazon research']\n",
        "toPop.append('amazon research')\n",
        "affDict['vinai'] += affDict['vinai research']\n",
        "toPop.append('vinai research')\n",
        "affDict['sensetime'] += affDict['sensetime research']\n",
        "toPop.append('sensetime research')\n",
        "affDict['samsung'] += affDict['samsung research']\n",
        "toPop.append('samsung research')\n",
        "affDict['samsung'] += affDict['samsung sds']\n",
        "toPop.append('samsung sds')\n",
        "affDict['sas institute'] += affDict['sas']\n",
        "toPop.append('sas')\n",
        "affDict['mit'] += affDict['mit ']\n",
        "toPop.append('mit ')\n",
        "affDict['éts montréal'] += affDict['ets montreal']\n",
        "toPop.append('ets montreal')\n",
        "affDict['nvidia'] += affDict['nvidia corporation']\n",
        "toPop.append('nvidia corporation')\n",
        "affDict['nvidia'] += affDict['nvidia research']\n",
        "toPop.append('nvidia research')\n",
        "affDict['nnaisense'] += affDict['nnaisense sa']\n",
        "toPop.append('nnaisense sa')\n",
        "affDict['intel labs'] += affDict['intel']\n",
        "toPop.append('intel')\n",
        "affDict['cmu'] += affDict['carnegie']\n",
        "toPop.append('carnegie')\n",
        "affDict['radboud university'] += affDict['radboud universiteit']\n",
        "toPop.append('radboud universiteit')\n",
        "affDict['riken'] += affDict['riken aip']\n",
        "toPop.append('riken aip')\n",
        "affDict['eth zürich'] += affDict['ethz']\n",
        "toPop.append('ethz')\n",
        "affDict['sungkyunkwan university'] += affDict['sunkyunkwan university']\n",
        "toPop.append('sunkyunkwan university')\n",
        "affDict['uber'] += affDict['uber atg']\n",
        "toPop.append('uber atg')\n",
        "affDict['georgia tech'] += affDict['gatech']\n",
        "toPop.append('gatech')\n",
        "affDict['telecom paristech'] += affDict['telecom paristec']\n",
        "toPop.append('telecom paristec')\n",
        "affDict['tsinghua university'] += affDict['tsinghua univeristy']\n",
        "toPop.append('tsinghua univeristy')\n",
        "affDict['tsinghua university'] += affDict['tsinghua univiersity']\n",
        "toPop.append('tsinghua univiersity')\n",
        "affDict['oxford university'] += affDict['u oxford']\n",
        "toPop.append('u oxford')\n",
        "affDict['uw madison'] += affDict['uw']\n",
        "toPop.append('uw')\n",
        "affDict['waymo'] += affDict['waymo llc']\n",
        "toPop.append('waymo llc')\n",
        "affDict['weizmann institute'] += affDict['weizmann']\n",
        "toPop.append('weizmann')\n",
        "affDict['weizmann institute'] += affDict['institute weizmann']\n",
        "toPop.append('institute weizmann')\n",
        "affDict['yale university'] += affDict['yale univ']\n",
        "toPop.append('yale univ')\n",
        "\n",
        "for p in toPop:\n",
        "  affDict.pop(p)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5NHMfYjxAyTL"
      },
      "source": [
        "pd.DataFrame.from_dict(affDict,orient='index').to_csv('aff_freqs_updated_3-8.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFSgPWVfD8n7"
      },
      "source": [
        "# Country frequencies\n",
        "countries = megaDf['country']\n",
        "countries\n",
        "countryDict = {}\n",
        "\n",
        "for row in countries:\n",
        "  matches = re.findall(regex,row)\n",
        "  for match in matches:\n",
        "    match = match.lower()[1:-1]\n",
        "    if match in countryDict:\n",
        "      countryDict[match] += 1\n",
        "    else:\n",
        "      countryDict[match] = 1\n",
        "\n",
        "pd.DataFrame.from_dict(countryDict,orient='index').to_csv('country_freqs.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lsm2SzA39Yd1"
      },
      "source": [
        "# Make list of DataFrames and names for a split\n",
        "# @sub: \n",
        "# 'all' = all papers\n",
        "# 'aff' = split by affiliation\n",
        "# 'primary' = primary subject area (top 10)\n",
        "# 'cluster' = clustering subject area (top 10)\n",
        "# 'loc' = continent\n",
        "# 'us-ch' = US and China affiliations\n",
        "# 'bigTech' = Apple, Amazon, Microsoft, Google, Facebook, Huawei\n",
        "# Returns: list of DataFrames and list of names corresponding to each DataFrame\n",
        "def makeDfandNames(sub):\n",
        "  dfList = []\n",
        "  names = []\n",
        "\n",
        "  if sub == 'all':\n",
        "    dfList = [megaDf]\n",
        "    names = ['all']\n",
        "  elif sub == 'aff':\n",
        "    acaDf = megaDf.loc[(megaDf['academic'] == 1) & (megaDf['mixed'] == 0)]\n",
        "    indDf = megaDf.loc[(megaDf['industry'] == 1) & (megaDf['mixed'] == 0)]\n",
        "    mixedDf = megaDf[megaDf['mixed'] == 1]\n",
        "    dfList = [acaDf,indDf,mixedDf]\n",
        "    names = ['academic','industry','mixed-affiliation']\n",
        "  elif sub == 'primary':\n",
        "    (dfList,names) = generateSbjDfs('primary subject area')\n",
        "  elif sub == 'cluster':\n",
        "    (dfList,names) = generateSbjDfs('clustering subject preference')\n",
        "  elif sub == 'loc':\n",
        "    (dfList,names) = generateContinentDfs()\n",
        "  elif sub == 'us-ch':\n",
        "    usOnlyDf = megaDf[megaDf['country'] == \"{'USA'}\"]\n",
        "    chOnlyDf = megaDf[megaDf['country'] == \"{'China'}\"]\n",
        "    dfList = [usOnlyDf,chOnlyDf]\n",
        "    names = ['United States','China']\n",
        "  elif sub == 'bigTech':\n",
        "    appleDf = megaDf[megaDf['affiliations'].str.contains('Apple')]\n",
        "    amazonDf = megaDf[megaDf['affiliations'].str.contains('Amazon')]\n",
        "    microsoftDf = megaDf[megaDf['affiliations'].str.contains('Microsoft')]\n",
        "    googleDf = megaDf[megaDf['affiliations'].str.contains('Google')]\n",
        "    facebookDf = megaDf[megaDf['affiliations'].str.contains('Facebook')]\n",
        "    huaweiDf = megaDf[megaDf['affiliations'].str.contains('Huawei')]\n",
        "    dfList = [appleDf, amazonDf, microsoftDf, googleDf, facebookDf, huaweiDf]\n",
        "    names = ['Apple', 'Amazon', 'Microsoft', 'Google', 'Facebook', 'Huawei']\n",
        "  else:\n",
        "      print(\"Subset string not recognized.\")\n",
        "      return\n",
        "  \n",
        "  return (dfList, names)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YwCBoYgENzDU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "c8153390-b2cb-4b4a-8717-fdc18f0f4228"
      },
      "source": [
        "# Generate descriptive stats\n",
        "for sub in ['cluster','primary']:\n",
        "  (dfs, names) = makeDfandNames(sub)\n",
        "  fullDf = pd.concat(dfs)\n",
        "  if sub != 'all':\n",
        "    dfs.insert(0,fullDf)\n",
        "\n",
        "    names.insert(0,'<full set>')\n",
        "  listForDf = []\n",
        "\n",
        "  for (df,name) in zip(dfs,names):\n",
        "    numStmts = len(df)\n",
        "    pctTot = numStmts/1898\n",
        "    pctSplit = numStmts/len(fullDf)\n",
        "    avgWc = df['word count'].mean()\n",
        "    medWc = df['word count'].median()\n",
        "    maxWc = df['word count'].max()\n",
        "    minWc = df['word count'].min()\n",
        "\n",
        "    avgSc = df['sentence count'].mean()\n",
        "    medSc = df['sentence count'].median()\n",
        "    maxSc = df['sentence count'].max()\n",
        "    minSc = df['sentence count'].min()\n",
        "\n",
        "    numOptOut = len(df[df['opt out']=='TRUE'])\n",
        "    numNoOptOut = len(df[df['opt out']=='FALSE'])\n",
        "    unOptOut = numStmts - numOptOut - numNoOptOut\n",
        "    if numOptOut + numNoOptOut != 0: optOutMProp = numOptOut/(numOptOut + numNoOptOut)\n",
        "    else: optOutMProp = float('nan')\n",
        "    optOutTProp = numOptOut/numStmts\n",
        "\n",
        "    numAmbOptOut = len(df[df['ambiguous opt out']=='TRUE'])\n",
        "    numNoAmbOptOut = len(df[df['ambiguous opt out']=='FALSE'])\n",
        "    unAmbOptOut = numStmts - numNoAmbOptOut - numAmbOptOut\n",
        "    if numAmbOptOut + numNoAmbOptOut != 0: ambOptOutMProp = numAmbOptOut/(numAmbOptOut + numNoAmbOptOut)\n",
        "    else: ambOptOutMProp = float('nan')\n",
        "    ambOptOutTProp = numAmbOptOut/numStmts\n",
        "\n",
        "    listForDf.append([sub,name,numStmts,pctTot,pctSplit,avgWc,medWc,maxWc,minWc,\n",
        "                      avgSc,medSc,maxSc,minSc,\n",
        "                      numOptOut,numOptOut+numNoOptOut,unOptOut,\n",
        "                      optOutMProp,optOutTProp,\n",
        "                      numAmbOptOut,numAmbOptOut+numNoAmbOptOut,unAmbOptOut,\n",
        "                      ambOptOutMProp,ambOptOutTProp])\n",
        "\n",
        "\n",
        "  cols = ['split','subset','statement count','proportion of total','proportion of split','average word count',\n",
        "          'median word count','max word count','min word count',\n",
        "          'average sentence count','median sentence count','max sentence count','min sentence count',\n",
        "          'explicit opt out count', 'marked explicit opt out count', 'unmarked opt out count', \n",
        "          'explicit opt out (proportion of marked)','explicit opt out (proportion of total)',\n",
        "          'ambiguous opt out count','marked ambiguous opt out count', 'unmarked ambiguous opt out count',\n",
        "          'ambiguous opt out (proportion of marked)','ambiguous opt out (proportion of total)']\n",
        "  subDf = pd.DataFrame.from_records(listForDf,columns=cols)\n",
        "  subDf.to_csv(f'./descriptive_stats/{sub}_descr.csv')\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-b130bd575d55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Generate descriptive stats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msub\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'cluster'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'primary'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0;34m(\u001b[0m\u001b[0mdfs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmakeDfandNames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mfullDf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0msub\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'all'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'makeDfandNames' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "unm5vab7YawZ",
        "outputId": "041dc71c-f1c7-45af-e37f-abb322411ffb"
      },
      "source": [
        "shutil.make_archive('descriptive_stats', 'zip', 'descriptive_stats')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/descriptive_stats.zip'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWvCqu9VDNVc"
      },
      "source": [
        "# Creates CSV with top numWords words, excluding stopwords\n",
        "def getTopWords(numWords, stopwords, fileName, col='impact statement'):\n",
        "  for sub in splits:\n",
        "    (dfs, names) = makeDfandNames(sub)\n",
        "    fullDf = pd.concat(dfs)\n",
        "    if col != 'impact statement': useList = True\n",
        "    else: useList = False\n",
        "\n",
        "    if sub != 'all':\n",
        "      dfs.insert(0,fullDf)\n",
        "      names.insert(0,'<full set>')\n",
        "\n",
        "    listForDf = []\n",
        "\n",
        "    for (df,name) in zip(dfs,names):\n",
        "      # concatenate all statements, split into words, and get most common\n",
        "      if not useList: \n",
        "        statements = ' '.join(statement for statement in df[col])\n",
        "        split = statements.split(' ')\n",
        "      else: \n",
        "        split = ' '.join(' '.join(statement) for statement in df[col])\n",
        "      split = [x.translate(str.maketrans('', '', string.punctuation)).lower().strip() for x in split]\n",
        "\n",
        "      words = [x for x in split if x not in stopwords] # exclude stopwords\n",
        "      count = Counter(words)\n",
        "      top = count.most_common(numWords)\n",
        "      listForDf.append([sub,name,top])\n",
        "\n",
        "    cols = ['split','subset','words']\n",
        "    topWordsDf = pd.DataFrame.from_records(listForDf,columns=cols)\n",
        "    topWordsDf.to_csv(f'./top_words/{sub}_{fileName}.csv')\n",
        "\n",
        "def create_human_readable(array_tuple):\n",
        "    d1 = {}\n",
        "    for i in array_tuple:\n",
        "        d1[i[0]] = i[1]\n",
        "    df = pd.DataFrame(d1.items(), columns=['Word', 'Count'])\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03ZoN1fAGiaV"
      },
      "source": [
        "stops = set(STOPWORDS)\n",
        "stops.update(['the','we','may','in','this','our','','\\n'])\n",
        "getTopWords(20,stops,'top_20_regStops')\n",
        "\n",
        "getTopWords(40,stops,'top_40_regStops')\n",
        "\n",
        "techStops = stops.update(['algorithm','algorithms','model','data','models','neural','system','systems'])\n",
        "getTopWords(20,stops,'top_20_techStops')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWLpjzUtR1GN"
      },
      "source": [
        "stops = set(STOPWORDS)\n",
        "stops.update(['the','we','may','in','this','our','','\\n'])\n",
        "getTopWords(500,stops,'top_500_regStops_with_double_negs','bis_with_neg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iz8pWGB38_-R"
      },
      "source": [
        "stops = set(STOPWORDS)\n",
        "stops.update(['the','we','may','in','this','our','','\\n'])\n",
        "getTopWords(500,stops,'top_500_regStops_with_negs','bis_with_neg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Pzk_TmWYJOX6",
        "outputId": "ed2ba676-f639-4ae2-83df-d78a5f898bc7"
      },
      "source": [
        "shutil.make_archive('top_words', 'zip', 'top_words')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/top_words.zip'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHVBpmntzDEw"
      },
      "source": [
        "###Academic vs Industry"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFmNsAaICBLX"
      },
      "source": [
        "acaDf = megaDf.loc[(megaDf['academic'] == 1) & (megaDf['mixed'] == 0)]\n",
        "indDf = megaDf.loc[(megaDf['industry'] == 1) & (megaDf['mixed'] == 0)]\n",
        "mixedDf = megaDf[megaDf['mixed'] == 1]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "23DO97x0CBLa",
        "outputId": "90f45c33-1334-4852-f8bb-e463e41c95d6"
      },
      "source": [
        "acaWC = acaDf['word count'].mean()\n",
        "acaSC = acaDf['sentence count'].mean()\n",
        "acaWM = acaDf['word count'].median()\n",
        "acaSM = acaDf['sentence count'].median()\n",
        "\n",
        "print(\"Word count:\")\n",
        "display(acaDf['word count'].describe())\n",
        "print(\"Sentence count:\")\n",
        "display(acaDf['sentence count'].describe())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word count:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "count    1163.000000\n",
              "mean      161.680997\n",
              "std       118.719620\n",
              "min         2.000000\n",
              "25%        81.000000\n",
              "50%       132.000000\n",
              "75%       217.000000\n",
              "max       800.000000\n",
              "Name: word count, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Sentence count:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "count    1163.000000\n",
              "mean        7.665520\n",
              "std         5.755405\n",
              "min         1.000000\n",
              "25%         4.000000\n",
              "50%         6.000000\n",
              "75%        10.000000\n",
              "max        49.000000\n",
              "Name: sentence count, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "B9aX3ECLCBLb",
        "outputId": "91aed2c0-e7a4-4424-eabc-cf5e91eb58e1"
      },
      "source": [
        "indWC = indDf['word count'].mean()\n",
        "indSC = indDf['sentence count'].mean()\n",
        "indWM = acaDf['word count'].median()\n",
        "indSM = acaDf['sentence count'].median()\n",
        "\n",
        "print(\"Word count:\")\n",
        "display(indDf['word count'].describe())\n",
        "print(\"Sentence count:\")\n",
        "display(indDf['sentence count'].describe())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word count:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "count    122.000000\n",
              "mean     187.139344\n",
              "std      142.700479\n",
              "min        5.000000\n",
              "25%       90.250000\n",
              "50%      140.000000\n",
              "75%      242.000000\n",
              "max      730.000000\n",
              "Name: word count, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Sentence count:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "count    122.000000\n",
              "mean       8.680328\n",
              "std        6.685703\n",
              "min        1.000000\n",
              "25%        4.000000\n",
              "50%        7.000000\n",
              "75%       12.000000\n",
              "max       38.000000\n",
              "Name: sentence count, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "ZM2_ff-8CBLb",
        "outputId": "a36224e8-db5c-48b4-cceb-80f6cc2c9f7e"
      },
      "source": [
        "mixWC = mixedDf['word count'].mean()\n",
        "mixSC = mixedDf['sentence count'].mean()\n",
        "mixWM = mixedDf['word count'].median()\n",
        "mixSM = mixedDf['sentence count'].median()\n",
        "\n",
        "print(\"Word count:\")\n",
        "display(mixedDf['word count'].describe())\n",
        "print(\"Sentence count:\")\n",
        "display(mixedDf['sentence count'].describe())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word count:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "count     613.000000\n",
              "mean      178.683524\n",
              "std       209.075191\n",
              "min         5.000000\n",
              "25%        88.000000\n",
              "50%       142.000000\n",
              "75%       227.000000\n",
              "max      4337.000000\n",
              "Name: word count, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Sentence count:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "count    613.000000\n",
              "mean       8.797716\n",
              "std       11.376426\n",
              "min        1.000000\n",
              "25%        4.000000\n",
              "50%        7.000000\n",
              "75%       11.000000\n",
              "max      241.000000\n",
              "Name: sentence count, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wbg5E0VTzMBx"
      },
      "source": [
        "### US vs China"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "-pDOqUrxCBLc",
        "outputId": "ea13e504-8332-4c87-8269-0dc052032698"
      },
      "source": [
        "# Includes all papers that have some American affiliation\n",
        "usaDf = megaDf[megaDf['country'].str.contains('USA')]\n",
        "print(\"Word count:\")\n",
        "display(usaDf['word count'].describe())\n",
        "print(\"Sentence count:\")\n",
        "display(usaDf['sentence count'].describe())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word count:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "count    1229.000000\n",
              "mean      171.765663\n",
              "std       170.871689\n",
              "min         2.000000\n",
              "25%        84.000000\n",
              "50%       138.000000\n",
              "75%       220.000000\n",
              "max      4337.000000\n",
              "Name: word count, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Sentence count:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "count    1229.000000\n",
              "mean        8.161920\n",
              "std         8.941211\n",
              "min         1.000000\n",
              "25%         4.000000\n",
              "50%         6.000000\n",
              "75%        11.000000\n",
              "max       241.000000\n",
              "Name: sentence count, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "CPUqZml1CBLd",
        "outputId": "3217e68d-8a92-4ec0-8068-0047d57dbfa7"
      },
      "source": [
        "# America-only affiliation\n",
        "usaOnlyDf = megaDf[megaDf['country'] == \"{'USA'}\"]\n",
        "usaOnlyDf\n",
        "print(\"Word count:\")\n",
        "display(usaOnlyDf['word count'].describe())\n",
        "print(\"Sentence count:\")\n",
        "display(usaOnlyDf['sentence count'].describe())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word count:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "count     795.000000\n",
              "mean      182.262893\n",
              "std       196.001671\n",
              "min         5.000000\n",
              "25%        89.000000\n",
              "50%       143.000000\n",
              "75%       233.000000\n",
              "max      4337.000000\n",
              "Name: word count, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Sentence count:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "count    795.00000\n",
              "mean       8.50566\n",
              "std       10.28772\n",
              "min        1.00000\n",
              "25%        4.00000\n",
              "50%        6.00000\n",
              "75%       11.00000\n",
              "max      241.00000\n",
              "Name: sentence count, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "XkZIb0GWCBLd",
        "outputId": "b4f06e8b-c0e4-46a3-efe9-e990d4f6cf13"
      },
      "source": [
        "chinaOnlyDf = megaDf[megaDf['country'] == \"{'China'}\"]\n",
        "chinaOnlyDf\n",
        "print(\"Word count:\")\n",
        "display(chinaOnlyDf['word count'].describe())\n",
        "print(\"Sentence count:\")\n",
        "display(chinaOnlyDf['sentence count'].describe())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word count:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "count     97.000000\n",
              "mean     135.608247\n",
              "std       97.296856\n",
              "min        9.000000\n",
              "25%       72.000000\n",
              "50%      122.000000\n",
              "75%      163.000000\n",
              "max      576.000000\n",
              "Name: word count, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Sentence count:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "count    97.000000\n",
              "mean      7.546392\n",
              "std       5.879025\n",
              "min       1.000000\n",
              "25%       4.000000\n",
              "50%       7.000000\n",
              "75%      10.000000\n",
              "max      38.000000\n",
              "Name: sentence count, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMYR2CJv9Naj"
      },
      "source": [
        "### Keywords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18BX1YW6_YWk"
      },
      "source": [
        "# Prints academic/industry/mixed statistics for individual word\n",
        "# @word: the word to consider\n",
        "def statsForWord(word):\n",
        "  megWordDf = megaDf[megaDf['impact statement'].str.contains(word)]\n",
        "  print(f\"proportion of all statements containing '{word}': {len(megWordDf)/len(megaDf)}\")\n",
        "  acaWordDf = acaDf[acaDf['impact statement'].str.contains(word)]\n",
        "  print(f\"proportion of academic statements containing '{word}': {len(acaWordDf)/len(acaDf)}\")\n",
        "  indWordDf = indDf[indDf['impact statement'].str.contains(word)]\n",
        "  print(f\"proportion of industry statements containing '{word}': {len(indWordDf)/len(indDf)}\")\n",
        "  mixWordDf = mixedDf[mixedDf['impact statement'].str.contains(word)]\n",
        "  print(f\"proportion of mixed statements containing '{word}': {len(mixWordDf)/len(mixedDf)}\")\n",
        "  \n",
        "  print(f\"All '{word}' statements means:\")\n",
        "  print(f\"word count: {megWordDf['word count'].mean()}\")\n",
        "  print(f\"sentence count: {megWordDf['sentence count'].mean()}\")\n",
        "\n",
        "  print(f\"Academic '{word}' statements means:\")\n",
        "  print(f\"word count: {acaWordDf['word count'].mean()}\")\n",
        "  print(f\"sentence count: {acaWordDf['sentence count'].mean()}\")\n",
        "\n",
        "  print(f\"Industry '{word}' statements means:\")\n",
        "  print(f\"word count: {indWordDf['word count'].mean()}\")\n",
        "  print(f\"sentence count: {indWordDf['sentence count'].mean()}\")\n",
        "\n",
        "  print(f\"Mixed '{word}' statements means:\")\n",
        "  print(f\"word count: {mixWordDf['word count'].mean()}\")\n",
        "  print(f\"sentence count: {mixWordDf['sentence count'].mean()}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FknschcJ6QLF",
        "outputId": "227c2a4c-a3d4-49f1-88ff-ceada7b136cc"
      },
      "source": [
        "# benefit\n",
        "statsForWord('benefit')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "proportion of all statements containing benefit: 0.23182297154899895\n",
            "proportion of academic statements containing benefit: 0.22785898538263114\n",
            "proportion of industry statements containing benefit: 0.22950819672131148\n",
            "proportion of mixed statements containing benefit: 0.2398042414355628\n",
            "All 'benefit' statements means:\n",
            "word count: 214.05454545454546\n",
            "sentence count: 10.377272727272727\n",
            "Academic 'benefit' statements means:\n",
            "word count: 214.2377358490566\n",
            "sentence count: 10.211320754716981\n",
            "Industry 'benefit' statements means:\n",
            "word count: 224.64285714285714\n",
            "sentence count: 10.321428571428571\n",
            "Mixed 'benefit' statements means:\n",
            "word count: 211.7074829931973\n",
            "sentence count: 10.687074829931973\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_mjr5Wg9b0U",
        "outputId": "05366b3b-4c7e-4196-effc-03e82200a5e3"
      },
      "source": [
        "statsForWord('method')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "proportion of all statements containing method: 0.4315068493150685\n",
            "proportion of academic statements containing method: 0.40498710232158214\n",
            "proportion of industry statements containing method: 0.45901639344262296\n",
            "proportion of mixed statements containing method: 0.4763458401305057\n",
            "All 'method' statements means:\n",
            "word count: 206.07570207570208\n",
            "sentence count: 9.87912087912088\n",
            "Academic 'method' statements means:\n",
            "word count: 197.21868365180467\n",
            "sentence count: 9.250530785562633\n",
            "Industry 'method' statements means:\n",
            "word count: 232.85714285714286\n",
            "sentence count: 10.928571428571429\n",
            "Mixed 'method' statements means:\n",
            "word count: 215.22602739726028\n",
            "sentence count: 10.691780821917808\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5Lss9JKlYKs"
      },
      "source": [
        "### Clustering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sOwi-JDQ946G",
        "outputId": "4a696af9-e622-4212-9aca-603e662bff9c"
      },
      "source": [
        "# Poking around the subject areas\n",
        "sbjDf = megaDf.groupby(by='primary subject area')['title'].count()\n",
        "sbjDf = sbjDf.sort_values(ascending=False)\n",
        "top10 = sbjDf[:10]\n",
        "\n",
        "top10"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "primary subject area\n",
              "Applications -> Computer Vision                                  96\n",
              "Deep Learning                                                    63\n",
              "Reinforcement Learning and Planning -> Reinforcement Learning    57\n",
              "Reinforcement Learning and Planning                              57\n",
              "Deep Learning -> Analysis and Understanding of Deep Networks     56\n",
              "Deep Learning -> Generative Models                               54\n",
              "Algorithms -> Representation Learning                            50\n",
              "Theory -> Statistical Learning Theory                            48\n",
              "Algorithms -> Bandit Algorithms                                  40\n",
              "Applications -> Natural Language Processing                      35\n",
              "Name: title, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 146
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBWErF_NnEBT"
      },
      "source": [
        "# Make a word could for a dataframe\n",
        "# @df: the dataframe to look at\n",
        "# @filename: the root of the file to output image to (outputs to <filename>.png)\n",
        "# @stopwords: the words to exclude\n",
        "def makeWordCloud(df, filename, stopwords=STOPWORDS):\n",
        "  text = ' '.join(statement for statement in df['impact statement'])\n",
        "  wc = WordCloud(stopwords = stopwords, background_color='white').generate(text=text)\n",
        "  plt.imshow(wc, interpolation='bilinear')\n",
        "  plt.axis('off')\n",
        "  plt.show()\n",
        "  wc.to_file(f'{filename}.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfWk1anFuEAJ"
      },
      "source": [
        "##Top Words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wb9Q9_pQuIom"
      },
      "source": [
        "### Geography\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5S63eMtiuTXj",
        "outputId": "b35ac719-aab4-4cbf-b639-fa8e1d9d233c"
      },
      "source": [
        "# Poking around countries/continents\n",
        "regex = r\"'(\\w*\\s*\\w*)'\"\n",
        "countrySet = set()\n",
        "\n",
        "countries = megaDf['country']\n",
        "for row in countries:\n",
        "  matches = re.findall(regex,row)\n",
        "  for match in matches:\n",
        "    countrySet.add(match)\n",
        "print(\"set\")\n",
        "countrySet\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "set\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Australia',\n",
              " 'Austria',\n",
              " 'Belgium',\n",
              " 'Brazil',\n",
              " 'Canada',\n",
              " 'Chile',\n",
              " 'China',\n",
              " 'Cyprus',\n",
              " 'Czech Republic',\n",
              " 'Denmark',\n",
              " 'Egypt',\n",
              " 'Finland',\n",
              " 'France',\n",
              " 'Germany',\n",
              " 'Greece',\n",
              " 'Hong Kong',\n",
              " 'Independent',\n",
              " 'India',\n",
              " 'Iran',\n",
              " 'Israel',\n",
              " 'Italy',\n",
              " 'Japan',\n",
              " 'Malaysia',\n",
              " 'Netherlands',\n",
              " 'Norway',\n",
              " 'Pakistan',\n",
              " 'Poland',\n",
              " 'Portugal',\n",
              " 'Qatar',\n",
              " 'Romania',\n",
              " 'Russia',\n",
              " 'Saudi Arabia',\n",
              " 'Singapore',\n",
              " 'South Africa',\n",
              " 'South Korea',\n",
              " 'Spain',\n",
              " 'Sweden',\n",
              " 'Switzerland',\n",
              " 'Taiwan',\n",
              " 'Thailand',\n",
              " 'Turkey',\n",
              " 'UAE',\n",
              " 'UK',\n",
              " 'USA',\n",
              " 'Vietnam'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 186
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZlJ5ecThqi4"
      },
      "source": [
        "countryRegex = r\"'(\\w*\\s*\\w*)'\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLg3Fj0mBcKT"
      },
      "source": [
        "# Export countries for labelling\n",
        "contDf = pd.DataFrame(columns=['country','continent'])\n",
        "contDf['country'] = list(countrySet)\n",
        "contDf.to_csv('countries_with_continents.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVt4CuHaDZ4j"
      },
      "source": [
        "# Read in updated list\n",
        "#ccSheet = gc.open('/content/drive/MyDrive/NeurIPS_BIS_Analysis/countries_with_continents_labelled.csv')\n",
        "auth.authenticate_user()\n",
        "gc = gspread.authorize(GoogleCredentials.get_application_default())\n",
        "\n",
        "ccSheet = gc.open('labelled_countries').sheet1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "kQ6FU6K4GZ4x",
        "outputId": "ab03f27f-2ff8-49bb-8e23-ec0542de19ec"
      },
      "source": [
        "ccRows = ccSheet.get_all_values()\n",
        "upContDf = pd.DataFrame.from_records(ccRows)\n",
        "upContDf.columns = upContDf.iloc[0]\n",
        "upContDf = upContDf.drop(upContDf.index[0])\n",
        "upContDf = upContDf.drop(upContDf.columns[0],axis=1)\n",
        "upContDf#"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>country</th>\n",
              "      <th>continent</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Iran</td>\n",
              "      <td>Asia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Denmark</td>\n",
              "      <td>Europe</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>India</td>\n",
              "      <td>Asia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>China</td>\n",
              "      <td>Asia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>South Africa</td>\n",
              "      <td>Africa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>South Korea</td>\n",
              "      <td>Asia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Cyprus</td>\n",
              "      <td>Europe</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Independent</td>\n",
              "      <td>N/a</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Austria</td>\n",
              "      <td>Europe</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Brazil</td>\n",
              "      <td>South America</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Turkey</td>\n",
              "      <td>Asia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Pakistan</td>\n",
              "      <td>Asia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>Portugal</td>\n",
              "      <td>Europe</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>UAE</td>\n",
              "      <td>Asia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>Norway</td>\n",
              "      <td>Europe</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>Egypt</td>\n",
              "      <td>Africa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>Poland</td>\n",
              "      <td>Europe</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>France</td>\n",
              "      <td>Europe</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>Hong Kong</td>\n",
              "      <td>Asia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>Romania</td>\n",
              "      <td>Europe</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>Canada</td>\n",
              "      <td>North America</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>Czech Republic</td>\n",
              "      <td>Europe</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>Vietnam</td>\n",
              "      <td>Asia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>Chile</td>\n",
              "      <td>South America</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>Switzerland</td>\n",
              "      <td>Europe</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>Qatar</td>\n",
              "      <td>Asia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>Germany</td>\n",
              "      <td>Europe</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>Italy</td>\n",
              "      <td>Europe</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>Singapore</td>\n",
              "      <td>Asia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>Malaysia</td>\n",
              "      <td>Asia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>Sweden</td>\n",
              "      <td>Europe</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>Russia</td>\n",
              "      <td>Europe</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>Belgium</td>\n",
              "      <td>Europe</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>Saudi Arabia</td>\n",
              "      <td>Asia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>Netherlands</td>\n",
              "      <td>Europe</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>Japan</td>\n",
              "      <td>Asia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>Taiwan</td>\n",
              "      <td>Asia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>Australia</td>\n",
              "      <td>Oceania</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>Israel</td>\n",
              "      <td>Asia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>Spain</td>\n",
              "      <td>Europe</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>Greece</td>\n",
              "      <td>Europe</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>Finland</td>\n",
              "      <td>Europe</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>Thailand</td>\n",
              "      <td>Asia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>UK</td>\n",
              "      <td>Europe</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>USA</td>\n",
              "      <td>North America</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "0          country      continent\n",
              "1             Iran           Asia\n",
              "2          Denmark         Europe\n",
              "3            India           Asia\n",
              "4            China           Asia\n",
              "5     South Africa         Africa\n",
              "6      South Korea           Asia\n",
              "7           Cyprus         Europe\n",
              "8      Independent            N/a\n",
              "9          Austria         Europe\n",
              "10          Brazil  South America\n",
              "11          Turkey           Asia\n",
              "12        Pakistan           Asia\n",
              "13        Portugal         Europe\n",
              "14             UAE           Asia\n",
              "15          Norway         Europe\n",
              "16           Egypt         Africa\n",
              "17          Poland         Europe\n",
              "18          France         Europe\n",
              "19       Hong Kong           Asia\n",
              "20         Romania         Europe\n",
              "21          Canada  North America\n",
              "22  Czech Republic         Europe\n",
              "23         Vietnam           Asia\n",
              "24           Chile  South America\n",
              "25     Switzerland         Europe\n",
              "26           Qatar           Asia\n",
              "27         Germany         Europe\n",
              "28           Italy         Europe\n",
              "29       Singapore           Asia\n",
              "30        Malaysia           Asia\n",
              "31          Sweden         Europe\n",
              "32          Russia         Europe\n",
              "33         Belgium         Europe\n",
              "34    Saudi Arabia           Asia\n",
              "35     Netherlands         Europe\n",
              "36           Japan           Asia\n",
              "37          Taiwan           Asia\n",
              "38       Australia        Oceania\n",
              "39          Israel           Asia\n",
              "40           Spain         Europe\n",
              "41          Greece         Europe\n",
              "42         Finland         Europe\n",
              "43        Thailand           Asia\n",
              "44              UK         Europe\n",
              "45             USA  North America"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 190
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1orvuJ9N6g5"
      },
      "source": [
        "asia = upContDf[upContDf['continent'] == 'Asia']\n",
        "asiaList = list(asia['country'].values)\n",
        "africa = upContDf[upContDf['continent'] == 'Africa']\n",
        "africaList = list(africa['country'].values)\n",
        "northAm = upContDf[upContDf['continent'] == 'North America']\n",
        "northAmList = list(northAm['country'].values)\n",
        "southAm = upContDf[upContDf['continent'] == 'South America']\n",
        "southAmList = list(southAm['country'].values)\n",
        "europe = upContDf[upContDf['continent'] == 'Europe']\n",
        "europeList = list(europe['country'].values)\n",
        "oceania = upContDf[upContDf['continent'] == 'Oceania']\n",
        "oceaniaList = list(oceania['country'].values)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRVYW1PtPA9z"
      },
      "source": [
        "lists = [asiaList,africaList,northAmList,southAmList,europeList,oceaniaList]\n",
        "\n",
        "def makeDf(relList):\n",
        "  rowList = []\n",
        "\n",
        "  for index, row in megaDf.iterrows():\n",
        "    countries = re.findall(countryRegex,row['country'])\n",
        "    for country in countries:\n",
        "      if country in relList:\n",
        "        rowList.append(row)\n",
        "        break\n",
        "  return pd.DataFrame(rowList)\n",
        "                         "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_W-3OeoHjeC7"
      },
      "source": [
        "# Separate dfs\n",
        "\n",
        "#for contList, contDf in zip(lists, dfs):\n",
        "#  contDf = makeDf(contList)\n",
        "asiaDf = makeDf(asiaList)\n",
        "africaDf = makeDf(africaList)\n",
        "northAmDf = makeDf(northAmList)\n",
        "southAmDf = makeDf(southAmList)\n",
        "europeDf = makeDf(europeList)\n",
        "oceaniaDf = makeDf(oceaniaList)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UnzcAqnRkd2A",
        "outputId": "9537e01b-8ea6-49d7-d7e2-a22fd00a4339"
      },
      "source": [
        "x = 0\n",
        "contDfs = [asiaDf,africaDf,northAmDf,southAmDf,europeDf,oceaniaDf]\n",
        "for df in contDfs:\n",
        "  x += len(df)\n",
        "x\n",
        "# We expect some overlap"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2418"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 194
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAe1UnF4refB"
      },
      "source": [
        "### Top Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMffqTZd41RN"
      },
      "source": [
        "# make lists of keywords\n",
        "goodWords = 'Positive / Benefit / Beneficial / Good / Help / Profit / Gain / Advantage / Utility / Help / Improve / Enhance / Advance / Advantage / Assist / Promote / Effective / Useful / Productive / Constructive'\n",
        "goodWordList = makeWordList(goodWords)\n",
        "\n",
        "badWords = 'Cost / Bad / Hurt / Abuse / Damage / Ruin / Impair / Injury / Suffering / Damage / Ill / Loss / Detriment / Wrong / Oppress / Impose upon / Maltreat / Damaging / Dangerous / Evil / Destructive / Hazardous / Detrimental / Disadvantage / Toxic / Destructive'\n",
        "badWordList = makeWordList(badWords)\n",
        "\n",
        "agWordList = 'Agriculture / Farming / Crops'\n",
        "\n",
        "busWordList = 'Products / Services / Business / Industry / Corporate / Advertising / marketing / Logistics / Customer service/ Human resources / Hiring'\n",
        "\n",
        "culWordList = 'Culture / Media / Sport / News / Internet / Search / Personal assistant / Social media / Art'\n",
        "\n",
        "energyWordList = 'Energy'\n",
        "\n",
        "eduWordList = 'Education / Teach / School / Research / Science / Technology / Engineering'\n",
        "\n",
        "envWordList = 'Environment / Climate change / Climate'\n",
        "\n",
        "devWordList = 'Development / Charity / Charities / Non-profit / SDG / Sustainable development goals / Poverty'\n",
        "\n",
        "tradeWordList = 'Trade'\n",
        "\n",
        "irWordList = 'International relations / Institutional relations'\n",
        "\n",
        "transitWordList = 'Transport / Autonomous vehicles / driverless car'\n",
        "\n",
        "workWordList = 'Work / Worker / Labour / Labor / Job / Employment / Automation / Automated'\n",
        "\n",
        "healthWordList = 'Health / Medicine / Pharmaceuticals / Drugs / Government / Public sector / Public services / Social care / Social credit'\n",
        "\n",
        "econWordList = 'Finance / Financial / Economy / Economic / Banking / Banks / Credit / Loan / Mortgage'\n",
        "\n",
        "secWordList = 'Security / Defence / Cybersecurity / Military / Surveillance / Tracking / Face recognition / Facial recognition / Emotion recognition / Affect recognition / Biometric / Weapons / Warfare / War / Disinformation / misinformation'\n",
        "\n",
        "commWordList = 'Community / Housing / Accomodation'\n",
        "\n",
        "crimjustWordList = 'Crime / Criminal / Terrorism / Terrorist / Justice / Court / Recidivism / Risk score / Prison / Police / Policing / Cybercrime / Hacking / Phishing / Spam / Scam / Pornography / Deepfake'\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "posApps = 'Health / Science / Engineering / Education / Climate change / Charity / Development Goals / Art'\n",
        "posAppList = makeWordList(posApps)\n",
        "\n",
        "negApps = 'Weapon / Porn / Scam / Spam / Surveillance'\n",
        "negAppList = makeWordList(negApps)\n",
        "\n",
        "neutApps = 'Customer service / Advertising / Finance / Logistics / Industry / Robotics'\n",
        "neutAppList = makeWordList(neutApps)\n",
        "\n",
        "contApps = 'Policing / Military / Government'\n",
        "contAppList = makeWordList(contApps)\n",
        "\n",
        "techExpl = 'Limitation / Weakness / Defect / Drawback / Shortcoming / Strength / Improvement / Enhance / Increase / Improve / Advance'\n",
        "techExplList = makeWordList(techExpl)\n",
        "\n",
        "techNonSoc = 'Accuracy / Performance / Efficiency / Speed / Optimal / Compute / Memory / Data / Storage / AUC / FPR / TPR / Recall / Precision / BLEU / Benchmark / Sensitivity / Score / Metric'\n",
        "techNonSocList = makeWordList(techNonSoc)\n",
        "\n",
        "techSoc = 'Explainable / Interpretable / Fairness / Discrimination / Bias / Equality / Privacy / Anonymity / Consent / Data / Personal data / GDPR / Robustness / Adversarial robustness / Verification / Safety / Generalisable / Side effects / Reward hacking / Scalable Supervision / Safe exploration / Robustness / Distributional shift / Scalable oversight / Security / Vulnerability / Cybersecurity / Feedback loops'\n",
        "techSocList = makeWordList(techSoc)\n",
        "\n",
        "nonTechLim = 'Accountable / Responsibility / Transparency / Human Values / Human Rights / Ethical / Moral decisions / Trust / Trustworthy'\n",
        "nonTechLimList = makeWordList(nonTechLim)\n",
        "\n",
        "explRisk = 'Accident / Error / Unintended / Misuse / Malicious / Structural / Second-order / Diffuse / Immediate / Short / Medium / Long / Minor / Major / Severe / Extreme / Tail / X-risk / Existential / Extinction'\n",
        "explRiskList = makeWordList(explRisk)\n",
        "\n",
        "implRisk = 'Collision / Hacker / terrorist / criminal / War / AGI / TAI / Superintelligence'\n",
        "implRiskList = makeWordList(implRisk)\n",
        "\n",
        "structRisk = 'Environment / Energy / Electricity / Compute / Economy / Economic Growth / Jobs / Labour / Employment / Labor / Markets / Employment / Worker / Worker rights / Political / Democracy / Power / National politics / Public opinion / Repression / International relations / Conflict / International security / National security / War / Weapons / Arms race / Misinformation / Fake news / Manipulation / Direct attention / Persuasion / Synthetic media / Generated media / Wellbeing / Autonomy / Health / Mental health / Legal / Liability / Law / Regulation / Governance / Oversight / Policies / Human agency / Autonomy / Dignity / Global inequality / Fairness / Discrimination / Inequality / Bias / Fair'\n",
        "structRiskList = makeWordList(structRisk)\n",
        "\n",
        "stakeholders = 'Users / Owners / Developers / Regulators / Public / Researchers / Tech companies / Technology companies / Google / Facebook / EU / Impacted communities / Minorities / Marginalised / Underrepresented / Gender / Race / Demographics'\n",
        "stakeholdersList = makeWordList(stakeholders)\n",
        "\n",
        "epistemics = \"Uncertain / Maybe / Perhaps / Might / May / Don't know / Could / Unknown / Further investigation / Future work / Further work / More work\"\n",
        "epistemicsList = makeWordList(epistemics)\n",
        "\n",
        "usedGuide = 'Impact stack / Applications / Implications /  Initiatives'\n",
        "usedGuideList = makeWordList(usedGuide)\n",
        "\n",
        "makingRecs = 'Should / Recommend / Recommendation'\n",
        "makingRecsList = makeWordList(makingRecs)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpOkhxHO6OsU"
      },
      "source": [
        "getFreqsForSubset('aff','aff_good_total_freqs.csv','aff_good_stmt_freqs.csv',wordlist=goodWordList)\n",
        "getFreqsForSubset('aff','aff_bad_total_freqs.csv','aff_bad_stmt_freqs.csv',wordlist=badWordList)\n",
        "getFreqsForSubset('loc','loc_good_total_freqs.csv','loc_good_stmt_freqs.csv',wordlist=goodWordList)\n",
        "getFreqsForSubset('loc','loc_bad_total_freqs.csv','loc_bad_stmt_freqs.csv',wordlist=badWordList)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SjFU1SB-xZt"
      },
      "source": [
        "getFreqsForSubset('primary','prim_bad_total_freqs.csv','prim_bad_stmt_freqs.csv',wordlist=badWordList)\n",
        "getFreqsForSubset('cluster','clust_bad_total_freqs.csv','clust_bad_stmt_freqs.csv',wordlist=badWordList)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOghhwq9XT90"
      },
      "source": [
        "### Citations\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3LZ3usiSXVNq",
        "outputId": "be82cb1c-51ba-4794-f1f5-d00eb178fb9f"
      },
      "source": [
        "# Looking at citations\n",
        "megaDf[megaDf['citation count'] != 0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>paper title (separate scrape)</th>\n",
              "      <th>paper authors (separate scrape)</th>\n",
              "      <th>title</th>\n",
              "      <th>paper identifier</th>\n",
              "      <th>paper link</th>\n",
              "      <th>impact statement</th>\n",
              "      <th>impact title</th>\n",
              "      <th>word count</th>\n",
              "      <th>sentence count</th>\n",
              "      <th>citation count</th>\n",
              "      <th>has positive</th>\n",
              "      <th>has negative</th>\n",
              "      <th>has opt out</th>\n",
              "      <th>has NA</th>\n",
              "      <th>has impact statement</th>\n",
              "      <th>manually corrected</th>\n",
              "      <th>human review</th>\n",
              "      <th>Image based PDF</th>\n",
              "      <th>paper title (subjects)</th>\n",
              "      <th>primary subject area</th>\n",
              "      <th>secondary subject areas</th>\n",
              "      <th>clustering subject preference</th>\n",
              "      <th>authors</th>\n",
              "      <th>affiliations</th>\n",
              "      <th>academic</th>\n",
              "      <th>industry</th>\n",
              "      <th>mixed</th>\n",
              "      <th>country</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Fast and Flexible Temporal Point Processes wit...</td>\n",
              "      <td>Oleksandr Shchur, Nicholas Gao, Marin Biloš, S...</td>\n",
              "      <td>Fast and Flexible Temporal Point Processes wit...</td>\n",
              "      <td>00ac8ed3b4327bdd4ebbebcb2ba10a00</td>\n",
              "      <td>https://proceedings.neurips.cc/paper/2020/file...</td>\n",
              "      <td>Existing works have applied TPPs and MJPs for ...</td>\n",
              "      <td>Broader impact</td>\n",
              "      <td>120</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>TRUE</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>Fast and Flexible Temporal Point Processes wit...</td>\n",
              "      <td>Probabilistic Methods</td>\n",
              "      <td>Algorithms -&gt; Density Estimation; Applications...</td>\n",
              "      <td>Probabilistic methods and inference</td>\n",
              "      <td>['Oleksandr Shchur', ' Nicholas Gao', ' Marin ...</td>\n",
              "      <td>{'Technical University of Munich'}</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>{'Germany'}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Synbols: Probing Learning Algorithms with Synt...</td>\n",
              "      <td>Alexandre Lacoste, Pau Rodríguez López, Freder...</td>\n",
              "      <td>Synbols: Probing Learning Algorithms with Synt...</td>\n",
              "      <td>0169cf885f882efd795951253db5cdfb</td>\n",
              "      <td>https://proceedings.neurips.cc/paper/2020/file...</td>\n",
              "      <td>The introduction of benchmark new datasets has...</td>\n",
              "      <td>Broader Impact</td>\n",
              "      <td>386</td>\n",
              "      <td>13</td>\n",
              "      <td>3</td>\n",
              "      <td>TRUE</td>\n",
              "      <td>TRUE</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>TRUE</td>\n",
              "      <td>TRUE</td>\n",
              "      <td>TRUE</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>Synbols: Probing Learning Algorithms with Synt...</td>\n",
              "      <td>Data, Challenges, Implementations, and Softwar...</td>\n",
              "      <td>Algorithms -&gt; Active Learning; Algorithms -&gt; F...</td>\n",
              "      <td>Datasets, challenges, software</td>\n",
              "      <td>['Alexandre Lacoste', ' Pau Rodríguez López', ...</td>\n",
              "      <td>{'ElementAI', 'MILA', 'University of British C...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>{'Canada'}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>Cascaded Text Generation with Markov Transformers</td>\n",
              "      <td>Yuntian Deng, Alexander Rush</td>\n",
              "      <td>Cascaded Text Generation with Markov Transformers</td>\n",
              "      <td>01a0683665f38d8e5e567b3b15ca98bf</td>\n",
              "      <td>https://proceedings.neurips.cc/paper/2020/file...</td>\n",
              "      <td>Our work proposes an alternative approach to b...</td>\n",
              "      <td>Broader Impact</td>\n",
              "      <td>204</td>\n",
              "      <td>9</td>\n",
              "      <td>11</td>\n",
              "      <td>TRUE</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>TRUE</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>Cascaded Text Generation with Markov Transformers</td>\n",
              "      <td>Applications -&gt; Natural Language Processing</td>\n",
              "      <td>Deep Learning -&gt; Generative Models</td>\n",
              "      <td>Natural language processing</td>\n",
              "      <td>['Yuntian Deng', ' Alexander Rush']</td>\n",
              "      <td>{'Harvard University', 'Cornell University'}</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>{'USA'}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>Coresets for Regressions with Panel Data</td>\n",
              "      <td>Lingxiao Huang, K Sudhir, Nisheeth Vishnoi</td>\n",
              "      <td>Coresets for Regressions with Panel Data</td>\n",
              "      <td>03287fcce194dbd958c2ec5b33705912</td>\n",
              "      <td>https://proceedings.neurips.cc/paper/2020/file...</td>\n",
              "      <td>Many organizations have to routinely outsource...</td>\n",
              "      <td>Broader impact</td>\n",
              "      <td>257</td>\n",
              "      <td>11</td>\n",
              "      <td>4</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>TRUE</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>TRUE</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>Coresets for Regressions with Panel Data</td>\n",
              "      <td>Algorithms -&gt; Data Compression</td>\n",
              "      <td>Algorithms -&gt; Regression</td>\n",
              "      <td>Core machine learning methods (e.g., supervise...</td>\n",
              "      <td>['Lingxiao Huang', ' K Sudhir', ' Nisheeth Vis...</td>\n",
              "      <td>{'Yale University', 'EPFL'}</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>{'USA', 'Switzerland'}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>Multi-Robot Collision Avoidance under Uncertai...</td>\n",
              "      <td>Wenhao Luo, Wen Sun, Ashish Kapoor</td>\n",
              "      <td>Multi-Robot Collision Avoidance under Uncertai...</td>\n",
              "      <td>03793ef7d06ffd63d34ade9d091f1ced</td>\n",
              "      <td>https://proceedings.neurips.cc/paper/2020/file...</td>\n",
              "      <td>The objective of this work is to provide an ex...</td>\n",
              "      <td>Broader Impact</td>\n",
              "      <td>329</td>\n",
              "      <td>13</td>\n",
              "      <td>2</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>TRUE</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>Multi-Robot Collision Avoidance under Uncertai...</td>\n",
              "      <td>Social Aspects of Machine Learning -&gt; AI Safety</td>\n",
              "      <td>Applications -&gt; Robotics; Theory -&gt; Control Th...</td>\n",
              "      <td>Safety and Robustness for Autonomous Systems</td>\n",
              "      <td>['Wenhao Luo', ' Wen Sun', ' Ashish Kapoor']</td>\n",
              "      <td>{'Microsoft', 'Carnegie Mellon University', 'M...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>{'USA'}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1864</th>\n",
              "      <td>Measuring Systematic Generalization in Neural ...</td>\n",
              "      <td>Nicolas Gontier, Koustuv Sinha, Siva Reddy, Ch...</td>\n",
              "      <td>Measuring Systematic Generalization in Neural ...</td>\n",
              "      <td>fc84ad56f9f547eb89c72b9bac209312</td>\n",
              "      <td>https://proceedings.neurips.cc/paper/2020/file...</td>\n",
              "      <td>Transformer based models have been very effect...</td>\n",
              "      <td>Broader Impact</td>\n",
              "      <td>211</td>\n",
              "      <td>11</td>\n",
              "      <td>1</td>\n",
              "      <td>TRUE</td>\n",
              "      <td>TRUE</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>TRUE</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>Measuring Systematic Generalization in Neural ...</td>\n",
              "      <td>Applications -&gt; Natural Language Processing</td>\n",
              "      <td>Algorithms -&gt; Structured Prediction; Deep Lear...</td>\n",
              "      <td>Natural language processing</td>\n",
              "      <td>['Nicolas Gontier', ' Koustuv Sinha', ' Siva R...</td>\n",
              "      <td>{'McGill University / Mila / FAIR', 'Montreal ...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>{'Canada', 'France', 'USA'}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1867</th>\n",
              "      <td>Fast Matrix Square Roots with Applications to ...</td>\n",
              "      <td>Geoff Pleiss, Martin Jankowiak, David Eriksson...</td>\n",
              "      <td>Fast Matrix Square Roots with Applications to ...</td>\n",
              "      <td>fcf55a303b71b84d326fb1d06e332a26</td>\n",
              "      <td>https://proceedings.neurips.cc/paper/2020/file...</td>\n",
              "      <td>This paper introduces an algorithm to improve ...</td>\n",
              "      <td>Broader Impact</td>\n",
              "      <td>413</td>\n",
              "      <td>20</td>\n",
              "      <td>8</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>TRUE</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>Fast Matrix Square Roots with Applications to ...</td>\n",
              "      <td>Probabilistic Methods -&gt; Gaussian Processes</td>\n",
              "      <td></td>\n",
              "      <td>Probabilistic methods and inference</td>\n",
              "      <td>['Geoff Pleiss', ' Martin Jankowiak', ' David ...</td>\n",
              "      <td>{'Facebook', 'University of Pennsylvania', 'Co...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>{'USA'}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1889</th>\n",
              "      <td>Multi-agent Trajectory Prediction with Fuzzy Q...</td>\n",
              "      <td>Nitin Kamra, Hao Zhu, Dweep Kumarbhai Trivedi,...</td>\n",
              "      <td>Multi-agent Trajectory Prediction with Fuzzy Q...</td>\n",
              "      <td>fe87435d12ef7642af67d9bc82a8b3cd</td>\n",
              "      <td>https://proceedings.neurips.cc/paper/2020/file...</td>\n",
              "      <td>We have presented a general architecture for m...</td>\n",
              "      <td>Broader Impact</td>\n",
              "      <td>132</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>TRUE</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>TRUE</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>Multi-agent Trajectory Prediction with Fuzzy Q...</td>\n",
              "      <td>Deep Learning -&gt; Attention Models</td>\n",
              "      <td>Algorithms -&gt; Relational Learning; Deep Learni...</td>\n",
              "      <td>Other applications (e.g., robotics, biology, c...</td>\n",
              "      <td>['Nitin Kamra', ' Hao Zhu', ' Dweep Kumarbhai ...</td>\n",
              "      <td>{'University of Southern California', 'Peking ...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>{'USA', 'China'}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1892</th>\n",
              "      <td>Can the Brain Do Backpropagation? --- Exact Im...</td>\n",
              "      <td>Yuhang Song, Thomas Lukasiewicz, Zhenghua Xu, ...</td>\n",
              "      <td>Can the Brain Do Backpropagation? — Exact Impl...</td>\n",
              "      <td>fec87a37cdeec1c6ecf8181c0aa2d3bf</td>\n",
              "      <td>https://proceedings.neurips.cc/paper/2020/file...</td>\n",
              "      <td>This work shows that backpropagation in artifi...</td>\n",
              "      <td>Broader Impact</td>\n",
              "      <td>280</td>\n",
              "      <td>12</td>\n",
              "      <td>2</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>TRUE</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>TRUE</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>Can the Brain Do Backpropagation? --- Exact Im...</td>\n",
              "      <td>Deep Learning -&gt; Biologically Plausible Deep N...</td>\n",
              "      <td>Deep Learning -&gt; Analysis and Understanding of...</td>\n",
              "      <td>Neuroscience and cognitive science</td>\n",
              "      <td></td>\n",
              "      <td>{'University of Oxford', 'Hebei University of ...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>{'UK', 'China'}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1893</th>\n",
              "      <td>Manifold GPLVMs for discovering non-Euclidean ...</td>\n",
              "      <td>Kristopher Jensen, Ta-Chu Kao, Marco Tripodi, ...</td>\n",
              "      <td>Manifold GPLVMs for discovering non-Euclidean ...</td>\n",
              "      <td>fedc604da8b0f9af74b6cfc0fab2163c</td>\n",
              "      <td>https://proceedings.neurips.cc/paper/2020/file...</td>\n",
              "      <td>There are two broad fields which we expect mig...</td>\n",
              "      <td>Broader Impact</td>\n",
              "      <td>244</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>TRUE</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>TRUE</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>Manifold GPLVMs for discovering non-Euclidean ...</td>\n",
              "      <td>Neuroscience and Cognitive Science -&gt; Neurosci...</td>\n",
              "      <td>Algorithms -&gt; Nonlinear Dimensionality Reducti...</td>\n",
              "      <td>Neuroscience and cognitive science</td>\n",
              "      <td>['Kristopher Jensen', 'Chu Kao', ' Marco Tripo...</td>\n",
              "      <td>{'University of Cambridge', 'MRC', 'Cambridge'}</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>{'UK'}</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>405 rows × 28 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "0                         paper title (separate scrape)  ...                      country\n",
              "7     Fast and Flexible Temporal Point Processes wit...  ...                  {'Germany'}\n",
              "12    Synbols: Probing Learning Algorithms with Synt...  ...                   {'Canada'}\n",
              "15    Cascaded Text Generation with Markov Transformers  ...                      {'USA'}\n",
              "28             Coresets for Regressions with Panel Data  ...       {'USA', 'Switzerland'}\n",
              "32    Multi-Robot Collision Avoidance under Uncertai...  ...                      {'USA'}\n",
              "...                                                 ...  ...                          ...\n",
              "1864  Measuring Systematic Generalization in Neural ...  ...  {'Canada', 'France', 'USA'}\n",
              "1867  Fast Matrix Square Roots with Applications to ...  ...                      {'USA'}\n",
              "1889  Multi-agent Trajectory Prediction with Fuzzy Q...  ...             {'USA', 'China'}\n",
              "1892  Can the Brain Do Backpropagation? --- Exact Im...  ...              {'UK', 'China'}\n",
              "1893  Manifold GPLVMs for discovering non-Euclidean ...  ...                       {'UK'}\n",
              "\n",
              "[405 rows x 28 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCi-5n3RXvyx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wiiNV9eY1AE"
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-iQKBJKX7TN",
        "outputId": "e4371036-4c33-4851-f4f3-4e7de44cab52"
      },
      "source": [
        "describeCitations('primary')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Applications -> Computer Vision citations:\n",
            " count    96.000000\n",
            "mean      0.750000\n",
            "std       2.357519\n",
            "min       0.000000\n",
            "25%       0.000000\n",
            "50%       0.000000\n",
            "75%       0.000000\n",
            "max      15.000000\n",
            "Name: citation count, dtype: float64\n",
            "Deep Learning citations:\n",
            " count    63.000000\n",
            "mean      0.841270\n",
            "std       2.343187\n",
            "min       0.000000\n",
            "25%       0.000000\n",
            "50%       0.000000\n",
            "75%       0.000000\n",
            "max      10.000000\n",
            "Name: citation count, dtype: float64\n",
            "Reinforcement Learning and Planning -> Reinforcement Learning citations:\n",
            " count    57.000000\n",
            "mean      0.964912\n",
            "std       1.945369\n",
            "min       0.000000\n",
            "25%       0.000000\n",
            "50%       0.000000\n",
            "75%       1.000000\n",
            "max      10.000000\n",
            "Name: citation count, dtype: float64\n",
            "Reinforcement Learning and Planning citations:\n",
            " count    57.000000\n",
            "mean      0.912281\n",
            "std       2.523325\n",
            "min       0.000000\n",
            "25%       0.000000\n",
            "50%       0.000000\n",
            "75%       0.000000\n",
            "max      12.000000\n",
            "Name: citation count, dtype: float64\n",
            "Deep Learning -> Analysis and Understanding of Deep Networks citations:\n",
            " count    56.000000\n",
            "mean      0.910714\n",
            "std       2.056270\n",
            "min       0.000000\n",
            "25%       0.000000\n",
            "50%       0.000000\n",
            "75%       0.000000\n",
            "max       8.000000\n",
            "Name: citation count, dtype: float64\n",
            "Deep Learning -> Generative Models citations:\n",
            " count    54.000000\n",
            "mean      0.500000\n",
            "std       1.209241\n",
            "min       0.000000\n",
            "25%       0.000000\n",
            "50%       0.000000\n",
            "75%       0.000000\n",
            "max       7.000000\n",
            "Name: citation count, dtype: float64\n",
            "Algorithms -> Representation Learning citations:\n",
            " count    50.000000\n",
            "mean      1.460000\n",
            "std       3.881984\n",
            "min       0.000000\n",
            "25%       0.000000\n",
            "50%       0.000000\n",
            "75%       1.000000\n",
            "max      23.000000\n",
            "Name: citation count, dtype: float64\n",
            "Theory -> Statistical Learning Theory citations:\n",
            " count    48.000000\n",
            "mean      0.625000\n",
            "std       2.038199\n",
            "min       0.000000\n",
            "25%       0.000000\n",
            "50%       0.000000\n",
            "75%       0.000000\n",
            "max      10.000000\n",
            "Name: citation count, dtype: float64\n",
            "Algorithms -> Bandit Algorithms citations:\n",
            " count    40.000000\n",
            "mean      0.675000\n",
            "std       1.939964\n",
            "min       0.000000\n",
            "25%       0.000000\n",
            "50%       0.000000\n",
            "75%       0.000000\n",
            "max       9.000000\n",
            "Name: citation count, dtype: float64\n",
            "Applications -> Natural Language Processing citations:\n",
            " count    35.000000\n",
            "mean      1.657143\n",
            "std       4.221394\n",
            "min       0.000000\n",
            "25%       0.000000\n",
            "50%       0.000000\n",
            "75%       1.500000\n",
            "max      22.000000\n",
            "Name: citation count, dtype: float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iboyEty_ZGX9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOjR2qfUtSpc"
      },
      "source": [
        "## NLTK\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vgX28LCtUr-",
        "outputId": "eb9ec75a-0cfd-4ef4-bf91-447bfa5488c2"
      },
      "source": [
        "# Negating words\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipva4UW_tkyN"
      },
      "source": [
        "dfWithNeg = megaDf\n",
        "# separate punctuation so negation knows when to stop\n",
        "dfWithNeg['bis_with_neg'] = dfWithNeg.apply(lambda x: x['impact statement'].translate(str.maketrans({'.':' . ','!':' ! ','?':' ? ',',':' , ',':':' : ',';':' ; '})),axis=1)\n",
        "# apply negation\n",
        "dfWithNeg['bis_with_neg'] = dfWithNeg.apply(lambda x: mark_negation(x['bis_with_neg'].split()), axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEa3r7let7k6"
      },
      "source": [
        "dfWithNeg.to_csv('BIS_with_negations.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__UUJk1gvkfW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}