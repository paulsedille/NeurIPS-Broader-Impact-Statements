{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "BIS_analysis_for_release.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "gHVBpmntzDEw",
        "Wbg5E0VTzMBx",
        "LN8NY3I9zOid",
        "aIe-QK-u5XQK",
        "iMYR2CJv9Naj",
        "M5Lss9JKlYKs",
        "Wb9Q9_pQuIom",
        "RQD1dcYhuLHh",
        "WAe1UnF4refB",
        "AOghhwq9XT90"
      ],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/paulsedille/NeurIPS-Broader-Impact-Statements/blob/main/BIS_analysis_for_release.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7mFuBu_CBLJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de767bad-d784-4b65-c46a-9b6e430df1a5"
      },
      "source": [
        "import pandas as pd\n",
        "from os import sep\n",
        "import numpy as np\n",
        "import re\n",
        "from google.colab import drive\n",
        "from PIL import Image\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import auth\n",
        "import gspread\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import shutil\n",
        "import glob\n",
        "from collections import Counter\n",
        "import string\n",
        "import nltk\n",
        "from nltk.sentiment.util import mark_negation\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
            "  warnings.warn(\"The twython library has not been installed. \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7oPLf9kCprT"
      },
      "source": [
        "auth.authenticate_user()\n",
        "\n",
        "gc = gspread.authorize(GoogleCredentials.get_application_default())"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8o6QI-sNFYG3",
        "outputId": "6190320e-cc1b-417d-f7b1-8fd681252771"
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2V0hmidqJhSP"
      },
      "source": [
        "# It should be authenticated and able to get the NeurIPS 2020 BIS spreadsheet"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJo-qe7KsNb6"
      },
      "source": [
        "### Run these next three cells"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vv0NjaHiFX5N"
      },
      "source": [
        "# Open sheet and make dataframe\n",
        "sheet = gc.open('NeurIPS 2020 BIS').sheet1\n",
        "rows = sheet.get_all_values()\n",
        "megaDf = pd.DataFrame.from_records(rows)\n",
        "megaDf.columns = megaDf.iloc[0]\n",
        "megaDf = megaDf.drop(megaDf.index[0])\n",
        "#megaDf = megaDf.drop('Random Check', axis=1)\n",
        "\n",
        "# Convert strings to numeric\n",
        "megaDf['academic'] = pd.to_numeric(megaDf['academic'])\n",
        "megaDf['industry'] = pd.to_numeric(megaDf['industry'])\n",
        "megaDf['mixed'] = pd.to_numeric(megaDf['mixed'])\n",
        "megaDf['word count'] = pd.to_numeric(megaDf['word count'])\n",
        "megaDf['sentence count'] = pd.to_numeric(megaDf['sentence count'])\n",
        "megaDf['citation count'] = pd.to_numeric(megaDf['citation count'])\n",
        "\n",
        "# define valid split groupings\n",
        "splits = ['all','aff','cluster','loc','us-ch','bigTech'] # no primary grouping but can add\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sz3uqnztoBrA"
      },
      "source": [
        "# Generate statements with negated terms\n",
        "megaDf['bis_with_neg'] = megaDf.apply(lambda x: x['impact statement'].translate(str.maketrans({'.':' . ','!':' ! ','?':' ? ',',':' , ',':':' : ',';':' ; '})),axis=1)\n",
        "# apply negation\n",
        "useDoubleNeg = True\n",
        "megaDf['bis_with_neg'] = megaDf.apply(lambda x: mark_negation(x['bis_with_neg'].split(),double_neg_flip=useDoubleNeg), axis=1)\n",
        "\n",
        "megaDf['bis_with_neg'].apply(lambda x: ' '.join(x)).to_csv('negated statements.csv')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvAFWVr3cfzX"
      },
      "source": [
        "#### Helper functions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggMcNc54cTNu"
      },
      "source": [
        "# Helper functions to group by continent\n",
        "def makeDf(relList):\n",
        "  countryRegex = r\"'(\\w*\\s*\\w*)'\"\n",
        "  rowList = []\n",
        "\n",
        "  for index, row in megaDf.iterrows():\n",
        "    countries = re.findall(countryRegex,row['country'])\n",
        "    for country in countries:\n",
        "      if country in relList:\n",
        "        rowList.append(row)\n",
        "        break\n",
        "  return pd.DataFrame(rowList)\n",
        "\n",
        "\n",
        "# Helper function to generate dataframes for each continent\n",
        "def generateContinentDfs():\n",
        "  ccSheet = gc.open('labelled_countries').sheet1\n",
        "  ccRows = ccSheet.get_all_values()\n",
        "  upContDf = pd.DataFrame.from_records(ccRows)\n",
        "  upContDf.columns = upContDf.iloc[0]\n",
        "  upContDf = upContDf.drop(upContDf.index[0])\n",
        "  upContDf = upContDf.drop(upContDf.columns[0],axis=1)\n",
        "  \n",
        "  asia = upContDf[upContDf['continent'] == 'Asia']\n",
        "  asiaList = list(asia['country'].values)\n",
        "  africa = upContDf[upContDf['continent'] == 'Africa']\n",
        "  africaList = list(africa['country'].values)\n",
        "  northAm = upContDf[upContDf['continent'] == 'North America']\n",
        "  northAmList = list(northAm['country'].values)\n",
        "  southAm = upContDf[upContDf['continent'] == 'South America']\n",
        "  southAmList = list(southAm['country'].values)\n",
        "  europe = upContDf[upContDf['continent'] == 'Europe']\n",
        "  europeList = list(europe['country'].values)\n",
        "  oceania = upContDf[upContDf['continent'] == 'Oceania']\n",
        "  oceaniaList = list(oceania['country'].values)\n",
        "\n",
        "  asiaDf = makeDf(asiaList)\n",
        "  africaDf = makeDf(africaList)\n",
        "  northAmDf = makeDf(northAmList)\n",
        "  southAmDf = makeDf(southAmList)\n",
        "  europeDf = makeDf(europeList)\n",
        "  oceaniaDf = makeDf(oceaniaList)\n",
        "\n",
        "  return([asiaDf,africaDf,northAmDf,southAmDf,europeDf,oceaniaDf],\n",
        "         ['Asia','Africa','North America','South America','Europe','Oceania'])\n",
        "\n",
        "\n",
        "# Helper function to generate dataframes grouped by a column\n",
        "# @col: the column name\n",
        "# @top10: True to limit to 10 largest groupings\n",
        "def generateSbjDfs(col,top10=False):\n",
        "  dfs = []\n",
        "\n",
        "  sbjDf = megaDf.groupby(by=col)['title'].count()\n",
        "  sbjDf = sbjDf.sort_values(ascending=False)\n",
        "\n",
        "  if top10:\n",
        "    sbjDf = sbjDf[:10]\n",
        "\n",
        "  for sbj in sbjDf.keys():\n",
        "    dfs.append(megaDf[megaDf[col] == str(sbj)])\n",
        "  return (dfs,sbjDf.keys().tolist())\n",
        "\n",
        "\n",
        "# Generates a subset of the megaDf\n",
        "# @sub: \n",
        "# 'all' = all papers\n",
        "# 'aff' = split by affiliation (academic, industry, mixed)\n",
        "# 'primary' = primary subject area (top 10)\n",
        "# 'cluster' = clustering subject area (top 10)\n",
        "# 'loc' = continent\n",
        "# 'us-ch' = US and China affiliations\n",
        "# 'bigTech' = Apple, Amazon, Microsoft, Google, Facebook, Huawei\n",
        "def generateSubset(sub):\n",
        "  # subset data\n",
        "  if sub == 'all':\n",
        "    dfList = [megaDf]\n",
        "    names = ['all']\n",
        "  elif sub == 'aff':\n",
        "    acaDf = megaDf.loc[(megaDf['academic'] == 1) & (megaDf['mixed'] == 0)]\n",
        "    indDf = megaDf.loc[(megaDf['industry'] == 1) & (megaDf['mixed'] == 0)]\n",
        "    mixedDf = megaDf[megaDf['mixed'] == 1]\n",
        "    dfList = [acaDf,indDf,mixedDf]\n",
        "    names = ['academic','industry','mixed-affiliation']\n",
        "  elif sub == 'primary':\n",
        "    (dfList,names) = generateSbjDfs('primary subject area')\n",
        "  elif sub == 'cluster':\n",
        "    (dfList,names) = generateSbjDfs('clustering subject preference')\n",
        "  elif sub == 'loc':\n",
        "    (dfList,names) = generateContinentDfs()\n",
        "  elif sub == 'us-ch':\n",
        "    usOnlyDf = megaDf[megaDf['country'] == \"{'USA'}\"]\n",
        "    chOnlyDf = megaDf[megaDf['country'] == \"{'China'}\"]\n",
        "    dfList = [usOnlyDf,chOnlyDf]\n",
        "    names = ['United States','China']\n",
        "  elif sub == 'bigTech':\n",
        "    appleDf = megaDf[megaDf['affiliations'].str.contains('Apple')]#megaDf['apple' in str(megaDf['affiliations']).lower()]\n",
        "    amazonDf = megaDf[megaDf['affiliations'].str.contains('Amazon')]\n",
        "    microsoftDf = megaDf[megaDf['affiliations'].str.contains('Microsoft')]\n",
        "    googleDf = megaDf[megaDf['affiliations'].str.contains('Google')]\n",
        "    facebookDf = megaDf[megaDf['affiliations'].str.contains('Facebook')]\n",
        "    huaweiDf = megaDf[megaDf['affiliations'].str.contains('Huawei')]\n",
        "    dfList = [appleDf, amazonDf, microsoftDf, googleDf, facebookDf, huaweiDf]\n",
        "    names = ['Apple', 'Amazon', 'Microsoft', 'Google', 'Facebook', 'Huawei']\n",
        "  else:\n",
        "      print(\"Subset string not recognized.\")\n",
        "      return\n",
        "\n",
        "  return [dfList, names]\n",
        "\n",
        "\n",
        "# Helper function to make a list of words from a string separated by /\n",
        "# @words: the word string\n",
        "def makeWordList(words):\n",
        "  wL = words.split('/')\n",
        "  return [x.lower().strip() for x in wL]\n",
        "\n",
        "\n",
        "# Make frequency DataFrames with a wordlist\n",
        "# @df: the DataFrame to calculate frequencies for\n",
        "# @wordlist: the wordlist containing the words to calculate frequencies for\n",
        "# @useNeg: whether or not to also use the negated versions of the wordlist \n",
        "def makeFreqDfsWithWordlist(df,wordlist,useNeg=False):\n",
        "  wordFreqDict = {}\n",
        "  stmtFreqDict = {}\n",
        "  \n",
        "  any = '<any>' # any words count here\n",
        "  anyNoNeg = '<any_no_neg>' # only words that aren't negated count here\n",
        "\n",
        "  wordFreqDict[any] = 0\n",
        "  wordFreqDict[anyNoNeg] = 0\n",
        "\n",
        "  stmtFreqDict[any] = 0\n",
        "  stmtFreqDict[anyNoNeg] = 0\n",
        "\n",
        "  countForAnyProps = 0\n",
        "\n",
        "  stopwords = set(STOPWORDS) # using wordcloud stopwords\n",
        "  stopwords.update([' ','','\\n'])\n",
        "  totalLen = 0\n",
        "\n",
        "  wordlist = [x.strip().lower() for x in wordlist] # assumes no punctuation in wordlist\n",
        "  wordlist = [' ' + x + ' ' for x in wordlist] # add padding spaces \n",
        "\n",
        "  # get version with negation tags if necessary\n",
        "  if useNeg:\n",
        "    statements = df['bis_with_neg'].values\n",
        "  else:\n",
        "    statements = df['impact statement'].values\n",
        "\n",
        "  for statement in statements:\n",
        "    componentWords = set()\n",
        "\n",
        "    if useNeg: \n",
        "      words = statement # presplit if using negated\n",
        "    else: \n",
        "      words = statement.split(' ')\n",
        "    totalLen += len(words)\n",
        "\n",
        "    # remove punctuation and go lowercase\n",
        "    # using spaces so can count with regex\n",
        "    words = [x.strip().lower() for x in words]\n",
        "    words = [re.sub(\"[^\\w\\s]\", \" \", x) for x in words]\n",
        "\n",
        "    joinedStmt = ' ' + ' '.join(words) + ' '\n",
        "    \n",
        "    for target in wordlist:\n",
        "      #numOccs = len(re.findall(target,joinedStmt))\n",
        "\n",
        "      strippedTar = target.strip()\n",
        "      numOccs = len(re.findall(f'{strippedTar} ',joinedStmt))\n",
        "\n",
        "      if numOccs > 0:\n",
        "        componentWords.add(strippedTar)\n",
        "        wordFreqDict[any] += numOccs # update <any> row\n",
        "\n",
        "        if '_neg' not in strippedTar: # update <any_no_neg> row if word not negated\n",
        "          wordFreqDict[anyNoNeg] += numOccs\n",
        "\n",
        "        if strippedTar in wordFreqDict:\n",
        "          wordFreqDict[strippedTar] += numOccs \n",
        "        else: \n",
        "          wordFreqDict[strippedTar] = numOccs \n",
        "\n",
        "\n",
        "    # add to number of statements words appear in\n",
        "    if len(componentWords) > 0: # if any of the words have occurred, updated <any> row\n",
        "      stmtFreqDict[any] += 1\n",
        "\n",
        "      compsNoNeg = [x for x in componentWords if '_neg' not in x] # if at least one non-negated word, update <any_no_neg> row\n",
        "      if len(compsNoNeg) > 0:\n",
        "        stmtFreqDict[anyNoNeg] += 1\n",
        "\n",
        "    for word in componentWords: # update statement count\n",
        "      if word in stmtFreqDict:\n",
        "        stmtFreqDict[word] += 1\n",
        "      else: stmtFreqDict[word] = 1\n",
        "\n",
        "  # make dataframes\n",
        "  wordFreqs = pd.DataFrame.from_dict(wordFreqDict,orient='index',columns=['word_freq'])\n",
        "  wordFreqs['word_prop'] = wordFreqs['word_freq']/totalLen\n",
        "  wordFreqs = wordFreqs.sort_values(by='word_prop',ascending=False)\n",
        "\n",
        "  stmtFreqs = pd.DataFrame.from_dict(stmtFreqDict,orient='index',columns=['stmt_freq'])\n",
        "  stmtFreqs['stmt_prop'] = stmtFreqs['stmt_freq']/len(statements)\n",
        "  stmtFreqs = stmtFreqs.sort_values(by='stmt_prop',ascending=False)\n",
        "\n",
        "  # calculate average number of occurrences\n",
        "  avgs = {}\n",
        "  for word in wordFreqDict.keys():\n",
        "    avgs[word] = wordFreqDict[word]/len(df)\n",
        "  avgOccs = pd.DataFrame.from_dict(avgs,orient='index',columns=['avg_occs'])\n",
        "  avgOccs = avgOccs.sort_values(by='avg_occs',ascending=False)\n",
        "\n",
        "  return (wordFreqs, stmtFreqs, avgOccs)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOfW2Ybzfr4r"
      },
      "source": [
        "#### The important functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_t7bG_GffdDx"
      },
      "source": [
        "# Function to print proportions of records containing words and save to csv.\n",
        "# @sub:\n",
        "# all = all papers\n",
        "# aff = split by affiliation\n",
        "# primary = primary subject area (top 10)\n",
        "# cluster = clustering subject area (top 10)\n",
        "# loc = continent\n",
        "# (potentially to come: countries of interest)\n",
        "# @wordlist: list of words to consider\n",
        "# @filename: '<path/to/file.csv>'\n",
        "def getProportions(sub, wordlist, fileName):\n",
        "  dfList = []\n",
        "  names = []\n",
        "\n",
        "  # subset data\n",
        "  if sub == 'all':\n",
        "    dfList = [megaDf]\n",
        "    names = ['all']\n",
        "  elif sub == 'aff':\n",
        "    acaDf = megaDf.loc[(megaDf['academic'] == 1) & (megaDf['mixed'] == 0)]\n",
        "    indDf = megaDf.loc[(megaDf['industry'] == 1) & (megaDf['mixed'] == 0)]\n",
        "    mixedDf = megaDf[megaDf['mixed'] == 1]\n",
        "    dfList = [acaDf,indDf,mixedDf]\n",
        "    names = ['academic','industry','mixed-affiliation']\n",
        "  elif sub == 'primary':\n",
        "    (dfList,names) = generateSbjDfs('primary subject area')\n",
        "  elif sub == 'cluster':\n",
        "    (dfList,names) = generateSbjDfs('clustering subject preference')\n",
        "  elif sub == 'loc':\n",
        "    (dfList,names) = generateContinentDfs()\n",
        "  elif sub =='us-ch':\n",
        "    usOnlyDf = megaDf[megaDf['country'] == \"{'USA'}\"]\n",
        "    chOnlyDf = megaDf[megaDf['country'] == \"{'China'}\"]\n",
        "    dfList = [usOnlyDf,chOnlyDf]\n",
        "    names = ['United States','China']\n",
        "  elif sub == 'bigTech':\n",
        "    appleDf = megaDf[megaDf['affiliations'].str.contains('Apple')]#megaDf['apple' in str(megaDf['affiliations']).lower()]\n",
        "    amazonDf = megaDf[megaDf['affiliations'].str.contains('Amazon')]\n",
        "    microsoftDf = megaDf[megaDf['affiliations'].str.contains('Microsoft')]\n",
        "    googleDf = megaDf[megaDf['affiliations'].str.contains('Google')]\n",
        "    facebookDf = megaDf[megaDf['affiliations'].str.contains('Facebook')]\n",
        "    huaweiDf = megaDf[megaDf['affiliations'].str.contains('Huawei')]\n",
        "    dfList = [appleDf, amazonDf, microsoftDf, googleDf, facebookDf, huaweiDf]\n",
        "    names = ['Apple', 'Amazon', 'Microsoft', 'Google', 'Facebook', 'Huawei']\n",
        "  else:\n",
        "    print(\"Subset string not recognized.\")\n",
        "    return\n",
        "\n",
        "  # calculate proportions and write to csv\n",
        "  bigListForDf = []\n",
        "  for word in wordlist:\n",
        "    word = re.sub(\"[^\\w\\s]\", \"\", word.lower().strip()) \n",
        "    listForDf = []\n",
        "    for df, name in zip(dfList,names):\n",
        "      containsDf = df[df['impact statement'].str.contains(word)]\n",
        "      prop = len(containsDf)/len(df)\n",
        "      listForDf.append([word, name, prop])\n",
        "    bigListForDf = bigListForDf + listForDf\n",
        "  allWordsDf = pd.DataFrame(bigListForDf,columns=['word','category','proportion'])#allWordsDf.append(pd.Series(listForDf,index=allWordsDf.columns),ignore_index=True)\n",
        "  allWordsDf.to_csv(fileName)\n",
        "\n",
        "\n",
        "# Describe citations for a subset of data\n",
        "# @sub: \n",
        "# 'all' = all papers\n",
        "# 'aff' = split by affiliation\n",
        "# 'primary' = primary subject area (top 10)\n",
        "# 'cluster' = clustering subject area (top 10)\n",
        "# 'loc' = continent\n",
        "# 'us-ch' = US and China affiliations\n",
        "# 'bigTech' = Apple, Amazon, Microsoft, Google, Facebook, Huawei\n",
        "def describeCitations(sub):\n",
        "  # subset data\n",
        "  if sub == 'all':\n",
        "    dfList = [megaDf]\n",
        "    names = ['all']\n",
        "  elif sub == 'aff':\n",
        "    acaDf = megaDf.loc[(megaDf['academic'] == 1) & (megaDf['mixed'] == 0)]\n",
        "    indDf = megaDf.loc[(megaDf['industry'] == 1) & (megaDf['mixed'] == 0)]\n",
        "    mixedDf = megaDf[megaDf['mixed'] == 1]\n",
        "    dfList = [acaDf,indDf,mixedDf]\n",
        "    names = ['academic','industry','mixed-affiliation']\n",
        "  elif sub == 'primary':\n",
        "    (dfList,names) = generateSbjDfs('primary subject area')\n",
        "  elif sub == 'cluster':\n",
        "    (dfList,names) = generateSbjDfs('clustering subject preference')\n",
        "  elif sub == 'loc':\n",
        "    (dfList,names) = generateContinentDfs()\n",
        "  elif sub =='us-ch':\n",
        "    usOnlyDf = megaDf[megaDf['country'] == \"{'USA'}\"]\n",
        "    chOnlyDf = megaDf[megaDf['country'] == \"{'China'}\"]\n",
        "    dfList = [usOnlyDf,chOnlyDf]\n",
        "    names = ['United States','China']\n",
        "  elif sub == 'bigTech':\n",
        "    appleDf = megaDf[megaDf['affiliations'].str.contains('Apple')]#megaDf['apple' in str(megaDf['affiliations']).lower()]\n",
        "    amazonDf = megaDf[megaDf['affiliations'].str.contains('Amazon')]\n",
        "    microsoftDf = megaDf[megaDf['affiliations'].str.contains('Microsoft')]\n",
        "    googleDf = megaDf[megaDf['affiliations'].str.contains('Google')]\n",
        "    facebookDf = megaDf[megaDf['affiliations'].str.contains('Facebook')]\n",
        "    huaweiDf = megaDf[megaDf['affiliations'].str.contains('Huawei')]\n",
        "    dfList = [appleDf, amazonDf, microsoftDf, googleDf, facebookDf, huaweiDf]\n",
        "    names = ['Apple', 'Amazon', 'Microsoft', 'Google', 'Facebook', 'Huawei']\n",
        "  else:\n",
        "    print(\"Subset string not recognized.\")\n",
        "    return\n",
        "\n",
        "  for df,name in zip(dfList,names):\n",
        "    print(f\"{name} citations:\\n {df['citation count'].describe()}\")\n",
        "\n",
        "\n",
        "# Calculate the word frequencies for a pregenerated subset and outputs to CSV.\n",
        "# @dfList: a list of dataframes\n",
        "# @names: a list of names for the dataframes in the dfList\n",
        "# @fileName: '<path/to/file.csv>'\n",
        "# @wordList: the list of qords to get frequencies for\n",
        "# @numWords: the max number of word frequencies to output. Defaults to all words.\n",
        "# @useNeg: whether or not to use negated words in the list. Defaults to no. \n",
        "def getFreqsForGeneratedSubset(dfList,names,fileName,wordList,numWords=None,useNeg=False):\n",
        "  wfDfList = []\n",
        "  stDfList = []\n",
        "  avgDfList = []\n",
        "\n",
        "  for df, name in zip(dfList, names):\n",
        "    (wDf,sDf,avgDf) = makeFreqDfsWithWordlist(df,wordList,useNeg)\n",
        "    # All frequencies are subset frequencies--\n",
        "    # so the frequency of occurrence in academic, or South American,\n",
        "    # or NLP, etc statements\n",
        "\n",
        "    wCols = ['category','word','word_freq','word_prop'] \n",
        "    wDf['category'] = name\n",
        "    wDf['word'] = wDf.index\n",
        "    wDf = wDf.reset_index(drop=True)\n",
        "    wDf = wDf[wCols]\n",
        "    \n",
        "    sCols = ['category','word','stmt_freq','stmt_prop'] \n",
        "    sDf['category'] = name\n",
        "    sDf['word'] = sDf.index\n",
        "    sDf = sDf.reset_index(drop=True)\n",
        "    sDf = sDf[sCols]\n",
        "    \n",
        "    avgCols = ['category','word','avg_occs']\n",
        "    avgDf['category'] = name\n",
        "    avgDf['word'] = avgDf.index\n",
        "    avgDf = avgDf.reset_index(drop=True)\n",
        "    avgDf = avgDf[avgCols]\n",
        "\n",
        "    wfDfList.append(wDf[:numWords])\n",
        "    stDfList.append(sDf[:numWords])\n",
        "    avgDfList.append(avgDf[:numWords])\n",
        "\n",
        "  mlWfDf = pd.concat(wfDfList).sort_values(['word','category']).set_index(['word','category'])#.to_csv(totalFreqFile)\n",
        "  mlStDf = pd.concat(stDfList).sort_values(['word','category']).set_index(['word','category'])\n",
        "  mlAvgDf = pd.concat(avgDfList).sort_values(['word','category']).set_index(['word','category'])#.to_csv('avg.csv')\n",
        "\n",
        "  megaMerge = mlWfDf.merge(mlStDf,how='outer',on=['word','category'])\n",
        "  megaMerge = megaMerge.merge(mlAvgDf,how='outer',on=['word','category'])\n",
        "  megaMerge.columns = ['total word frequency (count)','total word frequency (proportion)',\n",
        "                       'statement frequency (count)', 'statement frequency (proportion)',\n",
        "                       'average number of occurrences']\n",
        "  megaMerge.to_csv(fileName)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttwQjMzujfe3"
      },
      "source": [
        "### Generating the big database of frequencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bzvvaPgUoTk"
      },
      "source": [
        "splitList = []\n",
        "for split in splits:\n",
        "  splitList.append(generateSubset(split))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "id": "5Mf7U4J4ZV1M",
        "outputId": "6500bc8f-f42b-47ce-e6d5-e79ec5790b70"
      },
      "source": [
        "# Generate big database for standard wordlists with negations\n",
        "\n",
        "fileBase = 'BFD_norm_2'\n",
        "with open('/content/drive/My Drive/NeurIPS_BIS_Analysis/BIS_keywords.txt','r') as listFile:\n",
        "  wordLists = listFile.read().split('\\n')\n",
        "\n",
        "wordLists = [x for x in wordLists if x != '']\n",
        "wordLists = [makeWordList(x) for x in wordLists]\n",
        "for l in wordLists:\n",
        "  l_with_neg = [x + '_neg' for x in l]\n",
        "  l.extend(l_with_neg)\n",
        "\n",
        "idx = 1\n",
        "\n",
        "for wl in wordLists:\n",
        "  \n",
        "  for ind, zipped in enumerate(splitList):\n",
        "    if idx%20 == 0: print(f'working on {idx}')\n",
        "    fileName = f'./{fileBase}/{idx + 1000}_{str(wl[:3])}_{ind}.csv'\n",
        "    \n",
        "    df = zipped[0]\n",
        "    names = zipped[1]\n",
        "\n",
        "    getFreqsForGeneratedSubset(df,names,fileName,wl,useNeg=True)\n",
        "    idx += 1\n",
        "print('Completed.')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "working on 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-7ca6b02212c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzipped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mgetFreqsForGeneratedSubset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfileName\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0museNeg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0midx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Completed.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-4291bc4d5bf1>\u001b[0m in \u001b[0;36mgetFreqsForGeneratedSubset\u001b[0;34m(dfList, names, fileName, wordList, numWords, useNeg)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m     \u001b[0;34m(\u001b[0m\u001b[0mwDf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msDf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mavgDf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmakeFreqDfsWithWordlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwordList\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0museNeg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m     \u001b[0;31m# All frequencies are subset frequencies--\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;31m# so the frequency of occurrence in academic, or South American,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-d66251dd8f96>\u001b[0m in \u001b[0;36mmakeFreqDfsWithWordlist\u001b[0;34m(df, wordlist, useNeg)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m       \u001b[0mstrippedTar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m       \u001b[0mnumOccs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{strippedTar} '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mjoinedStmt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mnumOccs\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/re.py\u001b[0m in \u001b[0;36mfindall\u001b[0;34m(pattern, string, flags)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m     Empty matches are included in the result.\"\"\"\n\u001b[0;32m--> 225\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfinditer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prYYiCHlzKx9"
      },
      "source": [
        "# Zip it up, then turn into one excel file in separate file\n",
        "shutil.make_archive(fileBase, 'zip', fileBase)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ye2B09n8digA"
      },
      "source": [
        "# Generate big database for synonym wordlists\n",
        "# For each list of words, generate for each cluster:\n",
        "# Total frequency\n",
        "# Statement frequency\n",
        "# Average number of occurrences \n",
        "fileRoot = 'BFD_exp_5-8'\n",
        "with open('/content/drive/My Drive/NeurIPS_BIS_Analysis/synonym_list.txt','r') as listFile:\n",
        "  wordLists = listFile.read().split('\\n')\n",
        "\n",
        "wordLists = [x for x in wordLists if x != '']\n",
        "wordLists = [makeWordList(x) for x in wordLists]\n",
        "for l in wordLists:\n",
        "  l_with_neg = [x + '_neg' for x in l]\n",
        "  l.extend(l_with_neg)\n",
        "\n",
        "idx = 1\n",
        "\n",
        "for wl in wordLists:\n",
        "  for ind, zipped in enumerate(splitList):\n",
        "    if idx%20 == 0: print(f'working on {idx}')\n",
        "    fileName = f'./{fileRoot}/{idx + 1000}_{str(wl[:3])}_{ind}.csv'\n",
        "\n",
        "    df = zipped[0]\n",
        "    names = zipped[1]\n",
        "\n",
        "    getFreqsForGeneratedSubset(df,names,fileName,wl,useNeg=True)\n",
        "    idx += 1\n",
        "print('Completed.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EHYD_XdzGzK"
      },
      "source": [
        "# zip it all up\n",
        "# Conversion to excel took place in separate file\n",
        "shutil.make_archive(fileRoot, 'zip', fileRoot)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zywj8nKxE6xI"
      },
      "source": [
        "### Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYZEF2ywtQct"
      },
      "source": [
        "# Most common affiliations\n",
        "affs = megaDf['affiliations']\n",
        "regex = r\"'\\w*\\s*\\w*'\"\n",
        "affDict = {}\n",
        "\n",
        "for row in affs:\n",
        "  matches = re.findall(regex,row)\n",
        "  for match in matches:\n",
        "    match = match.lower()[1:-1]\n",
        "    if match in affDict:\n",
        "      affDict[match] += 1\n",
        "    else:\n",
        "      affDict[match] = 1\n",
        "\n",
        "toPop = []\n",
        "for k,v in affDict.items():\n",
        "  if 'university' in k and k != 'shanghaitech university': # find and combine universities\n",
        "    for k1,v1 in affDict.items(): # find prefix\n",
        "      if k1 in k and k != k1:\n",
        "        affDict[k] += v1\n",
        "        toPop.append(k1)\n",
        "        break\n",
        "\n",
        "# manual cleaning\n",
        "affDict['deepmind'] += affDict['google deepmind']\n",
        "toPop.append('google deepmind')\n",
        "affDict['element ai'] += affDict['elementai']\n",
        "toPop.append('elementai')\n",
        "affDict['eth zürich'] += affDict['eth zurich']\n",
        "toPop.append('eth zurich')\n",
        "affDict['facebook'] += affDict['facebook inc']\n",
        "toPop.append('facebook inc')\n",
        "affDict['facebook'] += affDict['facebook research']\n",
        "toPop.append('facebook research')\n",
        "affDict['facebook'] += affDict['facebook ai']\n",
        "toPop.append('facebook ai')\n",
        "affDict['facebook'] += affDict['fair']\n",
        "toPop.append('fair')\n",
        "affDict['google'] += affDict['google inc']\n",
        "toPop.append('google inc')\n",
        "affDict['google'] += affDict['google llc']\n",
        "toPop.append('google llc')\n",
        "affDict['google'] += affDict['google research']\n",
        "toPop.append('google research')\n",
        "affDict['google'] += affDict['google brain']\n",
        "toPop.append('google brain')\n",
        "affDict['google'] += affDict['google ai']\n",
        "toPop.append('google ai')\n",
        "affDict['google'] += affDict['google health']\n",
        "toPop.append('google health')\n",
        "affDict['amazon'] += affDict['amazon aws']\n",
        "toPop.append('amazon aws')\n",
        "affDict['huawei'] += affDict['huawei technologies']\n",
        "toPop.append('huawei technologies')\n",
        "affDict['huawei'] += affDict['huawei noah']\n",
        "toPop.append('huawei noah')\n",
        "affDict['ibm'] += affDict['ibm corp']\n",
        "toPop.append('ibm corp')\n",
        "affDict['ibm'] += affDict['ibm research']\n",
        "toPop.append('ibm research')\n",
        "affDict['salesforce'] += affDict['salesforce research']\n",
        "toPop.append('salesforce research')\n",
        "affDict['intel'] += affDict['intel corporation']\n",
        "toPop.append('intel corporation')\n",
        "affDict['jp morgan'] += affDict['jpmorgan']\n",
        "toPop.append('jpmorgan')\n",
        "affDict['linkedin'] += affDict['linkedin corporation']\n",
        "toPop.append('linkedin corporation')\n",
        "affDict['megvii'] += affDict['megvii technology']\n",
        "toPop.append('megvii technology')\n",
        "affDict['microsoft'] += affDict['microsoft corporation']\n",
        "toPop.append('microsoft corporation')\n",
        "affDict['microsoft'] += affDict['microsoft research']\n",
        "toPop.append('microsoft research')\n",
        "affDict['baidu'] += affDict['baidu research']\n",
        "toPop.append('baidu research')\n",
        "affDict['adobe'] += affDict['adobe research']\n",
        "toPop.append('adobe research')\n",
        "affDict['amazon'] += affDict['amazon research']\n",
        "toPop.append('amazon research')\n",
        "affDict['vinai'] += affDict['vinai research']\n",
        "toPop.append('vinai research')\n",
        "affDict['sensetime'] += affDict['sensetime research']\n",
        "toPop.append('sensetime research')\n",
        "affDict['samsung'] += affDict['samsung research']\n",
        "toPop.append('samsung research')\n",
        "affDict['samsung'] += affDict['samsung sds']\n",
        "toPop.append('samsung sds')\n",
        "affDict['sas institute'] += affDict['sas']\n",
        "toPop.append('sas')\n",
        "affDict['mit'] += affDict['mit ']\n",
        "toPop.append('mit ')\n",
        "affDict['éts montréal'] += affDict['ets montreal']\n",
        "toPop.append('ets montreal')\n",
        "affDict['nvidia'] += affDict['nvidia corporation']\n",
        "toPop.append('nvidia corporation')\n",
        "affDict['nvidia'] += affDict['nvidia research']\n",
        "toPop.append('nvidia research')\n",
        "affDict['nnaisense'] += affDict['nnaisense sa']\n",
        "toPop.append('nnaisense sa')\n",
        "affDict['intel labs'] += affDict['intel']\n",
        "toPop.append('intel')\n",
        "affDict['cmu'] += affDict['carnegie']\n",
        "toPop.append('carnegie')\n",
        "affDict['radboud university'] += affDict['radboud universiteit']\n",
        "toPop.append('radboud universiteit')\n",
        "affDict['riken'] += affDict['riken aip']\n",
        "toPop.append('riken aip')\n",
        "affDict['eth zürich'] += affDict['ethz']\n",
        "toPop.append('ethz')\n",
        "affDict['sungkyunkwan university'] += affDict['sunkyunkwan university']\n",
        "toPop.append('sunkyunkwan university')\n",
        "affDict['uber'] += affDict['uber atg']\n",
        "toPop.append('uber atg')\n",
        "affDict['georgia tech'] += affDict['gatech']\n",
        "toPop.append('gatech')\n",
        "affDict['telecom paristech'] += affDict['telecom paristec']\n",
        "toPop.append('telecom paristec')\n",
        "affDict['tsinghua university'] += affDict['tsinghua univeristy']\n",
        "toPop.append('tsinghua univeristy')\n",
        "affDict['tsinghua university'] += affDict['tsinghua univiersity']\n",
        "toPop.append('tsinghua univiersity')\n",
        "affDict['oxford university'] += affDict['u oxford']\n",
        "toPop.append('u oxford')\n",
        "affDict['uw madison'] += affDict['uw']\n",
        "toPop.append('uw')\n",
        "affDict['waymo'] += affDict['waymo llc']\n",
        "toPop.append('waymo llc')\n",
        "affDict['weizmann institute'] += affDict['weizmann']\n",
        "toPop.append('weizmann')\n",
        "affDict['weizmann institute'] += affDict['institute weizmann']\n",
        "toPop.append('institute weizmann')\n",
        "affDict['yale university'] += affDict['yale univ']\n",
        "toPop.append('yale univ')\n",
        "\n",
        "for p in toPop:\n",
        "  affDict.pop(p)\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5NHMfYjxAyTL"
      },
      "source": [
        "pd.DataFrame.from_dict(affDict,orient='index').to_csv('aff_freqs_updated_3-8.csv')"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFSgPWVfD8n7"
      },
      "source": [
        "# Country frequencies\n",
        "countries = megaDf['country']\n",
        "countries\n",
        "countryDict = {}\n",
        "\n",
        "for row in countries:\n",
        "  matches = re.findall(regex,row)\n",
        "  for match in matches:\n",
        "    match = match.lower()[1:-1]\n",
        "    if match in countryDict:\n",
        "      countryDict[match] += 1\n",
        "    else:\n",
        "      countryDict[match] = 1\n",
        "\n",
        "pd.DataFrame.from_dict(countryDict,orient='index').to_csv('country_freqs.csv')"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lsm2SzA39Yd1"
      },
      "source": [
        "# Make list of DataFrames and names for a split\n",
        "# @sub: \n",
        "# 'all' = all papers\n",
        "# 'aff' = split by affiliation\n",
        "# 'primary' = primary subject area (top 10)\n",
        "# 'cluster' = clustering subject area (top 10)\n",
        "# 'loc' = continent\n",
        "# 'us-ch' = US and China affiliations\n",
        "# 'bigTech' = Apple, Amazon, Microsoft, Google, Facebook, Huawei\n",
        "# Returns: list of DataFrames and list of names corresponding to each DataFrame\n",
        "def makeDfandNames(sub):\n",
        "  dfList = []\n",
        "  names = []\n",
        "\n",
        "  if sub == 'all':\n",
        "    dfList = [megaDf]\n",
        "    names = ['all']\n",
        "  elif sub == 'aff':\n",
        "    acaDf = megaDf.loc[(megaDf['academic'] == 1) & (megaDf['mixed'] == 0)]\n",
        "    indDf = megaDf.loc[(megaDf['industry'] == 1) & (megaDf['mixed'] == 0)]\n",
        "    mixedDf = megaDf[megaDf['mixed'] == 1]\n",
        "    dfList = [acaDf,indDf,mixedDf]\n",
        "    names = ['academic','industry','mixed-affiliation']\n",
        "  elif sub == 'primary':\n",
        "    (dfList,names) = generateSbjDfs('primary subject area')\n",
        "  elif sub == 'cluster':\n",
        "    (dfList,names) = generateSbjDfs('clustering subject preference')\n",
        "  elif sub == 'loc':\n",
        "    (dfList,names) = generateContinentDfs()\n",
        "  elif sub == 'us-ch':\n",
        "    usOnlyDf = megaDf[megaDf['country'] == \"{'USA'}\"]\n",
        "    chOnlyDf = megaDf[megaDf['country'] == \"{'China'}\"]\n",
        "    dfList = [usOnlyDf,chOnlyDf]\n",
        "    names = ['United States','China']\n",
        "  elif sub == 'bigTech':\n",
        "    appleDf = megaDf[megaDf['affiliations'].str.contains('Apple')]\n",
        "    amazonDf = megaDf[megaDf['affiliations'].str.contains('Amazon')]\n",
        "    microsoftDf = megaDf[megaDf['affiliations'].str.contains('Microsoft')]\n",
        "    googleDf = megaDf[megaDf['affiliations'].str.contains('Google')]\n",
        "    facebookDf = megaDf[megaDf['affiliations'].str.contains('Facebook')]\n",
        "    huaweiDf = megaDf[megaDf['affiliations'].str.contains('Huawei')]\n",
        "    dfList = [appleDf, amazonDf, microsoftDf, googleDf, facebookDf, huaweiDf]\n",
        "    names = ['Apple', 'Amazon', 'Microsoft', 'Google', 'Facebook', 'Huawei']\n",
        "  else:\n",
        "      print(\"Subset string not recognized.\")\n",
        "      return\n",
        "  \n",
        "  return (dfList, names)\n",
        "\n",
        "\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YwCBoYgENzDU"
      },
      "source": [
        "# Generate descriptive stats\n",
        "for sub in ['cluster','primary']:\n",
        "  (dfs, names) = makeDfandNames(sub)\n",
        "  fullDf = pd.concat(dfs)\n",
        "  if sub != 'all':\n",
        "    dfs.insert(0,fullDf)\n",
        "\n",
        "    names.insert(0,'<full set>')\n",
        "  listForDf = []\n",
        "\n",
        "  for (df,name) in zip(dfs,names):\n",
        "    numStmts = len(df)\n",
        "    pctTot = numStmts/1898\n",
        "    pctSplit = numStmts/len(fullDf)\n",
        "    avgWc = df['word count'].mean()\n",
        "    medWc = df['word count'].median()\n",
        "    maxWc = df['word count'].max()\n",
        "    minWc = df['word count'].min()\n",
        "\n",
        "    avgSc = df['sentence count'].mean()\n",
        "    medSc = df['sentence count'].median()\n",
        "    maxSc = df['sentence count'].max()\n",
        "    minSc = df['sentence count'].min()\n",
        "\n",
        "    numOptOut = len(df[df['opt out']=='TRUE'])\n",
        "    numNoOptOut = len(df[df['opt out']=='FALSE'])\n",
        "    unOptOut = numStmts - numOptOut - numNoOptOut\n",
        "    if numOptOut + numNoOptOut != 0: optOutMProp = numOptOut/(numOptOut + numNoOptOut)\n",
        "    else: optOutMProp = float('nan')\n",
        "    optOutTProp = numOptOut/numStmts\n",
        "\n",
        "    numAmbOptOut = len(df[df['ambiguous opt out']=='TRUE'])\n",
        "    numNoAmbOptOut = len(df[df['ambiguous opt out']=='FALSE'])\n",
        "    unAmbOptOut = numStmts - numNoAmbOptOut - numAmbOptOut\n",
        "    if numAmbOptOut + numNoAmbOptOut != 0: ambOptOutMProp = numAmbOptOut/(numAmbOptOut + numNoAmbOptOut)\n",
        "    else: ambOptOutMProp = float('nan')\n",
        "    ambOptOutTProp = numAmbOptOut/numStmts\n",
        "\n",
        "    listForDf.append([sub,name,numStmts,pctTot,pctSplit,avgWc,medWc,maxWc,minWc,\n",
        "                      avgSc,medSc,maxSc,minSc,\n",
        "                      numOptOut,numOptOut+numNoOptOut,unOptOut,\n",
        "                      optOutMProp,optOutTProp,\n",
        "                      numAmbOptOut,numAmbOptOut+numNoAmbOptOut,unAmbOptOut,\n",
        "                      ambOptOutMProp,ambOptOutTProp])\n",
        "\n",
        "\n",
        "  cols = ['split','subset','statement count','proportion of total','proportion of split','average word count',\n",
        "          'median word count','max word count','min word count',\n",
        "          'average sentence count','median sentence count','max sentence count','min sentence count',\n",
        "          'explicit opt out count', 'marked explicit opt out count', 'unmarked opt out count', \n",
        "          'explicit opt out (proportion of marked)','explicit opt out (proportion of total)',\n",
        "          'ambiguous opt out count','marked ambiguous opt out count', 'unmarked ambiguous opt out count',\n",
        "          'ambiguous opt out (proportion of marked)','ambiguous opt out (proportion of total)']\n",
        "  subDf = pd.DataFrame.from_records(listForDf,columns=cols)\n",
        "  subDf.to_csv(f'./descriptive_stats/{sub}_descr.csv')\n",
        "\n",
        "\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "unm5vab7YawZ",
        "outputId": "1c0ecfe8-2399-4577-97fa-f73e76adab69"
      },
      "source": [
        "shutil.make_archive('descriptive_stats', 'zip', 'descriptive_stats')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/descriptive_stats.zip'"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWvCqu9VDNVc"
      },
      "source": [
        "# Creates CSV with top numWords words, excluding stopwords\n",
        "def getTopWords(numWords, stopwords, fileName, col='impact statement'):\n",
        "  for sub in splits:\n",
        "    (dfs, names) = makeDfandNames(sub)\n",
        "    fullDf = pd.concat(dfs)\n",
        "    if col != 'impact statement': useList = True\n",
        "    else: useList = False\n",
        "\n",
        "    if sub != 'all':\n",
        "      dfs.insert(0,fullDf)\n",
        "      names.insert(0,'<full set>')\n",
        "\n",
        "    listForDf = []\n",
        "\n",
        "    for (df,name) in zip(dfs,names):\n",
        "      # concatenate all statements, split into words, and get most common\n",
        "      if not useList: \n",
        "        statements = ' '.join(statement for statement in df[col])\n",
        "        split = statements.split(' ')\n",
        "      else: \n",
        "        split = ' '.join(' '.join(statement) for statement in df[col])\n",
        "      split = [x.translate(str.maketrans('', '', string.punctuation)).lower().strip() for x in split]\n",
        "\n",
        "      words = [x for x in split if x not in stopwords] # exclude stopwords\n",
        "      count = Counter(words)\n",
        "      top = count.most_common(numWords)\n",
        "      listForDf.append([sub,name,top])\n",
        "\n",
        "    cols = ['split','subset','words']\n",
        "    topWordsDf = pd.DataFrame.from_records(listForDf,columns=cols)\n",
        "    topWordsDf.to_csv(f'./top_words/{sub}_{fileName}.csv')\n",
        "\n",
        "def create_human_readable(array_tuple):\n",
        "    d1 = {}\n",
        "    for i in array_tuple:\n",
        "        d1[i[0]] = i[1]\n",
        "    df = pd.DataFrame(d1.items(), columns=['Word', 'Count'])\n",
        "    return df"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03ZoN1fAGiaV"
      },
      "source": [
        "stops = set(STOPWORDS)\n",
        "stops.update(['the','we','may','in','this','our','','\\n'])\n",
        "getTopWords(20,stops,'top_20_regStops')\n",
        "\n",
        "getTopWords(40,stops,'top_40_regStops')\n",
        "\n",
        "techStops = stops.update(['algorithm','algorithms','model','data','models','neural','system','systems'])\n",
        "getTopWords(20,stops,'top_20_techStops')\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWLpjzUtR1GN"
      },
      "source": [
        "stops = set(STOPWORDS)\n",
        "stops.update(['the','we','may','in','this','our','','\\n'])\n",
        "getTopWords(500,stops,'top_500_regStops_with_double_negs','bis_with_neg')"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iz8pWGB38_-R"
      },
      "source": [
        "stops = set(STOPWORDS)\n",
        "stops.update(['the','we','may','in','this','our','','\\n'])\n",
        "getTopWords(500,stops,'top_500_regStops_with_negs','bis_with_neg')"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Pzk_TmWYJOX6",
        "outputId": "442e60b4-a033-488c-b9ef-6b32f1aee68b"
      },
      "source": [
        "shutil.make_archive('top_words', 'zip', 'top_words')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/top_words.zip'"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHVBpmntzDEw"
      },
      "source": [
        "###Academic vs Industry"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFmNsAaICBLX"
      },
      "source": [
        "acaDf = megaDf.loc[(megaDf['academic'] == 1) & (megaDf['mixed'] == 0)]\n",
        "indDf = megaDf.loc[(megaDf['industry'] == 1) & (megaDf['mixed'] == 0)]\n",
        "mixedDf = megaDf[megaDf['mixed'] == 1]\n"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "23DO97x0CBLa",
        "outputId": "0be59343-b113-44ca-bff3-df709e44ff57"
      },
      "source": [
        "acaWC = acaDf['word count'].mean()\n",
        "acaSC = acaDf['sentence count'].mean()\n",
        "acaWM = acaDf['word count'].median()\n",
        "acaSM = acaDf['sentence count'].median()\n",
        "\n",
        "print(\"Word count:\")\n",
        "display(acaDf['word count'].describe())\n",
        "print(\"Sentence count:\")\n",
        "display(acaDf['sentence count'].describe())"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word count:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "count    1163.000000\n",
              "mean      161.409286\n",
              "std       118.763658\n",
              "min         2.000000\n",
              "25%        80.500000\n",
              "50%       132.000000\n",
              "75%       217.000000\n",
              "max       800.000000\n",
              "Name: word count, dtype: float64"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "text": [
            "Sentence count:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "count    1163.000000\n",
              "mean        6.901978\n",
              "std         4.972630\n",
              "min         1.000000\n",
              "25%         3.000000\n",
              "50%         6.000000\n",
              "75%         9.000000\n",
              "max        45.000000\n",
              "Name: sentence count, dtype: float64"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "B9aX3ECLCBLb",
        "outputId": "cc6c44c3-6423-4633-a276-b122f497f394"
      },
      "source": [
        "indWC = indDf['word count'].mean()\n",
        "indSC = indDf['sentence count'].mean()\n",
        "indWM = acaDf['word count'].median()\n",
        "indSM = acaDf['sentence count'].median()\n",
        "\n",
        "print(\"Word count:\")\n",
        "display(indDf['word count'].describe())\n",
        "print(\"Sentence count:\")\n",
        "display(indDf['sentence count'].describe())"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word count:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "count    122.000000\n",
              "mean     187.139344\n",
              "std      142.700479\n",
              "min        5.000000\n",
              "25%       90.250000\n",
              "50%      140.000000\n",
              "75%      242.000000\n",
              "max      730.000000\n",
              "Name: word count, dtype: float64"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "text": [
            "Sentence count:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "count    122.000000\n",
              "mean       7.991803\n",
              "std        6.227765\n",
              "min        1.000000\n",
              "25%        4.000000\n",
              "50%        6.000000\n",
              "75%       10.000000\n",
              "max       33.000000\n",
              "Name: sentence count, dtype: float64"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "ZM2_ff-8CBLb",
        "outputId": "d688e449-fd5a-40f6-af08-4f93ba2b6b7d"
      },
      "source": [
        "mixWC = mixedDf['word count'].mean()\n",
        "mixSC = mixedDf['sentence count'].mean()\n",
        "mixWM = mixedDf['word count'].median()\n",
        "mixSM = mixedDf['sentence count'].median()\n",
        "\n",
        "print(\"Word count:\")\n",
        "display(mixedDf['word count'].describe())\n",
        "print(\"Sentence count:\")\n",
        "display(mixedDf['sentence count'].describe())"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word count:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "count     613.000000\n",
              "mean      179.424144\n",
              "std       210.047784\n",
              "min         5.000000\n",
              "25%        88.000000\n",
              "50%       143.000000\n",
              "75%       227.000000\n",
              "max      4337.000000\n",
              "Name: word count, dtype: float64"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "text": [
            "Sentence count:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "count    613.000000\n",
              "mean       7.835237\n",
              "std        7.928846\n",
              "min        1.000000\n",
              "25%        4.000000\n",
              "50%        6.000000\n",
              "75%       10.000000\n",
              "max      150.000000\n",
              "Name: sentence count, dtype: float64"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wbg5E0VTzMBx"
      },
      "source": [
        "### US vs China"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "-pDOqUrxCBLc",
        "outputId": "413cd84d-90d2-458d-d1b5-b7f79113eb79"
      },
      "source": [
        "# Includes all papers that have some American affiliation\n",
        "usaDf = megaDf[megaDf['country'].str.contains('USA')]\n",
        "print(\"Word count:\")\n",
        "display(usaDf['word count'].describe())\n",
        "print(\"Sentence count:\")\n",
        "display(usaDf['sentence count'].describe())"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word count:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "count    1229.000000\n",
              "mean      171.516680\n",
              "std       170.881478\n",
              "min         2.000000\n",
              "25%        83.000000\n",
              "50%       138.000000\n",
              "75%       220.000000\n",
              "max      4337.000000\n",
              "Name: word count, dtype: float64"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "text": [
            "Sentence count:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "count    1229.000000\n",
              "mean        7.297803\n",
              "std         6.518765\n",
              "min         1.000000\n",
              "25%         4.000000\n",
              "50%         6.000000\n",
              "75%         9.000000\n",
              "max       150.000000\n",
              "Name: sentence count, dtype: float64"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "CPUqZml1CBLd",
        "outputId": "33a02270-45db-4831-c13b-d1d7196d07c7"
      },
      "source": [
        "# America-only affiliation\n",
        "usaOnlyDf = megaDf[megaDf['country'] == \"{'USA'}\"]\n",
        "usaOnlyDf\n",
        "print(\"Word count:\")\n",
        "display(usaOnlyDf['word count'].describe())\n",
        "print(\"Sentence count:\")\n",
        "display(usaOnlyDf['sentence count'].describe())"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word count:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "count     795.000000\n",
              "mean      182.000000\n",
              "std       196.031599\n",
              "min         5.000000\n",
              "25%        88.500000\n",
              "50%       142.000000\n",
              "75%       231.500000\n",
              "max      4337.000000\n",
              "Name: word count, dtype: float64"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "text": [
            "Sentence count:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "count    795.000000\n",
              "mean       7.593711\n",
              "std        7.309785\n",
              "min        1.000000\n",
              "25%        4.000000\n",
              "50%        6.000000\n",
              "75%       10.000000\n",
              "max      150.000000\n",
              "Name: sentence count, dtype: float64"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "XkZIb0GWCBLd",
        "outputId": "946a4b23-b21e-494b-d216-8b81a2035a11"
      },
      "source": [
        "chinaOnlyDf = megaDf[megaDf['country'] == \"{'China'}\"]\n",
        "chinaOnlyDf\n",
        "print(\"Word count:\")\n",
        "display(chinaOnlyDf['word count'].describe())\n",
        "print(\"Sentence count:\")\n",
        "display(chinaOnlyDf['sentence count'].describe())"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word count:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "count     97.000000\n",
              "mean     135.134021\n",
              "std       96.908852\n",
              "min        9.000000\n",
              "25%       72.000000\n",
              "50%      122.000000\n",
              "75%      163.000000\n",
              "max      576.000000\n",
              "Name: word count, dtype: float64"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "text": [
            "Sentence count:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "count    97.000000\n",
              "mean      6.628866\n",
              "std       5.029744\n",
              "min       1.000000\n",
              "25%       3.000000\n",
              "50%       6.000000\n",
              "75%       8.000000\n",
              "max      32.000000\n",
              "Name: sentence count, dtype: float64"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMYR2CJv9Naj"
      },
      "source": [
        "### Keywords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18BX1YW6_YWk"
      },
      "source": [
        "# Prints academic/industry/mixed statistics for individual word\n",
        "# @word: the word to consider\n",
        "def statsForWord(word):\n",
        "  megWordDf = megaDf[megaDf['impact statement'].str.contains(word)]\n",
        "  print(f\"proportion of all statements containing '{word}': {len(megWordDf)/len(megaDf)}\")\n",
        "  acaWordDf = acaDf[acaDf['impact statement'].str.contains(word)]\n",
        "  print(f\"proportion of academic statements containing '{word}': {len(acaWordDf)/len(acaDf)}\")\n",
        "  indWordDf = indDf[indDf['impact statement'].str.contains(word)]\n",
        "  print(f\"proportion of industry statements containing '{word}': {len(indWordDf)/len(indDf)}\")\n",
        "  mixWordDf = mixedDf[mixedDf['impact statement'].str.contains(word)]\n",
        "  print(f\"proportion of mixed statements containing '{word}': {len(mixWordDf)/len(mixedDf)}\")\n",
        "  \n",
        "  print(f\"All '{word}' statements means:\")\n",
        "  print(f\"word count: {megWordDf['word count'].mean()}\")\n",
        "  print(f\"sentence count: {megWordDf['sentence count'].mean()}\")\n",
        "\n",
        "  print(f\"Academic '{word}' statements means:\")\n",
        "  print(f\"word count: {acaWordDf['word count'].mean()}\")\n",
        "  print(f\"sentence count: {acaWordDf['sentence count'].mean()}\")\n",
        "\n",
        "  print(f\"Industry '{word}' statements means:\")\n",
        "  print(f\"word count: {indWordDf['word count'].mean()}\")\n",
        "  print(f\"sentence count: {indWordDf['sentence count'].mean()}\")\n",
        "\n",
        "  print(f\"Mixed '{word}' statements means:\")\n",
        "  print(f\"word count: {mixWordDf['word count'].mean()}\")\n",
        "  print(f\"sentence count: {mixWordDf['sentence count'].mean()}\")"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FknschcJ6QLF",
        "outputId": "ee5a7bf2-4622-41f0-8848-78a81f45827b"
      },
      "source": [
        "# benefit\n",
        "statsForWord('benefit')"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "proportion of all statements containing 'benefit': 0.23182297154899895\n",
            "proportion of academic statements containing 'benefit': 0.22785898538263114\n",
            "proportion of industry statements containing 'benefit': 0.22950819672131148\n",
            "proportion of mixed statements containing 'benefit': 0.2398042414355628\n",
            "All 'benefit' statements means:\n",
            "word count: 213.9\n",
            "sentence count: 9.320454545454545\n",
            "Academic 'benefit' statements means:\n",
            "word count: 214.07169811320756\n",
            "sentence count: 9.19622641509434\n",
            "Industry 'benefit' statements means:\n",
            "word count: 224.64285714285714\n",
            "sentence count: 9.678571428571429\n",
            "Mixed 'benefit' statements means:\n",
            "word count: 211.54421768707482\n",
            "sentence count: 9.476190476190476\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_mjr5Wg9b0U",
        "outputId": "e7aa29c0-2055-4eb3-b2c4-7fe61088e422"
      },
      "source": [
        "statsForWord('method')"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "proportion of all statements containing 'method': 0.4315068493150685\n",
            "proportion of academic statements containing 'method': 0.40498710232158214\n",
            "proportion of industry statements containing 'method': 0.45901639344262296\n",
            "proportion of mixed statements containing 'method': 0.4763458401305057\n",
            "All 'method' statements means:\n",
            "word count: 205.93040293040292\n",
            "sentence count: 8.800976800976802\n",
            "Academic 'method' statements means:\n",
            "word count: 197.06369426751593\n",
            "sentence count: 8.365180467091296\n",
            "Industry 'method' statements means:\n",
            "word count: 232.85714285714286\n",
            "sentence count: 9.982142857142858\n",
            "Mixed 'method' statements means:\n",
            "word count: 215.06849315068493\n",
            "sentence count: 9.277397260273972\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5Lss9JKlYKs"
      },
      "source": [
        "### Clustering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sOwi-JDQ946G",
        "outputId": "3acc1457-8c95-47ac-fd1e-6ad007dcbe61"
      },
      "source": [
        "# Poking around the subject areas\n",
        "sbjDf = megaDf.groupby(by='primary subject area')['title'].count()\n",
        "sbjDf = sbjDf.sort_values(ascending=False)\n",
        "top10 = sbjDf[:10]\n",
        "\n",
        "top10"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "primary subject area\n",
              "Applications -> Computer Vision                                  96\n",
              "Deep Learning                                                    63\n",
              "Reinforcement Learning and Planning -> Reinforcement Learning    57\n",
              "Reinforcement Learning and Planning                              57\n",
              "Deep Learning -> Analysis and Understanding of Deep Networks     56\n",
              "Deep Learning -> Generative Models                               54\n",
              "Algorithms -> Representation Learning                            50\n",
              "Theory -> Statistical Learning Theory                            48\n",
              "Algorithms -> Bandit Algorithms                                  40\n",
              "Applications -> Natural Language Processing                      35\n",
              "Name: title, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBWErF_NnEBT"
      },
      "source": [
        "# Make a word could for a dataframe\n",
        "# @df: the dataframe to look at\n",
        "# @filename: the root of the file to output image to (outputs to <filename>.png)\n",
        "# @stopwords: the words to exclude\n",
        "def makeWordCloud(df, filename, stopwords=STOPWORDS):\n",
        "  text = ' '.join(statement for statement in df['impact statement'])\n",
        "  wc = WordCloud(stopwords = stopwords, background_color='white').generate(text=text)\n",
        "  plt.imshow(wc, interpolation='bilinear')\n",
        "  plt.axis('off')\n",
        "  plt.show()\n",
        "  wc.to_file(f'{filename}.png')"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfWk1anFuEAJ"
      },
      "source": [
        "##Top Words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wb9Q9_pQuIom"
      },
      "source": [
        "### Geography\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5S63eMtiuTXj",
        "outputId": "930d0e15-aea8-4cf7-d7f0-62dd9e5bfd01"
      },
      "source": [
        "# Poking around countries/continents\n",
        "regex = r\"'(\\w*\\s*\\w*)'\"\n",
        "countrySet = set()\n",
        "\n",
        "countries = megaDf['country']\n",
        "for row in countries:\n",
        "  matches = re.findall(regex,row)\n",
        "  for match in matches:\n",
        "    countrySet.add(match)\n",
        "print(\"set\")\n",
        "countrySet\n"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "set\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Australia',\n",
              " 'Austria',\n",
              " 'Belgium',\n",
              " 'Brazil',\n",
              " 'Canada',\n",
              " 'Chile',\n",
              " 'China',\n",
              " 'Cyprus',\n",
              " 'Czech Republic',\n",
              " 'Denmark',\n",
              " 'Egypt',\n",
              " 'Finland',\n",
              " 'France',\n",
              " 'Germany',\n",
              " 'Greece',\n",
              " 'Hong Kong',\n",
              " 'Independent',\n",
              " 'India',\n",
              " 'Iran',\n",
              " 'Israel',\n",
              " 'Italy',\n",
              " 'Japan',\n",
              " 'Malaysia',\n",
              " 'Netherlands',\n",
              " 'Norway',\n",
              " 'Pakistan',\n",
              " 'Poland',\n",
              " 'Portugal',\n",
              " 'Qatar',\n",
              " 'Romania',\n",
              " 'Russia',\n",
              " 'Saudi Arabia',\n",
              " 'Singapore',\n",
              " 'South Africa',\n",
              " 'South Korea',\n",
              " 'Spain',\n",
              " 'Sweden',\n",
              " 'Switzerland',\n",
              " 'Taiwan',\n",
              " 'Thailand',\n",
              " 'Turkey',\n",
              " 'UAE',\n",
              " 'UK',\n",
              " 'USA',\n",
              " 'Vietnam'}"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZlJ5ecThqi4"
      },
      "source": [
        "countryRegex = r\"'(\\w*\\s*\\w*)'\"\n"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLg3Fj0mBcKT"
      },
      "source": [
        "# Export countries for labelling\n",
        "contDf = pd.DataFrame(columns=['country','continent'])\n",
        "contDf['country'] = list(countrySet)\n",
        "contDf.to_csv('countries_with_continents.csv')"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVt4CuHaDZ4j"
      },
      "source": [
        "# Read in updated list\n",
        "#ccSheet = gc.open('/content/drive/MyDrive/NeurIPS_BIS_Analysis/countries_with_continents_labelled.csv')\n",
        "auth.authenticate_user()\n",
        "gc = gspread.authorize(GoogleCredentials.get_application_default())\n",
        "\n",
        "ccSheet = gc.open('labelled_countries').sheet1"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "kQ6FU6K4GZ4x",
        "outputId": "c4b3d1a0-303e-403f-c840-abb47c9859a6"
      },
      "source": [
        "ccRows = ccSheet.get_all_values()\n",
        "upContDf = pd.DataFrame.from_records(ccRows)\n",
        "upContDf.columns = upContDf.iloc[0]\n",
        "upContDf = upContDf.drop(upContDf.index[0])\n",
        "upContDf = upContDf.drop(upContDf.columns[0],axis=1)\n",
        "upContDf#"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>country</th>\n",
              "      <th>continent</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Iran</td>\n",
              "      <td>Asia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Denmark</td>\n",
              "      <td>Europe</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>India</td>\n",
              "      <td>Asia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>China</td>\n",
              "      <td>Asia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>South Africa</td>\n",
              "      <td>Africa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>South Korea</td>\n",
              "      <td>Asia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Cyprus</td>\n",
              "      <td>Europe</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Independent</td>\n",
              "      <td>N/a</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Austria</td>\n",
              "      <td>Europe</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Brazil</td>\n",
              "      <td>South America</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Turkey</td>\n",
              "      <td>Asia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Pakistan</td>\n",
              "      <td>Asia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>Portugal</td>\n",
              "      <td>Europe</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>UAE</td>\n",
              "      <td>Asia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>Norway</td>\n",
              "      <td>Europe</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>Egypt</td>\n",
              "      <td>Africa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>Poland</td>\n",
              "      <td>Europe</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>France</td>\n",
              "      <td>Europe</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>Hong Kong</td>\n",
              "      <td>Asia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>Romania</td>\n",
              "      <td>Europe</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>Canada</td>\n",
              "      <td>North America</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>Czech Republic</td>\n",
              "      <td>Europe</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>Vietnam</td>\n",
              "      <td>Asia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>Chile</td>\n",
              "      <td>South America</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>Switzerland</td>\n",
              "      <td>Europe</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>Qatar</td>\n",
              "      <td>Asia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>Germany</td>\n",
              "      <td>Europe</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>Italy</td>\n",
              "      <td>Europe</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>Singapore</td>\n",
              "      <td>Asia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>Malaysia</td>\n",
              "      <td>Asia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>Sweden</td>\n",
              "      <td>Europe</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>Russia</td>\n",
              "      <td>Europe</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>Belgium</td>\n",
              "      <td>Europe</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>Saudi Arabia</td>\n",
              "      <td>Asia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>Netherlands</td>\n",
              "      <td>Europe</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>Japan</td>\n",
              "      <td>Asia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>Taiwan</td>\n",
              "      <td>Asia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>Australia</td>\n",
              "      <td>Oceania</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>Israel</td>\n",
              "      <td>Asia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>Spain</td>\n",
              "      <td>Europe</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>Greece</td>\n",
              "      <td>Europe</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>Finland</td>\n",
              "      <td>Europe</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>Thailand</td>\n",
              "      <td>Asia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>UK</td>\n",
              "      <td>Europe</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>USA</td>\n",
              "      <td>North America</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "0          country      continent\n",
              "1             Iran           Asia\n",
              "2          Denmark         Europe\n",
              "3            India           Asia\n",
              "4            China           Asia\n",
              "5     South Africa         Africa\n",
              "6      South Korea           Asia\n",
              "7           Cyprus         Europe\n",
              "8      Independent            N/a\n",
              "9          Austria         Europe\n",
              "10          Brazil  South America\n",
              "11          Turkey           Asia\n",
              "12        Pakistan           Asia\n",
              "13        Portugal         Europe\n",
              "14             UAE           Asia\n",
              "15          Norway         Europe\n",
              "16           Egypt         Africa\n",
              "17          Poland         Europe\n",
              "18          France         Europe\n",
              "19       Hong Kong           Asia\n",
              "20         Romania         Europe\n",
              "21          Canada  North America\n",
              "22  Czech Republic         Europe\n",
              "23         Vietnam           Asia\n",
              "24           Chile  South America\n",
              "25     Switzerland         Europe\n",
              "26           Qatar           Asia\n",
              "27         Germany         Europe\n",
              "28           Italy         Europe\n",
              "29       Singapore           Asia\n",
              "30        Malaysia           Asia\n",
              "31          Sweden         Europe\n",
              "32          Russia         Europe\n",
              "33         Belgium         Europe\n",
              "34    Saudi Arabia           Asia\n",
              "35     Netherlands         Europe\n",
              "36           Japan           Asia\n",
              "37          Taiwan           Asia\n",
              "38       Australia        Oceania\n",
              "39          Israel           Asia\n",
              "40           Spain         Europe\n",
              "41          Greece         Europe\n",
              "42         Finland         Europe\n",
              "43        Thailand           Asia\n",
              "44              UK         Europe\n",
              "45             USA  North America"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1orvuJ9N6g5"
      },
      "source": [
        "asia = upContDf[upContDf['continent'] == 'Asia']\n",
        "asiaList = list(asia['country'].values)\n",
        "africa = upContDf[upContDf['continent'] == 'Africa']\n",
        "africaList = list(africa['country'].values)\n",
        "northAm = upContDf[upContDf['continent'] == 'North America']\n",
        "northAmList = list(northAm['country'].values)\n",
        "southAm = upContDf[upContDf['continent'] == 'South America']\n",
        "southAmList = list(southAm['country'].values)\n",
        "europe = upContDf[upContDf['continent'] == 'Europe']\n",
        "europeList = list(europe['country'].values)\n",
        "oceania = upContDf[upContDf['continent'] == 'Oceania']\n",
        "oceaniaList = list(oceania['country'].values)\n"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRVYW1PtPA9z"
      },
      "source": [
        "lists = [asiaList,africaList,northAmList,southAmList,europeList,oceaniaList]\n",
        "\n",
        "def makeDf(relList):\n",
        "  rowList = []\n",
        "\n",
        "  for index, row in megaDf.iterrows():\n",
        "    countries = re.findall(countryRegex,row['country'])\n",
        "    for country in countries:\n",
        "      if country in relList:\n",
        "        rowList.append(row)\n",
        "        break\n",
        "  return pd.DataFrame(rowList)\n",
        "                         "
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_W-3OeoHjeC7"
      },
      "source": [
        "# Separate dfs\n",
        "\n",
        "#for contList, contDf in zip(lists, dfs):\n",
        "#  contDf = makeDf(contList)\n",
        "asiaDf = makeDf(asiaList)\n",
        "africaDf = makeDf(africaList)\n",
        "northAmDf = makeDf(northAmList)\n",
        "southAmDf = makeDf(southAmList)\n",
        "europeDf = makeDf(europeList)\n",
        "oceaniaDf = makeDf(oceaniaList)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UnzcAqnRkd2A",
        "outputId": "f65fd04b-3c08-4c6b-87e2-e4a02cfcdb84"
      },
      "source": [
        "x = 0\n",
        "contDfs = [asiaDf,africaDf,northAmDf,southAmDf,europeDf,oceaniaDf]\n",
        "for df in contDfs:\n",
        "  x += len(df)\n",
        "x\n",
        "# We expect some overlap"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2418"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOghhwq9XT90"
      },
      "source": [
        "### Citations\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3LZ3usiSXVNq",
        "outputId": "d9e9becc-482c-4ab6-ad47-788a79e40797"
      },
      "source": [
        "# Looking at citations\n",
        "megaDf[megaDf['citation count'] != 0]"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>paper title (separate scrape)</th>\n",
              "      <th>paper authors (separate scrape)</th>\n",
              "      <th>title</th>\n",
              "      <th>paper identifier</th>\n",
              "      <th>paper link</th>\n",
              "      <th>impact statement</th>\n",
              "      <th>impact title</th>\n",
              "      <th>word count</th>\n",
              "      <th>sentence count</th>\n",
              "      <th>citation count</th>\n",
              "      <th>has positive</th>\n",
              "      <th>has negative</th>\n",
              "      <th>opt out</th>\n",
              "      <th>ambiguous opt out</th>\n",
              "      <th>has impact statement</th>\n",
              "      <th>manually corrected</th>\n",
              "      <th>human review</th>\n",
              "      <th>Image based PDF</th>\n",
              "      <th>paper title (subjects)</th>\n",
              "      <th>primary subject area</th>\n",
              "      <th>secondary subject areas</th>\n",
              "      <th>clustering subject preference</th>\n",
              "      <th>authors</th>\n",
              "      <th>affiliations</th>\n",
              "      <th>academic</th>\n",
              "      <th>industry</th>\n",
              "      <th>mixed</th>\n",
              "      <th>country</th>\n",
              "      <th>bis_with_neg</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Fast and Flexible Temporal Point Processes wit...</td>\n",
              "      <td>Oleksandr Shchur, Nicholas Gao, Marin Biloš, S...</td>\n",
              "      <td>Fast and Flexible Temporal Point Processes wit...</td>\n",
              "      <td>00ac8ed3b4327bdd4ebbebcb2ba10a00</td>\n",
              "      <td>https://proceedings.neurips.cc/paper/2020/file...</td>\n",
              "      <td>Existing works have applied TPPs and MJPs for ...</td>\n",
              "      <td>Broader impact</td>\n",
              "      <td>120</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>FALSE</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>TRUE</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>Fast and Flexible Temporal Point Processes wit...</td>\n",
              "      <td>Probabilistic Methods</td>\n",
              "      <td>Algorithms -&gt; Density Estimation; Applications...</td>\n",
              "      <td>Probabilistic methods and inference</td>\n",
              "      <td>['Oleksandr Shchur', ' Nicholas Gao', ' Marin ...</td>\n",
              "      <td>{'Technical University of Munich'}</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>{'Germany'}</td>\n",
              "      <td>[Existing, works, have, applied, TPPs, and, MJ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Synbols: Probing Learning Algorithms with Synt...</td>\n",
              "      <td>Alexandre Lacoste, Pau Rodríguez López, Freder...</td>\n",
              "      <td>Synbols: Probing Learning Algorithms with Synt...</td>\n",
              "      <td>0169cf885f882efd795951253db5cdfb</td>\n",
              "      <td>https://proceedings.neurips.cc/paper/2020/file...</td>\n",
              "      <td>The introduction of benchmark new datasets has...</td>\n",
              "      <td>Broader Impact</td>\n",
              "      <td>386</td>\n",
              "      <td>13</td>\n",
              "      <td>3</td>\n",
              "      <td>TRUE</td>\n",
              "      <td>TRUE</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>TRUE</td>\n",
              "      <td>TRUE</td>\n",
              "      <td>TRUE</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>Synbols: Probing Learning Algorithms with Synt...</td>\n",
              "      <td>Data, Challenges, Implementations, and Softwar...</td>\n",
              "      <td>Algorithms -&gt; Active Learning; Algorithms -&gt; F...</td>\n",
              "      <td>Datasets, challenges, software</td>\n",
              "      <td>['Alexandre Lacoste', ' Pau Rodríguez López', ...</td>\n",
              "      <td>{'ElementAI', 'MILA', 'University of British C...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>{'Canada'}</td>\n",
              "      <td>[The, introduction, of, benchmark, new, datase...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>Cascaded Text Generation with Markov Transformers</td>\n",
              "      <td>Yuntian Deng, Alexander Rush</td>\n",
              "      <td>Cascaded Text Generation with Markov Transformers</td>\n",
              "      <td>01a0683665f38d8e5e567b3b15ca98bf</td>\n",
              "      <td>https://proceedings.neurips.cc/paper/2020/file...</td>\n",
              "      <td>Our work proposes an alternative approach to b...</td>\n",
              "      <td>Broader Impact</td>\n",
              "      <td>204</td>\n",
              "      <td>9</td>\n",
              "      <td>11</td>\n",
              "      <td>TRUE</td>\n",
              "      <td>FALSE</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>TRUE</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>Cascaded Text Generation with Markov Transformers</td>\n",
              "      <td>Applications -&gt; Natural Language Processing</td>\n",
              "      <td>Deep Learning -&gt; Generative Models</td>\n",
              "      <td>Natural language processing</td>\n",
              "      <td>['Yuntian Deng', ' Alexander Rush']</td>\n",
              "      <td>{'Harvard University', 'Cornell University'}</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>{'USA'}</td>\n",
              "      <td>[Our, work, proposes, an, alternative, approac...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>Algorithmic recourse under imperfect causal kn...</td>\n",
              "      <td>Amir-Hossein Karimi, Bodo Julius von Kügelgen,...</td>\n",
              "      <td>Algorithmic recourse under imperfect causal kn...</td>\n",
              "      <td>02a3c7fb3f489288ae6942498498db20</td>\n",
              "      <td>https://proceedings.neurips.cc/paper/2020/file...</td>\n",
              "      <td>Our work falls into the domain of explainable ...</td>\n",
              "      <td>Broader Impact</td>\n",
              "      <td>694</td>\n",
              "      <td>20</td>\n",
              "      <td>10</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>FALSE</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>TRUE</td>\n",
              "      <td>TRUE</td>\n",
              "      <td>TRUE</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>Algorithmic recourse under imperfect causal kn...</td>\n",
              "      <td>Social Aspects of Machine Learning -&gt; Fairness...</td>\n",
              "      <td>Probabilistic Methods -&gt; Causal Inference</td>\n",
              "      <td>Social aspects of machine learning (e.g., fair...</td>\n",
              "      <td>['Hossein Karimi', ' Julius von Kügelgen', ' B...</td>\n",
              "      <td>{'UWaterloo', 'MPI for Intelligent Systems', '...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>{'Canada', 'Germany'}</td>\n",
              "      <td>[Our, work, falls, into, the, domain, of, expl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>Coresets for Regressions with Panel Data</td>\n",
              "      <td>Lingxiao Huang, K Sudhir, Nisheeth Vishnoi</td>\n",
              "      <td>Coresets for Regressions with Panel Data</td>\n",
              "      <td>03287fcce194dbd958c2ec5b33705912</td>\n",
              "      <td>https://proceedings.neurips.cc/paper/2020/file...</td>\n",
              "      <td>Many organizations have to routinely outsource...</td>\n",
              "      <td>Broader impact</td>\n",
              "      <td>257</td>\n",
              "      <td>11</td>\n",
              "      <td>4</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>TRUE</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>TRUE</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>Coresets for Regressions with Panel Data</td>\n",
              "      <td>Algorithms -&gt; Data Compression</td>\n",
              "      <td>Algorithms -&gt; Regression</td>\n",
              "      <td>Core machine learning methods (e.g., supervise...</td>\n",
              "      <td>['Lingxiao Huang', ' K Sudhir', ' Nisheeth Vis...</td>\n",
              "      <td>{'Yale University', 'EPFL'}</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>{'USA', 'Switzerland'}</td>\n",
              "      <td>[Many, organizations, have, to, routinely, out...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1864</th>\n",
              "      <td>Measuring Systematic Generalization in Neural ...</td>\n",
              "      <td>Nicolas Gontier, Koustuv Sinha, Siva Reddy, Ch...</td>\n",
              "      <td>Measuring Systematic Generalization in Neural ...</td>\n",
              "      <td>fc84ad56f9f547eb89c72b9bac209312</td>\n",
              "      <td>https://proceedings.neurips.cc/paper/2020/file...</td>\n",
              "      <td>Transformer based models have been very effect...</td>\n",
              "      <td>Broader Impact</td>\n",
              "      <td>211</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>TRUE</td>\n",
              "      <td>TRUE</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>TRUE</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>Measuring Systematic Generalization in Neural ...</td>\n",
              "      <td>Applications -&gt; Natural Language Processing</td>\n",
              "      <td>Algorithms -&gt; Structured Prediction; Deep Lear...</td>\n",
              "      <td>Natural language processing</td>\n",
              "      <td>['Nicolas Gontier', ' Koustuv Sinha', ' Siva R...</td>\n",
              "      <td>{'McGill University / Mila / FAIR', 'Montreal ...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>{'Canada', 'France', 'USA'}</td>\n",
              "      <td>[Transformer, based, models, have, been, very,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1867</th>\n",
              "      <td>Fast Matrix Square Roots with Applications to ...</td>\n",
              "      <td>Geoff Pleiss, Martin Jankowiak, David Eriksson...</td>\n",
              "      <td>Fast Matrix Square Roots with Applications to ...</td>\n",
              "      <td>fcf55a303b71b84d326fb1d06e332a26</td>\n",
              "      <td>https://proceedings.neurips.cc/paper/2020/file...</td>\n",
              "      <td>This paper introduces an algorithm to improve ...</td>\n",
              "      <td>Broader Impact</td>\n",
              "      <td>413</td>\n",
              "      <td>18</td>\n",
              "      <td>8</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>FALSE</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>TRUE</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>Fast Matrix Square Roots with Applications to ...</td>\n",
              "      <td>Probabilistic Methods -&gt; Gaussian Processes</td>\n",
              "      <td></td>\n",
              "      <td>Probabilistic methods and inference</td>\n",
              "      <td>['Geoff Pleiss', ' Martin Jankowiak', ' David ...</td>\n",
              "      <td>{'Facebook', 'University of Pennsylvania', 'Co...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>{'USA'}</td>\n",
              "      <td>[This, paper, introduces, an, algorithm, to, i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1889</th>\n",
              "      <td>Multi-agent Trajectory Prediction with Fuzzy Q...</td>\n",
              "      <td>Nitin Kamra, Hao Zhu, Dweep Kumarbhai Trivedi,...</td>\n",
              "      <td>Multi-agent Trajectory Prediction with Fuzzy Q...</td>\n",
              "      <td>fe87435d12ef7642af67d9bc82a8b3cd</td>\n",
              "      <td>https://proceedings.neurips.cc/paper/2020/file...</td>\n",
              "      <td>We have presented a general architecture for m...</td>\n",
              "      <td>Broader Impact</td>\n",
              "      <td>132</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>FALSE</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>TRUE</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>TRUE</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>Multi-agent Trajectory Prediction with Fuzzy Q...</td>\n",
              "      <td>Deep Learning -&gt; Attention Models</td>\n",
              "      <td>Algorithms -&gt; Relational Learning; Deep Learni...</td>\n",
              "      <td>Other applications (e.g., robotics, biology, c...</td>\n",
              "      <td>['Nitin Kamra', ' Hao Zhu', ' Dweep Kumarbhai ...</td>\n",
              "      <td>{'University of Southern California', 'Peking ...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>{'USA', 'China'}</td>\n",
              "      <td>[We, have, presented, a, general, architecture...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1892</th>\n",
              "      <td>Can the Brain Do Backpropagation? --- Exact Im...</td>\n",
              "      <td>Yuhang Song, Thomas Lukasiewicz, Zhenghua Xu, ...</td>\n",
              "      <td>Can the Brain Do Backpropagation? — Exact Impl...</td>\n",
              "      <td>fec87a37cdeec1c6ecf8181c0aa2d3bf</td>\n",
              "      <td>https://proceedings.neurips.cc/paper/2020/file...</td>\n",
              "      <td>This work shows that backpropagation in artifi...</td>\n",
              "      <td>Broader Impact</td>\n",
              "      <td>280</td>\n",
              "      <td>12</td>\n",
              "      <td>2</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>FALSE</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>TRUE</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>TRUE</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>Can the Brain Do Backpropagation? --- Exact Im...</td>\n",
              "      <td>Deep Learning -&gt; Biologically Plausible Deep N...</td>\n",
              "      <td>Deep Learning -&gt; Analysis and Understanding of...</td>\n",
              "      <td>Neuroscience and cognitive science</td>\n",
              "      <td></td>\n",
              "      <td>{'University of Oxford', 'Hebei University of ...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>{'UK', 'China'}</td>\n",
              "      <td>[This, work, shows, that, backpropagation, in,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1893</th>\n",
              "      <td>Manifold GPLVMs for discovering non-Euclidean ...</td>\n",
              "      <td>Kristopher Jensen, Ta-Chu Kao, Marco Tripodi, ...</td>\n",
              "      <td>Manifold GPLVMs for discovering non-Euclidean ...</td>\n",
              "      <td>fedc604da8b0f9af74b6cfc0fab2163c</td>\n",
              "      <td>https://proceedings.neurips.cc/paper/2020/file...</td>\n",
              "      <td>There are two broad fields which we expect mig...</td>\n",
              "      <td>Broader Impact</td>\n",
              "      <td>244</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>FALSE</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>TRUE</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>TRUE</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>Manifold GPLVMs for discovering non-Euclidean ...</td>\n",
              "      <td>Neuroscience and Cognitive Science -&gt; Neurosci...</td>\n",
              "      <td>Algorithms -&gt; Nonlinear Dimensionality Reducti...</td>\n",
              "      <td>Neuroscience and cognitive science</td>\n",
              "      <td>['Kristopher Jensen', 'Chu Kao', ' Marco Tripo...</td>\n",
              "      <td>{'University of Cambridge', 'MRC', 'Cambridge'}</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>{'UK'}</td>\n",
              "      <td>[There, are, two, broad, fields, which, we, ex...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>409 rows × 29 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "0                         paper title (separate scrape)  ...                                       bis_with_neg\n",
              "7     Fast and Flexible Temporal Point Processes wit...  ...  [Existing, works, have, applied, TPPs, and, MJ...\n",
              "12    Synbols: Probing Learning Algorithms with Synt...  ...  [The, introduction, of, benchmark, new, datase...\n",
              "15    Cascaded Text Generation with Markov Transformers  ...  [Our, work, proposes, an, alternative, approac...\n",
              "23    Algorithmic recourse under imperfect causal kn...  ...  [Our, work, falls, into, the, domain, of, expl...\n",
              "28             Coresets for Regressions with Panel Data  ...  [Many, organizations, have, to, routinely, out...\n",
              "...                                                 ...  ...                                                ...\n",
              "1864  Measuring Systematic Generalization in Neural ...  ...  [Transformer, based, models, have, been, very,...\n",
              "1867  Fast Matrix Square Roots with Applications to ...  ...  [This, paper, introduces, an, algorithm, to, i...\n",
              "1889  Multi-agent Trajectory Prediction with Fuzzy Q...  ...  [We, have, presented, a, general, architecture...\n",
              "1892  Can the Brain Do Backpropagation? --- Exact Im...  ...  [This, work, shows, that, backpropagation, in,...\n",
              "1893  Manifold GPLVMs for discovering non-Euclidean ...  ...  [There, are, two, broad, fields, which, we, ex...\n",
              "\n",
              "[409 rows x 29 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-iQKBJKX7TN",
        "outputId": "42c94fc9-615d-49df-c349-b0e564304854"
      },
      "source": [
        "describeCitations('primary')"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Applications -> Computer Vision citations:\n",
            " count    96.000000\n",
            "mean      0.750000\n",
            "std       2.357519\n",
            "min       0.000000\n",
            "25%       0.000000\n",
            "50%       0.000000\n",
            "75%       0.000000\n",
            "max      15.000000\n",
            "Name: citation count, dtype: float64\n",
            "Deep Learning citations:\n",
            " count    63.000000\n",
            "mean      0.841270\n",
            "std       2.343187\n",
            "min       0.000000\n",
            "25%       0.000000\n",
            "50%       0.000000\n",
            "75%       0.000000\n",
            "max      10.000000\n",
            "Name: citation count, dtype: float64\n",
            "Reinforcement Learning and Planning -> Reinforcement Learning citations:\n",
            " count    57.000000\n",
            "mean      0.964912\n",
            "std       1.945369\n",
            "min       0.000000\n",
            "25%       0.000000\n",
            "50%       0.000000\n",
            "75%       1.000000\n",
            "max      10.000000\n",
            "Name: citation count, dtype: float64\n",
            "Reinforcement Learning and Planning citations:\n",
            " count    57.000000\n",
            "mean      0.912281\n",
            "std       2.523325\n",
            "min       0.000000\n",
            "25%       0.000000\n",
            "50%       0.000000\n",
            "75%       0.000000\n",
            "max      12.000000\n",
            "Name: citation count, dtype: float64\n",
            "Deep Learning -> Analysis and Understanding of Deep Networks citations:\n",
            " count    56.000000\n",
            "mean      0.910714\n",
            "std       2.056270\n",
            "min       0.000000\n",
            "25%       0.000000\n",
            "50%       0.000000\n",
            "75%       0.000000\n",
            "max       8.000000\n",
            "Name: citation count, dtype: float64\n",
            "Deep Learning -> Generative Models citations:\n",
            " count    54.000000\n",
            "mean      0.500000\n",
            "std       1.209241\n",
            "min       0.000000\n",
            "25%       0.000000\n",
            "50%       0.000000\n",
            "75%       0.000000\n",
            "max       7.000000\n",
            "Name: citation count, dtype: float64\n",
            "Algorithms -> Representation Learning citations:\n",
            " count    50.000000\n",
            "mean      1.460000\n",
            "std       3.881984\n",
            "min       0.000000\n",
            "25%       0.000000\n",
            "50%       0.000000\n",
            "75%       1.000000\n",
            "max      23.000000\n",
            "Name: citation count, dtype: float64\n",
            "Theory -> Statistical Learning Theory citations:\n",
            " count    48.000000\n",
            "mean      0.625000\n",
            "std       2.038199\n",
            "min       0.000000\n",
            "25%       0.000000\n",
            "50%       0.000000\n",
            "75%       0.000000\n",
            "max      10.000000\n",
            "Name: citation count, dtype: float64\n",
            "Algorithms -> Bandit Algorithms citations:\n",
            " count    40.000000\n",
            "mean      0.675000\n",
            "std       1.939964\n",
            "min       0.000000\n",
            "25%       0.000000\n",
            "50%       0.000000\n",
            "75%       0.000000\n",
            "max       9.000000\n",
            "Name: citation count, dtype: float64\n",
            "Applications -> Natural Language Processing citations:\n",
            " count    35.000000\n",
            "mean      1.657143\n",
            "std       4.221394\n",
            "min       0.000000\n",
            "25%       0.000000\n",
            "50%       0.000000\n",
            "75%       1.500000\n",
            "max      22.000000\n",
            "Name: citation count, dtype: float64\n",
            "Social Aspects of Machine Learning -> Privacy, Anonymity, and Security citations:\n",
            " count    34.000000\n",
            "mean      1.000000\n",
            "std       2.662876\n",
            "min       0.000000\n",
            "25%       0.000000\n",
            "50%       0.000000\n",
            "75%       0.000000\n",
            "max      11.000000\n",
            "Name: citation count, dtype: float64\n",
            "Probabilistic Methods -> Causal Inference citations:\n",
            " count    33.000000\n",
            "mean      0.484848\n",
            "std       1.277723\n",
            "min       0.000000\n",
            "25%       0.000000\n",
            "50%       0.000000\n",
            "75%       0.000000\n",
            "max       6.000000\n",
            "Name: citation count, dtype: float64\n",
            "Algorithms -> Adversarial Learning citations:\n",
            " count    33.000000\n",
            "mean      0.757576\n",
            "std       1.714466\n",
            "min       0.000000\n",
            "25%       0.000000\n",
            "50%       0.000000\n",
            "75%       1.000000\n",
            "max       7.000000\n",
            "Name: citation count, dtype: float64\n",
            "Theory -> Game Theory and Computational Economics citations:\n",
            " count    32.000000\n",
            "mean      2.000000\n",
            "std       8.199134\n",
            "min       0.000000\n",
            "25%       0.000000\n",
            "50%       0.000000\n",
            "75%       0.000000\n",
            "max      46.000000\n",
            "Name: citation count, dtype: float64\n",
            "Optimization citations:\n",
            " count    30.000000\n",
            "mean      2.000000\n",
            "std       6.258787\n",
            "min       0.000000\n",
            "25%       0.000000\n",
            "50%       0.000000\n",
            "75%       0.000000\n",
            "max      33.000000\n",
            "Name: citation count, dtype: float64\n",
            "Optimization -> Convex Optimization citations:\n",
            " count    28.000000\n",
            "mean      0.285714\n",
            "std       1.329359\n",
            "min       0.000000\n",
            "25%       0.000000\n",
            "50%       0.000000\n",
            "75%       0.000000\n",
            "max       7.000000\n",
            "Name: citation count, dtype: float64\n",
            "Theory -> Computational Learning Theory citations:\n",
            " count    28.000000\n",
            "mean      0.321429\n",
            "std       1.090483\n",
            "min       0.000000\n",
            "25%       0.000000\n",
            "50%       0.000000\n",
            "75%       0.000000\n",
            "max       5.000000\n",
            "Name: citation count, dtype: float64\n",
            "Optimization -> Non-Convex Optimization citations:\n",
            " count    25.0\n",
            "mean      0.0\n",
            "std       0.0\n",
            "min       0.0\n",
            "25%       0.0\n",
            "50%       0.0\n",
            "75%       0.0\n",
            "max       0.0\n",
            "Name: citation count, dtype: float64\n",
            "Algorithms citations:\n",
            " count    24.000000\n",
            "mean      0.833333\n",
            "std       1.833663\n",
            "min       0.000000\n",
            "25%       0.000000\n",
            "50%       0.000000\n",
            "75%       0.250000\n",
            "max       6.000000\n",
            "Name: citation count, dtype: float64\n",
            "Social Aspects of Machine Learning -> Fairness, Accountability, and Transparency citations:\n",
            " count    24.000000\n",
            "mean      2.625000\n",
            "std       4.062688\n",
            "min       0.000000\n",
            "25%       0.000000\n",
            "50%       0.500000\n",
            "75%       3.000000\n",
            "max      13.000000\n",
            "Name: citation count, dtype: float64\n",
            "Algorithms -> Online Learning citations:\n",
            " count    23.0\n",
            "mean      0.0\n",
            "std       0.0\n",
            "min       0.0\n",
            "25%       0.0\n",
            "50%       0.0\n",
            "75%       0.0\n",
            "max       0.0\n",
            "Name: citation count, dtype: float64\n",
            "Deep Learning -> Visualization, Interpretability, and Explainability citations:\n",
            " count    23.000000\n",
            "mean      1.782609\n",
            "std       3.605003\n",
            "min       0.000000\n",
            "25%       0.000000\n",
            "50%       0.000000\n",
            "75%       2.000000\n",
            "max      13.000000\n",
            "Name: citation count, dtype: float64\n",
            "Algorithms -> Meta-Learning citations:\n",
            " count    23.000000\n",
            "mean      0.434783\n",
            "std       1.273010\n",
            "min       0.000000\n",
            "25%       0.000000\n",
            "50%       0.000000\n",
            "75%       0.000000\n",
            "max       6.000000\n",
            "Name: citation count, dtype: float64\n",
            "Theory citations:\n",
            " count    22.0\n",
            "mean      0.0\n",
            "std       0.0\n",
            "min       0.0\n",
            "25%       0.0\n",
            "50%       0.0\n",
            "75%       0.0\n",
            "max       0.0\n",
            "Name: citation count, dtype: float64\n",
            "Algorithms -> Classification citations:\n",
            " count    22.000000\n",
            "mean      0.227273\n",
            "std       0.611930\n",
            "min       0.000000\n",
            "25%       0.000000\n",
            "50%       0.000000\n",
            "75%       0.000000\n",
            "max       2.000000\n",
            "Name: citation count, dtype: float64\n",
            "Deep Learning -> Efficient Training Methods citations:\n",
            " count    22.000000\n",
            "mean      1.272727\n",
            "std       1.881788\n",
            "min       0.000000\n",
            "25%       0.000000\n",
            "50%       0.000000\n",
            "75%       2.000000\n",
            "max       5.000000\n",
            "Name: citation count, dtype: float64\n",
            "Probabilistic Methods -> Variational Inference citations:\n",
            " count    21.000000\n",
            "mean      0.714286\n",
            "std       1.419255\n",
            "min       0.000000\n",
            "25%       0.000000\n",
            "50%       0.000000\n",
            "75%       1.000000\n",
            "max       5.000000\n",
            "Name: citation count, dtype: float64\n",
            "Reinforcement Learning and Planning -> Multi-Agent RL citations:\n",
            " count    20.000000\n",
            "mean      1.300000\n",
            "std       2.848822\n",
            "min       0.000000\n",
            "25%       0.000000\n",
            "50%       0.000000\n",
            "75%       1.000000\n",
            "max      11.000000\n",
            "Name: citation count, dtype: float64\n",
            "Deep Learning -> Optimization for Deep Networks citations:\n",
            " count    20.000000\n",
            "mean      0.650000\n",
            "std       1.268028\n",
            "min       0.000000\n",
            "25%       0.000000\n",
            "50%       0.000000\n",
            "75%       0.250000\n",
            "max       4.000000\n",
            "Name: citation count, dtype: float64\n",
            "Optimization -> Stochastic Optimization citations:\n",
            " count    20.000000\n",
            "mean      1.300000\n",
            "std       4.305443\n",
            "min       0.000000\n",
            "25%       0.000000\n",
            "50%       0.000000\n",
            "75%       0.000000\n",
            "max      19.000000\n",
            "Name: citation count, dtype: float64\n",
            "Algorithms -> AutoML citations:\n",
            " count    19.0\n",
            "mean      1.0\n",
            "std       2.0\n",
            "min       0.0\n",
            "25%       0.0\n",
            "50%       0.0\n",
            "75%       1.0\n",
            "max       8.0\n",
            "Name: citation count, dtype: float64\n",
            "Algorithms -> Uncertainty Estimation citations:\n",
            " count    19.000000\n",
            "mean      0.578947\n",
            "std       1.304513\n",
            "min       0.000000\n",
            "25%       0.000000\n",
            "50%       0.000000\n",
            "75%       0.000000\n",
            "max       4.000000\n",
            "Name: citation count, dtype: float64\n",
            "Algorithms -> Clustering citations:\n",
            " count    18.000000\n",
            "mean      1.166667\n",
            "std       1.790498\n",
            "min       0.000000\n",
            "25%       0.000000\n",
            "50%       0.000000\n",
            "75%       2.000000\n",
            "max       6.000000\n",
            "Name: citation count, dtype: float64\n",
            "Neuroscience and Cognitive Science citations:\n",
            " count    18.000000\n",
            "mean      0.500000\n",
            "std       1.248529\n",
            "min       0.000000\n",
            "25%       0.000000\n",
            "50%       0.000000\n",
            "75%       0.000000\n",
            "max       5.000000\n",
            "Name: citation count, dtype: float64\n",
            "Algorithms -> Unsupervised Learning citations:\n",
            " count    18.000000\n",
            "mean      0.444444\n",
            "std       1.247219\n",
            "min       0.000000\n",
            "25%       0.000000\n",
            "50%       0.000000\n",
            "75%       0.000000\n",
            "max       5.000000\n",
            "Name: citation count, dtype: float64\n",
            "Probabilistic Methods -> Gaussian Processes citations:\n",
            " count    17.000000\n",
            "mean      1.411765\n",
            "std       2.399448\n",
            "min       0.000000\n",
            "25%       0.000000\n",
            "50%       0.000000\n",
            "75%       2.000000\n",
            "max       8.000000\n",
            "Name: citation count, dtype: float64\n",
            "Algorithms -> Relational Learning citations:\n",
            " count    17.000000\n",
            "mean      0.941176\n",
            "std       1.784327\n",
            "min       0.000000\n",
            "25%       0.000000\n",
            "50%       0.000000\n",
            "75%       0.000000\n",
            "max       5.000000\n",
            "Name: citation count, dtype: float64\n",
            "Algorithms -> Dynamical Systems citations:\n",
            " count    17.000000\n",
            "mean      0.470588\n",
            "std       1.007326\n",
            "min       0.000000\n",
            "25%       0.000000\n",
            "50%       0.000000\n",
            "75%       0.000000\n",
            "max       3.000000\n",
            "Name: citation count, dtype: float64\n",
            "Social Aspects of Machine Learning -> AI Safety citations:\n",
            " count    16.000000\n",
            "mean      2.125000\n",
            "std       5.162364\n",
            "min       0.000000\n",
            "25%       0.000000\n",
            "50%       0.000000\n",
            "75%       1.000000\n",
            "max      20.000000\n",
            "Name: citation count, dtype: float64\n",
            "Algorithms -> Continual Learning citations:\n",
            " count    16.000000\n",
            "mean      0.437500\n",
            "std       1.209339\n",
            "min       0.000000\n",
            "25%       0.000000\n",
            "50%       0.000000\n",
            "75%       0.000000\n",
            "max       4.000000\n",
            "Name: citation count, dtype: float64\n",
            "Algorithms -> Semi-Supervised Learning citations:\n",
            " count    16.000000\n",
            "mean      0.562500\n",
            "std       1.364734\n",
            "min       0.000000\n",
            "25%       0.000000\n",
            "50%       0.000000\n",
            "75%       0.000000\n",
            "max       4.000000\n",
            "Name: citation count, dtype: float64\n",
            "Neuroscience and Cognitive Science -> Neuroscience citations:\n",
            " count    16.000000\n",
            "mean      0.187500\n",
            "std       0.543906\n",
            "min       0.000000\n",
            "25%       0.000000\n",
            "50%       0.000000\n",
            "75%       0.000000\n",
            "max       2.000000\n",
            "Name: citation count, dtype: float64\n",
            "Probabilistic Methods -> Graphical Models citations:\n",
            " count    15.000000\n",
            "mean      0.200000\n",
            "std       0.560612\n",
            "min       0.000000\n",
            "25%       0.000000\n",
            "50%       0.000000\n",
            "75%       0.000000\n",
            "max       2.000000\n",
            "Name: citation count, dtype: float64\n",
            "Applications citations:\n",
            " count    15.000000\n",
            "mean      0.200000\n",
            "std       0.560612\n",
            "min       0.000000\n",
            "25%       0.000000\n",
            "50%       0.000000\n",
            "75%       0.000000\n",
            "max       2.000000\n",
            "Name: citation count, dtype: float64\n",
            "Deep Learning -> Efficient Inference Methods citations:\n",
            " count    15.000000\n",
            "mean      0.133333\n",
            "std       0.351866\n",
            "min       0.000000\n",
            "25%       0.000000\n",
            "50%       0.000000\n",
            "75%       0.000000\n",
            "max       1.000000\n",
            "Name: citation count, dtype: float64\n",
            "Algorithms -> Kernel Methods citations:\n",
            " count    15.000000\n",
            "mean      0.333333\n",
            "std       0.816497\n",
            "min       0.000000\n",
            "25%       0.000000\n",
            "50%       0.000000\n",
            "75%       0.000000\n",
            "max       3.000000\n",
            "Name: citation count, dtype: float64\n",
            "Algorithms -> Multitask and Transfer Learning citations:\n",
            " count    15.000000\n",
            "mean      0.133333\n",
            "std       0.516398\n",
            "min       0.000000\n",
            "25%       0.000000\n",
            "50%       0.000000\n",
            "75%       0.000000\n",
            "max       2.000000\n",
            "Name: citation count, dtype: float64\n",
            "Reinforcement Learning and Planning -> Model-Based RL citations:\n",
            " count    14.000000\n",
            "mean      1.428571\n",
            "std       3.567250\n",
            "min       0.000000\n",
            "25%       0.000000\n",
            "50%       0.000000\n",
            "75%       0.000000\n",
            "max      13.000000\n",
            "Name: citation count, dtype: float64\n",
            "Algorithms -> Sparsity and Compressed Sensing citations:\n",
            " count    14.000000\n",
            "mean      0.642857\n",
            "std       1.905746\n",
            "min       0.000000\n",
            "25%       0.000000\n",
            "50%       0.000000\n",
            "75%       0.000000\n",
            "max       7.000000\n",
            "Name: citation count, dtype: float64\n",
            "Probabilistic Methods citations:\n",
            " count    12.000000\n",
            "mean      1.500000\n",
            "std       2.393172\n",
            "min       0.000000\n",
            "25%       0.000000\n",
            "50%       0.000000\n",
            "75%       2.750000\n",
            "max       6.000000\n",
            "Name: citation count, dtype: float64\n",
            "Probabilistic Methods -> MCMC citations:\n",
            " count    12.000000\n",
            "mean      0.166667\n",
            "std       0.577350\n",
            "min       0.000000\n",
            "25%       0.000000\n",
            "50%       0.000000\n",
            "75%       0.000000\n",
            "max       2.000000\n",
            "Name: citation count, dtype: float64\n",
            "Optimization -> Discrete Optimization citations:\n",
            " count    12.000000\n",
            "mean      0.250000\n",
            "std       0.621582\n",
            "min       0.000000\n",
            "25%       0.000000\n",
            "50%       0.000000\n",
            "75%       0.000000\n",
            "max       2.000000\n",
            "Name: citation count, dtype: float64\n",
            "Applications -> Computational Biology and Bioinformatics citations:\n",
            " count    12.000000\n",
            "mean      2.166667\n",
            "std       4.217568\n",
            "min       0.000000\n",
            "25%       0.000000\n",
            "50%       0.000000\n",
            "75%       2.500000\n",
            "max      14.000000\n",
            "Name: citation count, dtype: float64\n",
            "Applications -> Health citations:\n",
            " count    12.000000\n",
            "mean      1.750000\n",
            "std       3.980064\n",
            "min       0.000000\n",
            "25%       0.000000\n",
            "50%       0.000000\n",
            "75%       1.250000\n",
            "max      14.000000\n",
            "Name: citation count, dtype: float64\n",
            "Deep Learning -> Supervised Deep Networks citations:\n",
            " count    11.000000\n",
            "mean      0.727273\n",
            "std       1.103713\n",
            "min       0.000000\n",
            "25%       0.000000\n",
            "50%       0.000000\n",
            "75%       1.500000\n",
            "max       3.000000\n",
            "Name: citation count, dtype: float64\n",
            "Applications -> Network Analysis citations:\n",
            " count    11.000000\n",
            "mean      5.363636\n",
            "std       6.975281\n",
            "min       0.000000\n",
            "25%       0.000000\n",
            "50%       3.000000\n",
            "75%       8.500000\n",
            "max      22.000000\n",
            "Name: citation count, dtype: float64\n",
            "Theory -> High-Dimensional Inference citations:\n",
            " count    11.000000\n",
            "mean      0.363636\n",
            "std       0.809040\n",
            "min       0.000000\n",
            "25%       0.000000\n",
            "50%       0.000000\n",
            "75%       0.000000\n",
            "max       2.000000\n",
            "Name: citation count, dtype: float64\n",
            "Algorithms -> Boosting and Ensemble Methods citations:\n",
            " count    11.000000\n",
            "mean      1.090909\n",
            "std       1.758098\n",
            "min       0.000000\n",
            "25%       0.000000\n",
            "50%       0.000000\n",
            "75%       2.000000\n",
            "max       5.000000\n",
            "Name: citation count, dtype: float64\n",
            "Applications -> Time Series Analysis citations:\n",
            " count    10.000000\n",
            "mean      0.200000\n",
            "std       0.632456\n",
            "min       0.000000\n",
            "25%       0.000000\n",
            "50%       0.000000\n",
            "75%       0.000000\n",
            "max       2.000000\n",
            "Name: citation count, dtype: float64\n",
            "Deep Learning -> CNN Architectures citations:\n",
            " count    10.0\n",
            "mean      0.0\n",
            "std       0.0\n",
            "min       0.0\n",
            "25%       0.0\n",
            "50%       0.0\n",
            "75%       0.0\n",
            "max       0.0\n",
            "Name: citation count, dtype: float64\n",
            "Reinforcement Learning and Planning -> Exploration citations:\n",
            " count    10.000000\n",
            "mean      0.400000\n",
            "std       0.966092\n",
            "min       0.000000\n",
            "25%       0.000000\n",
            "50%       0.000000\n",
            "75%       0.000000\n",
            "max       3.000000\n",
            "Name: citation count, dtype: float64\n",
            "Deep Learning -> Adversarial Networks citations:\n",
            " count    10.000000\n",
            "mean      0.900000\n",
            "std       1.595131\n",
            "min       0.000000\n",
            "25%       0.000000\n",
            "50%       0.000000\n",
            "75%       1.000000\n",
            "max       5.000000\n",
            "Name: citation count, dtype: float64\n",
            "Deep Learning -> Biologically Plausible Deep Networks citations:\n",
            " count    9.000000\n",
            "mean     1.666667\n",
            "std      3.082207\n",
            "min      0.000000\n",
            "25%      0.000000\n",
            "50%      0.000000\n",
            "75%      2.000000\n",
            "max      9.000000\n",
            "Name: citation count, dtype: float64\n",
            "Deep Learning -> Recurrent Networks citations:\n",
            " count    9.000000\n",
            "mean     0.444444\n",
            "std      0.726483\n",
            "min      0.000000\n",
            "25%      0.000000\n",
            "50%      0.000000\n",
            "75%      1.000000\n",
            "max      2.000000\n",
            "Name: citation count, dtype: float64\n",
            "Neuroscience and Cognitive Science -> Cognitive Science citations:\n",
            " count    9.000000\n",
            "mean     2.111111\n",
            "std      3.443996\n",
            "min      0.000000\n",
            "25%      0.000000\n",
            "50%      0.000000\n",
            "75%      2.000000\n",
            "max      9.000000\n",
            "Name: citation count, dtype: float64\n",
            "Applications -> Body Pose, Face, and Gesture Analysis citations:\n",
            " count    9.000000\n",
            "mean     0.888889\n",
            "std      2.027588\n",
            "min      0.000000\n",
            "25%      0.000000\n",
            "50%      0.000000\n",
            "75%      0.000000\n",
            "max      6.000000\n",
            "Name: citation count, dtype: float64\n",
            "Theory -> Control Theory citations:\n",
            " count    8.000000\n",
            "mean     0.125000\n",
            "std      0.353553\n",
            "min      0.000000\n",
            "25%      0.000000\n",
            "50%      0.000000\n",
            "75%      0.000000\n",
            "max      1.000000\n",
            "Name: citation count, dtype: float64\n",
            "Reinforcement Learning and Planning -> Markov Decision Processes citations:\n",
            " count    8.00000\n",
            "mean     0.37500\n",
            "std      1.06066\n",
            "min      0.00000\n",
            "25%      0.00000\n",
            "50%      0.00000\n",
            "75%      0.00000\n",
            "max      3.00000\n",
            "Name: citation count, dtype: float64\n",
            "Theory -> Information Theory citations:\n",
            " count    8.0\n",
            "mean     0.0\n",
            "std      0.0\n",
            "min      0.0\n",
            "25%      0.0\n",
            "50%      0.0\n",
            "75%      0.0\n",
            "max      0.0\n",
            "Name: citation count, dtype: float64\n",
            "Optimization -> Submodular Optimization citations:\n",
            " count    8.000000\n",
            "mean     0.875000\n",
            "std      2.474874\n",
            "min      0.000000\n",
            "25%      0.000000\n",
            "50%      0.000000\n",
            "75%      0.000000\n",
            "max      7.000000\n",
            "Name: citation count, dtype: float64\n",
            "Deep Learning -> Attention Models citations:\n",
            " count    8.000000\n",
            "mean     1.875000\n",
            "std      2.850439\n",
            "min      0.000000\n",
            "25%      0.000000\n",
            "50%      0.500000\n",
            "75%      2.500000\n",
            "max      8.000000\n",
            "Name: citation count, dtype: float64\n",
            "Theory -> Models of Learning and Generalization citations:\n",
            " count    8.0\n",
            "mean     0.0\n",
            "std      0.0\n",
            "min      0.0\n",
            "25%      0.0\n",
            "50%      0.0\n",
            "75%      0.0\n",
            "max      0.0\n",
            "Name: citation count, dtype: float64\n",
            "Algorithms -> Regression citations:\n",
            " count    8.000000\n",
            "mean     0.125000\n",
            "std      0.353553\n",
            "min      0.000000\n",
            "25%      0.000000\n",
            "50%      0.000000\n",
            "75%      0.000000\n",
            "max      1.000000\n",
            "Name: citation count, dtype: float64\n",
            "Algorithms -> Components Analysis (e.g., CCA, ICA, LDA, PCA) citations:\n",
            " count    7.0\n",
            "mean     0.0\n",
            "std      0.0\n",
            "min      0.0\n",
            "25%      0.0\n",
            "50%      0.0\n",
            "75%      0.0\n",
            "max      0.0\n",
            "Name: citation count, dtype: float64\n",
            "Applications -> Robotics citations:\n",
            " count    7.000000\n",
            "mean     0.571429\n",
            "std      1.133893\n",
            "min      0.000000\n",
            "25%      0.000000\n",
            "50%      0.000000\n",
            "75%      0.500000\n",
            "max      3.000000\n",
            "Name: citation count, dtype: float64\n",
            "Applications -> Object Detection citations:\n",
            " count    7.0\n",
            "mean     0.0\n",
            "std      0.0\n",
            "min      0.0\n",
            "25%      0.0\n",
            "50%      0.0\n",
            "75%      0.0\n",
            "max      0.0\n",
            "Name: citation count, dtype: float64\n",
            "Algorithms -> Data Compression citations:\n",
            " count    7.000000\n",
            "mean     1.000000\n",
            "std      1.527525\n",
            "min      0.000000\n",
            "25%      0.000000\n",
            "50%      0.000000\n",
            "75%      1.500000\n",
            "max      4.000000\n",
            "Name: citation count, dtype: float64\n",
            "Applications -> Visual Question Answering citations:\n",
            " count    7.000000\n",
            "mean     0.714286\n",
            "std      1.253566\n",
            "min      0.000000\n",
            "25%      0.000000\n",
            "50%      0.000000\n",
            "75%      1.000000\n",
            "max      3.000000\n",
            "Name: citation count, dtype: float64\n",
            "Algorithms -> Few-Shot Learning citations:\n",
            " count    7.000000\n",
            "mean     0.571429\n",
            "std      0.975900\n",
            "min      0.000000\n",
            "25%      0.000000\n",
            "50%      0.000000\n",
            "75%      1.000000\n",
            "max      2.000000\n",
            "Name: citation count, dtype: float64\n",
            "Theory -> Statistical Physics of Learning citations:\n",
            " count    7.0\n",
            "mean     0.0\n",
            "std      0.0\n",
            "min      0.0\n",
            "25%      0.0\n",
            "50%      0.0\n",
            "75%      0.0\n",
            "max      0.0\n",
            "Name: citation count, dtype: float64\n",
            "Reinforcement Learning and Planning -> Planning citations:\n",
            " count    7.000000\n",
            "mean     0.285714\n",
            "std      0.487950\n",
            "min      0.000000\n",
            "25%      0.000000\n",
            "50%      0.000000\n",
            "75%      0.500000\n",
            "max      1.000000\n",
            "Name: citation count, dtype: float64\n",
            "Algorithms -> Spectral Methods citations:\n",
            " count    6.000000\n",
            "mean     0.333333\n",
            "std      0.816497\n",
            "min      0.000000\n",
            "25%      0.000000\n",
            "50%      0.000000\n",
            "75%      0.000000\n",
            "max      2.000000\n",
            "Name: citation count, dtype: float64\n",
            "Neuroscience and Cognitive Science -> Brain Imaging citations:\n",
            " count    6.000000\n",
            "mean     0.166667\n",
            "std      0.408248\n",
            "min      0.000000\n",
            "25%      0.000000\n",
            "50%      0.000000\n",
            "75%      0.000000\n",
            "max      1.000000\n",
            "Name: citation count, dtype: float64\n",
            "Deep Learning -> Deep Autoencoders citations:\n",
            " count    6.000000\n",
            "mean     0.666667\n",
            "std      1.632993\n",
            "min      0.000000\n",
            "25%      0.000000\n",
            "50%      0.000000\n",
            "75%      0.000000\n",
            "max      4.000000\n",
            "Name: citation count, dtype: float64\n",
            "Applications -> Program Understanding and Generation citations:\n",
            " count    6.0\n",
            "mean     0.0\n",
            "std      0.0\n",
            "min      0.0\n",
            "25%      0.0\n",
            "50%      0.0\n",
            "75%      0.0\n",
            "max      0.0\n",
            "Name: citation count, dtype: float64\n",
            "Deep Learning -> Embedding Approaches citations:\n",
            " count     6.000000\n",
            "mean      5.166667\n",
            "std       8.256311\n",
            "min       0.000000\n",
            "25%       0.000000\n",
            "50%       0.500000\n",
            "75%       7.750000\n",
            "max      20.000000\n",
            "Name: citation count, dtype: float64\n",
            "Applications -> Audio and Speech Processing citations:\n",
            " count    6.000000\n",
            "mean     0.166667\n",
            "std      0.408248\n",
            "min      0.000000\n",
            "25%      0.000000\n",
            "50%      0.000000\n",
            "75%      0.000000\n",
            "max      1.000000\n",
            "Name: citation count, dtype: float64\n",
            "Theory -> Large Deviations and Asymptotic Analysis citations:\n",
            " count    6.0\n",
            "mean     0.0\n",
            "std      0.0\n",
            "min      0.0\n",
            "25%      0.0\n",
            "50%      0.0\n",
            "75%      0.0\n",
            "max      0.0\n",
            "Name: citation count, dtype: float64\n",
            "Algorithms -> Missing Data citations:\n",
            " count    6.0\n",
            "mean     0.0\n",
            "std      0.0\n",
            "min      0.0\n",
            "25%      0.0\n",
            "50%      0.0\n",
            "75%      0.0\n",
            "max      0.0\n",
            "Name: citation count, dtype: float64\n",
            "Algorithms -> Multimodal Learning citations:\n",
            " count    6.000000\n",
            "mean     0.333333\n",
            "std      0.816497\n",
            "min      0.000000\n",
            "25%      0.000000\n",
            "50%      0.000000\n",
            "75%      0.000000\n",
            "max      2.000000\n",
            "Name: citation count, dtype: float64\n",
            "Algorithms -> Nonlinear Dimensionality Reduction and Manifold Learning citations:\n",
            " count    6.000000\n",
            "mean     0.166667\n",
            "std      0.408248\n",
            "min      0.000000\n",
            "25%      0.000000\n",
            "50%      0.000000\n",
            "75%      0.000000\n",
            "max      1.000000\n",
            "Name: citation count, dtype: float64\n",
            "Data, Challenges, Implementations, and Software -> Software Toolkits citations:\n",
            " count    5.0\n",
            "mean     0.0\n",
            "std      0.0\n",
            "min      0.0\n",
            "25%      0.0\n",
            "50%      0.0\n",
            "75%      0.0\n",
            "max      0.0\n",
            "Name: citation count, dtype: float64\n",
            "Applications -> Computational Photography citations:\n",
            " count    5.000000\n",
            "mean     1.200000\n",
            "std      2.683282\n",
            "min      0.000000\n",
            "25%      0.000000\n",
            "50%      0.000000\n",
            "75%      0.000000\n",
            "max      6.000000\n",
            "Name: citation count, dtype: float64\n",
            "Theory -> Frequentist Statistics citations:\n",
            " count    5.000000\n",
            "mean     0.200000\n",
            "std      0.447214\n",
            "min      0.000000\n",
            "25%      0.000000\n",
            "50%      0.000000\n",
            "75%      0.000000\n",
            "max      1.000000\n",
            "Name: citation count, dtype: float64\n",
            "Neuroscience and Cognitive Science -> Neural Coding citations:\n",
            " count    5.0\n",
            "mean     1.0\n",
            "std      1.0\n",
            "min      0.0\n",
            "25%      0.0\n",
            "50%      1.0\n",
            "75%      2.0\n",
            "max      2.0\n",
            "Name: citation count, dtype: float64\n",
            "Neuroscience and Cognitive Science -> Visual Perception citations:\n",
            " count    5.000000\n",
            "mean     1.200000\n",
            "std      1.788854\n",
            "min      0.000000\n",
            "25%      0.000000\n",
            "50%      0.000000\n",
            "75%      2.000000\n",
            "max      4.000000\n",
            "Name: citation count, dtype: float64\n",
            "Algorithms -> Structured Prediction citations:\n",
            " count    5.0\n",
            "mean     0.0\n",
            "std      0.0\n",
            "min      0.0\n",
            "25%      0.0\n",
            "50%      0.0\n",
            "75%      0.0\n",
            "max      0.0\n",
            "Name: citation count, dtype: float64\n",
            "Algorithms -> Large Scale Learning citations:\n",
            " count    4.000000\n",
            "mean     4.000000\n",
            "std      3.741657\n",
            "min      0.000000\n",
            "25%      2.250000\n",
            "50%      3.500000\n",
            "75%      5.250000\n",
            "max      9.000000\n",
            "Name: citation count, dtype: float64\n",
            "Probabilistic Methods -> Latent Variable Models citations:\n",
            " count    4.000000\n",
            "mean     1.500000\n",
            "std      1.732051\n",
            "min      0.000000\n",
            "25%      0.000000\n",
            "50%      1.500000\n",
            "75%      3.000000\n",
            "max      3.000000\n",
            "Name: citation count, dtype: float64\n",
            "Algorithms -> Ranking and Preference Learning citations:\n",
            " count    4.0\n",
            "mean     2.5\n",
            "std      3.0\n",
            "min      0.0\n",
            "25%      0.0\n",
            "50%      2.0\n",
            "75%      4.5\n",
            "max      6.0\n",
            "Name: citation count, dtype: float64\n",
            "Algorithms -> Metric Learning citations:\n",
            " count    4.0\n",
            "mean     0.0\n",
            "std      0.0\n",
            "min      0.0\n",
            "25%      0.0\n",
            "50%      0.0\n",
            "75%      0.0\n",
            "max      0.0\n",
            "Name: citation count, dtype: float64\n",
            "Applications -> Image Segmentation citations:\n",
            " count    4.0\n",
            "mean     0.0\n",
            "std      0.0\n",
            "min      0.0\n",
            "25%      0.0\n",
            "50%      0.0\n",
            "75%      0.0\n",
            "max      0.0\n",
            "Name: citation count, dtype: float64\n",
            "Reinforcement Learning and Planning -> Decision and Control citations:\n",
            " count    4.0\n",
            "mean     0.0\n",
            "std      0.0\n",
            "min      0.0\n",
            "25%      0.0\n",
            "50%      0.0\n",
            "75%      0.0\n",
            "max      0.0\n",
            "Name: citation count, dtype: float64\n",
            "Algorithms -> Active Learning citations:\n",
            " count     4.000000\n",
            "mean      4.000000\n",
            "std       6.733003\n",
            "min       0.000000\n",
            "25%       0.000000\n",
            "50%       1.000000\n",
            "75%       5.000000\n",
            "max      14.000000\n",
            "Name: citation count, dtype: float64\n",
            "Reinforcement Learning and Planning -> Navigation citations:\n",
            " count    4.0\n",
            "mean     0.5\n",
            "std      1.0\n",
            "min      0.0\n",
            "25%      0.0\n",
            "50%      0.0\n",
            "75%      0.5\n",
            "max      2.0\n",
            "Name: citation count, dtype: float64\n",
            "Theory -> Data-driven Algorithm Design citations:\n",
            " count    3.0\n",
            "mean     0.0\n",
            "std      0.0\n",
            "min      0.0\n",
            "25%      0.0\n",
            "50%      0.0\n",
            "75%      0.0\n",
            "max      0.0\n",
            "Name: citation count, dtype: float64\n",
            "Applications -> Matrix and Tensor Factorization citations:\n",
            " count    3.0\n",
            "mean     0.0\n",
            "std      0.0\n",
            "min      0.0\n",
            "25%      0.0\n",
            "50%      0.0\n",
            "75%      0.0\n",
            "max      0.0\n",
            "Name: citation count, dtype: float64\n",
            "Algorithms -> Stochastic Methods citations:\n",
            " count    3.000000\n",
            "mean     1.000000\n",
            "std      1.732051\n",
            "min      0.000000\n",
            "25%      0.000000\n",
            "50%      0.000000\n",
            "75%      1.500000\n",
            "max      3.000000\n",
            "Name: citation count, dtype: float64\n",
            "Theory -> Spaces of Functions and Kernels citations:\n",
            " count    3.0\n",
            "mean     0.0\n",
            "std      0.0\n",
            "min      0.0\n",
            "25%      0.0\n",
            "50%      0.0\n",
            "75%      0.0\n",
            "max      0.0\n",
            "Name: citation count, dtype: float64\n",
            "Theory -> Regularization citations:\n",
            " count    3.0\n",
            "mean     0.0\n",
            "std      0.0\n",
            "min      0.0\n",
            "25%      0.0\n",
            "50%      0.0\n",
            "75%      0.0\n",
            "max      0.0\n",
            "Name: citation count, dtype: float64\n",
            "Applications -> Visual Scene Analysis and Interpretation citations:\n",
            " count    3.0\n",
            "mean     0.0\n",
            "std      0.0\n",
            "min      0.0\n",
            "25%      0.0\n",
            "50%      0.0\n",
            "75%      0.0\n",
            "max      0.0\n",
            "Name: citation count, dtype: float64\n",
            "Applications -> Video Analysis citations:\n",
            " count    3.0\n",
            "mean     0.0\n",
            "std      0.0\n",
            "min      0.0\n",
            "25%      0.0\n",
            "50%      0.0\n",
            "75%      0.0\n",
            "max      0.0\n",
            "Name: citation count, dtype: float64\n",
            "Algorithms -> Program Induction citations:\n",
            " count    3.0\n",
            "mean     0.0\n",
            "std      0.0\n",
            "min      0.0\n",
            "25%      0.0\n",
            "50%      0.0\n",
            "75%      0.0\n",
            "max      0.0\n",
            "Name: citation count, dtype: float64\n",
            "Theory -> Hardness of Learning and Approximations citations:\n",
            " count    3.000000\n",
            "mean     0.333333\n",
            "std      0.577350\n",
            "min      0.000000\n",
            "25%      0.000000\n",
            "50%      0.000000\n",
            "75%      0.500000\n",
            "max      1.000000\n",
            "Name: citation count, dtype: float64\n",
            "Probabilistic Methods -> Bayesian Nonparametrics citations:\n",
            " count    3.000000\n",
            "mean     1.666667\n",
            "std      2.886751\n",
            "min      0.000000\n",
            "25%      0.000000\n",
            "50%      0.000000\n",
            "75%      2.500000\n",
            "max      5.000000\n",
            "Name: citation count, dtype: float64\n",
            "Algorithms -> Collaborative Filtering citations:\n",
            " count    3.0\n",
            "mean     0.0\n",
            "std      0.0\n",
            "min      0.0\n",
            "25%      0.0\n",
            "50%      0.0\n",
            "75%      0.0\n",
            "max      0.0\n",
            "Name: citation count, dtype: float64\n",
            "Applications -> Computational Social Science citations:\n",
            " count    3.0\n",
            "mean     0.0\n",
            "std      0.0\n",
            "min      0.0\n",
            "25%      0.0\n",
            "50%      0.0\n",
            "75%      0.0\n",
            "max      0.0\n",
            "Name: citation count, dtype: float64\n",
            "Applications -> Recommender Systems citations:\n",
            " count    3.0\n",
            "mean     0.0\n",
            "std      0.0\n",
            "min      0.0\n",
            "25%      0.0\n",
            "50%      0.0\n",
            "75%      0.0\n",
            "max      0.0\n",
            "Name: citation count, dtype: float64\n",
            "Social Aspects of Machine Learning citations:\n",
            " count    3.000000\n",
            "mean     0.666667\n",
            "std      1.154701\n",
            "min      0.000000\n",
            "25%      0.000000\n",
            "50%      0.000000\n",
            "75%      1.000000\n",
            "max      2.000000\n",
            "Name: citation count, dtype: float64\n",
            "Applications -> Game Playing citations:\n",
            " count    3.0\n",
            "mean     0.0\n",
            "std      0.0\n",
            "min      0.0\n",
            "25%      0.0\n",
            "50%      0.0\n",
            "75%      0.0\n",
            "max      0.0\n",
            "Name: citation count, dtype: float64\n",
            "Algorithms -> Density Estimation citations:\n",
            " count    3.000000\n",
            "mean     0.333333\n",
            "std      0.577350\n",
            "min      0.000000\n",
            "25%      0.000000\n",
            "50%      0.000000\n",
            "75%      0.500000\n",
            "max      1.000000\n",
            "Name: citation count, dtype: float64\n",
            "Algorithms -> Communication- or Memory-Bounded Learning citations:\n",
            " count    2.0\n",
            "mean     0.0\n",
            "std      0.0\n",
            "min      0.0\n",
            "25%      0.0\n",
            "50%      0.0\n",
            "75%      0.0\n",
            "max      0.0\n",
            "Name: citation count, dtype: float64\n",
            "Probabilistic Methods -> Bayesian Theory citations:\n",
            " count    2.0\n",
            "mean     0.0\n",
            "std      0.0\n",
            "min      0.0\n",
            "25%      0.0\n",
            "50%      0.0\n",
            "75%      0.0\n",
            "max      0.0\n",
            "Name: citation count, dtype: float64\n",
            "Probabilistic Methods -> Topic Models citations:\n",
            " count    2.00000\n",
            "mean     1.50000\n",
            "std      2.12132\n",
            "min      0.00000\n",
            "25%      0.75000\n",
            "50%      1.50000\n",
            "75%      2.25000\n",
            "max      3.00000\n",
            "Name: citation count, dtype: float64\n",
            "Algorithms -> Adaptive Data Analysis citations:\n",
            " count    2.000000\n",
            "mean     0.500000\n",
            "std      0.707107\n",
            "min      0.000000\n",
            "25%      0.250000\n",
            "50%      0.500000\n",
            "75%      0.750000\n",
            "max      1.000000\n",
            "Name: citation count, dtype: float64\n",
            "Reinforcement Learning and Planning -> Hierarchical RL citations:\n",
            " count    2.0\n",
            "mean     0.0\n",
            "std      0.0\n",
            "min      0.0\n",
            "25%      0.0\n",
            "50%      0.0\n",
            "75%      0.0\n",
            "max      0.0\n",
            "Name: citation count, dtype: float64\n",
            "Algorithms -> Similarity and Distance Learning citations:\n",
            " count    2.000000\n",
            "mean     0.500000\n",
            "std      0.707107\n",
            "min      0.000000\n",
            "25%      0.250000\n",
            "50%      0.500000\n",
            "75%      0.750000\n",
            "max      1.000000\n",
            "Name: citation count, dtype: float64\n",
            "Data, Challenges, Implementations, and Software -> Virtual Environments citations:\n",
            " count    2.00000\n",
            "mean     1.50000\n",
            "std      2.12132\n",
            "min      0.00000\n",
            "25%      0.75000\n",
            "50%      1.50000\n",
            "75%      2.25000\n",
            "max      3.00000\n",
            "Name: citation count, dtype: float64\n",
            "Optimization -> Evolutionary Computation citations:\n",
            " count    2.0\n",
            "mean     0.0\n",
            "std      0.0\n",
            "min      0.0\n",
            "25%      0.0\n",
            "50%      0.0\n",
            "75%      0.0\n",
            "max      0.0\n",
            "Name: citation count, dtype: float64\n",
            "Deep Learning -> Predictive Models citations:\n",
            " count    2.000000\n",
            "mean     0.500000\n",
            "std      0.707107\n",
            "min      0.000000\n",
            "25%      0.250000\n",
            "50%      0.500000\n",
            "75%      0.750000\n",
            "max      1.000000\n",
            "Name: citation count, dtype: float64\n",
            "Data, Challenges, Implementations, and Software -> Data Sets or Data Repositories citations:\n",
            " count    2.0\n",
            "mean     0.0\n",
            "std      0.0\n",
            "min      0.0\n",
            "25%      0.0\n",
            "50%      0.0\n",
            "75%      0.0\n",
            "max      0.0\n",
            "Name: citation count, dtype: float64\n",
            "Neuroscience and Cognitive Science -> Brain-Computer Interfaces and Neural Prostheses citations:\n",
            " count    2.00000\n",
            "mean     1.50000\n",
            "std      2.12132\n",
            "min      0.00000\n",
            "25%      0.75000\n",
            "50%      1.50000\n",
            "75%      2.25000\n",
            "max      3.00000\n",
            "Name: citation count, dtype: float64\n",
            "Data, Challenges, Implementations, and Software citations:\n",
            " count    2.0\n",
            "mean     0.0\n",
            "std      0.0\n",
            "min      0.0\n",
            "25%      0.0\n",
            "50%      0.0\n",
            "75%      0.0\n",
            "max      0.0\n",
            "Name: citation count, dtype: float64\n",
            "Applications -> Automated Reasoning and Formal Methods citations:\n",
            " count    2.0\n",
            "mean     0.0\n",
            "std      0.0\n",
            "min      0.0\n",
            "25%      0.0\n",
            "50%      0.0\n",
            "75%      0.0\n",
            "max      0.0\n",
            "Name: citation count, dtype: float64\n",
            "Neuroscience and Cognitive Science -> Plasticity and Adaptation citations:\n",
            " count    2.0\n",
            "mean     0.0\n",
            "std      0.0\n",
            "min      0.0\n",
            "25%      0.0\n",
            "50%      0.0\n",
            "75%      0.0\n",
            "max      0.0\n",
            "Name: citation count, dtype: float64\n",
            "Applications -> Web Applications and Internet Data citations:\n",
            " count    2.000000\n",
            "mean     0.500000\n",
            "std      0.707107\n",
            "min      0.000000\n",
            "25%      0.250000\n",
            "50%      0.500000\n",
            "75%      0.750000\n",
            "max      1.000000\n",
            "Name: citation count, dtype: float64\n",
            "Applications -> Tracking and Motion in Video citations:\n",
            " count    2.000000\n",
            "mean     3.000000\n",
            "std      4.242641\n",
            "min      0.000000\n",
            "25%      1.500000\n",
            "50%      3.000000\n",
            "75%      4.500000\n",
            "max      6.000000\n",
            "Name: citation count, dtype: float64\n",
            "Applications -> Denoising citations:\n",
            " count    2.000000\n",
            "mean     2.000000\n",
            "std      1.414214\n",
            "min      1.000000\n",
            "25%      1.500000\n",
            "50%      2.000000\n",
            "75%      2.500000\n",
            "max      3.000000\n",
            "Name: citation count, dtype: float64\n",
            "Applications -> Object Recognition citations:\n",
            " count    2.0\n",
            "mean     0.0\n",
            "std      0.0\n",
            "min      0.0\n",
            "25%      0.0\n",
            "50%      0.0\n",
            "75%      0.0\n",
            "max      0.0\n",
            "Name: citation count, dtype: float64\n",
            "Applications -> Dialog- or Communication-Based Learning citations:\n",
            " count    2.000000\n",
            "mean     0.500000\n",
            "std      0.707107\n",
            "min      0.000000\n",
            "25%      0.250000\n",
            "50%      0.500000\n",
            "75%      0.750000\n",
            "max      1.000000\n",
            "Name: citation count, dtype: float64\n",
            "Applications -> Hardware and Systems citations:\n",
            " count    2.000000\n",
            "mean     2.000000\n",
            "std      1.414214\n",
            "min      1.000000\n",
            "25%      1.500000\n",
            "50%      2.000000\n",
            "75%      2.500000\n",
            "max      3.000000\n",
            "Name: citation count, dtype: float64\n",
            "Neuroscience and Cognitive Science -> Human or Animal Learning citations:\n",
            " count    2.000000\n",
            "mean     1.000000\n",
            "std      1.414214\n",
            "min      0.000000\n",
            "25%      0.500000\n",
            "50%      1.000000\n",
            "75%      1.500000\n",
            "max      2.000000\n",
            "Name: citation count, dtype: float64\n",
            "Applications -> Music Modeling and Analysis citations:\n",
            " count    2.000000\n",
            "mean     1.000000\n",
            "std      1.414214\n",
            "min      0.000000\n",
            "25%      0.500000\n",
            "50%      1.000000\n",
            "75%      1.500000\n",
            "max      2.000000\n",
            "Name: citation count, dtype: float64\n",
            "Applications -> Quantum Learning citations:\n",
            " count    1.0\n",
            "mean     0.0\n",
            "std      NaN\n",
            "min      0.0\n",
            "25%      0.0\n",
            "50%      0.0\n",
            "75%      0.0\n",
            "max      0.0\n",
            "Name: citation count, dtype: float64\n",
            "Deep Learning -> Interaction-Based Deep Networks citations:\n",
            " count    1.0\n",
            "mean     0.0\n",
            "std      NaN\n",
            "min      0.0\n",
            "25%      0.0\n",
            "50%      0.0\n",
            "75%      0.0\n",
            "max      0.0\n",
            "Name: citation count, dtype: float64\n",
            "Applications -> Signal Processing citations:\n",
            " count    1.0\n",
            "mean     0.0\n",
            "std      NaN\n",
            "min      0.0\n",
            "25%      0.0\n",
            "50%      0.0\n",
            "75%      0.0\n",
            "max      0.0\n",
            "Name: citation count, dtype: float64\n",
            "Applications -> Speech Recognition citations:\n",
            " count    1.0\n",
            "mean     0.0\n",
            "std      NaN\n",
            "min      0.0\n",
            "25%      0.0\n",
            "50%      0.0\n",
            "75%      0.0\n",
            "max      0.0\n",
            "Name: citation count, dtype: float64\n",
            "Data, Challenges, Implementations, and Software -> Benchmarks citations:\n",
            " count    1.0\n",
            "mean     8.0\n",
            "std      NaN\n",
            "min      8.0\n",
            "25%      8.0\n",
            "50%      8.0\n",
            "75%      8.0\n",
            "max      8.0\n",
            "Name: citation count, dtype: float64\n",
            "Applications -> Quantitative Finance and Econometrics citations:\n",
            " count    1.0\n",
            "mean     0.0\n",
            "std      NaN\n",
            "min      0.0\n",
            "25%      0.0\n",
            "50%      0.0\n",
            "75%      0.0\n",
            "max      0.0\n",
            "Name: citation count, dtype: float64\n",
            "Applications -> Motor Control citations:\n",
            " count    1.0\n",
            "mean     0.0\n",
            "std      NaN\n",
            "min      0.0\n",
            "25%      0.0\n",
            "50%      0.0\n",
            "75%      0.0\n",
            "max      0.0\n",
            "Name: citation count, dtype: float64\n",
            "Neuroscience and Cognitive Science -> Brain Mapping citations:\n",
            " count    1.0\n",
            "mean     0.0\n",
            "std      NaN\n",
            "min      0.0\n",
            "25%      0.0\n",
            "50%      0.0\n",
            "75%      0.0\n",
            "max      0.0\n",
            "Name: citation count, dtype: float64\n",
            "Neuroscience and Cognitive Science -> Reasoning citations:\n",
            " count    1.0\n",
            "mean     0.0\n",
            "std      NaN\n",
            "min      0.0\n",
            "25%      0.0\n",
            "50%      0.0\n",
            "75%      0.0\n",
            "max      0.0\n",
            "Name: citation count, dtype: float64\n",
            "Probabilistic Methods  citations:\n",
            " count    1.0\n",
            "mean     0.0\n",
            "std      NaN\n",
            "min      0.0\n",
            "25%      0.0\n",
            "50%      0.0\n",
            "75%      0.0\n",
            "max      0.0\n",
            "Name: citation count, dtype: float64\n",
            "Algorithms -> Large Margin Methods citations:\n",
            " count    1.0\n",
            "mean     0.0\n",
            "std      NaN\n",
            "min      0.0\n",
            "25%      0.0\n",
            "50%      0.0\n",
            "75%      0.0\n",
            "max      0.0\n",
            "Name: citation count, dtype: float64\n",
            "Applications -> Information Retrieval citations:\n",
            " count    1.0\n",
            "mean     3.0\n",
            "std      NaN\n",
            "min      3.0\n",
            "25%      3.0\n",
            "50%      3.0\n",
            "75%      3.0\n",
            "max      3.0\n",
            "Name: citation count, dtype: float64\n",
            "Algorithms -> Model Selection and Structure Learning citations:\n",
            " count    1.0\n",
            "mean     2.0\n",
            "std      NaN\n",
            "min      2.0\n",
            "25%      2.0\n",
            "50%      2.0\n",
            "75%      2.0\n",
            "max      2.0\n",
            "Name: citation count, dtype: float64\n",
            "Probabilistic Methods -> Probabilistic Programming citations:\n",
            " count    1.0\n",
            "mean     0.0\n",
            "std      NaN\n",
            "min      0.0\n",
            "25%      0.0\n",
            "50%      0.0\n",
            "75%      0.0\n",
            "max      0.0\n",
            "Name: citation count, dtype: float64\n",
            "Neuroscience and Cognitive Science -> Memory citations:\n",
            " count    1.0\n",
            "mean     5.0\n",
            "std      NaN\n",
            "min      5.0\n",
            "25%      5.0\n",
            "50%      5.0\n",
            "75%      5.0\n",
            "max      5.0\n",
            "Name: citation count, dtype: float64\n",
            "Applications -> Activity and Event Recognition citations:\n",
            " count    1.0\n",
            "mean     0.0\n",
            "std      NaN\n",
            "min      0.0\n",
            "25%      0.0\n",
            "50%      0.0\n",
            "75%      0.0\n",
            "max      0.0\n",
            "Name: citation count, dtype: float64\n",
            "Probabilistic Methods -> Distributed Inference citations:\n",
            " count    1.0\n",
            "mean     0.0\n",
            "std      NaN\n",
            "min      0.0\n",
            "25%      0.0\n",
            "50%      0.0\n",
            "75%      0.0\n",
            "max      0.0\n",
            "Name: citation count, dtype: float64\n",
            "Probabilistic Methods -> Belief Propagation citations:\n",
            " count    1.0\n",
            "mean     0.0\n",
            "std      NaN\n",
            "min      0.0\n",
            "25%      0.0\n",
            "50%      0.0\n",
            "75%      0.0\n",
            "max      0.0\n",
            "Name: citation count, dtype: float64\n",
            "Neuroscience and Cognitive Science -> Spike Train Generation citations:\n",
            " count    1.0\n",
            "mean     0.0\n",
            "std      NaN\n",
            "min      0.0\n",
            "25%      0.0\n",
            "50%      0.0\n",
            "75%      0.0\n",
            "max      0.0\n",
            "Name: citation count, dtype: float64\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}